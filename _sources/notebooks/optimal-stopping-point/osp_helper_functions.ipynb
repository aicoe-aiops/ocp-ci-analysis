{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c62b729a-eb03-4ccb-8a0f-757144a42adc",
   "metadata": {},
   "source": [
    "# Optimal Stopping Point - Helper Functions\n",
    "\n",
    "This notebook contains some helper functions that are needed for the calculation of Optimal Stopping Point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c48bf8f-c820-42f5-9201-bb6a76698b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "\n",
    "from intersect import intersection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import boto3\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed472cd-7a91-4331-9775-b56c6acbc8d5",
   "metadata": {},
   "source": [
    "## Metric Template Functions\n",
    "\n",
    "These are functions copied from `../data_sources/TestGrid/metrics_template.ipynb` as a workaround to [this issue](https://github.com/elyra-ai/elyra/issues/1734) where functions imported from notebooks must be in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85b6a56b-ecaa-4215-a106-4a1697eae1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CephCommunication:\n",
    "    \"\"\"\n",
    "    Class to establish communication with a ceph s3 bucket.\n",
    "    It connects with the bucket and provides methods to read and write data in the parquet format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, s3_endpoint_url, aws_access_key_id, aws_secret_access_key, s3_bucket\n",
    "    ):\n",
    "        self.s3_endpoint_url = s3_endpoint_url\n",
    "        self.aws_access_key_id = aws_access_key_id\n",
    "        self.aws_secret_access_key = aws_secret_access_key\n",
    "        self.s3_resource = boto3.resource(\n",
    "            \"s3\",\n",
    "            endpoint_url=self.s3_endpoint_url,\n",
    "            aws_access_key_id=self.aws_access_key_id,\n",
    "            aws_secret_access_key=self.aws_secret_access_key,\n",
    "        )\n",
    "        self.bucket = s3_bucket\n",
    "        ## Todo: Add try catch\n",
    "\n",
    "    def upload_to_ceph(self, dataframe, s3_path, filename):\n",
    "        \"\"\"\n",
    "        This helper function takes as input the data frame to be uploaded, and the output filename.\n",
    "        It then saves the data frame in the defined ceph bucket.\n",
    "        \"\"\"\n",
    "        parquet_buffer = BytesIO()\n",
    "        dataframe.to_parquet(parquet_buffer)\n",
    "        s3_obj = self.s3_resource.Object(self.bucket, f\"{s3_path}/{filename}\")\n",
    "        status = s3_obj.put(Body=parquet_buffer.getvalue())\n",
    "        return status\n",
    "\n",
    "    def read_from_ceph(self, s3_path, filename):\n",
    "        \"\"\"\n",
    "        Helper function to read from ceph and see if the saved data is correct.\n",
    "        \"\"\"\n",
    "        buffer = BytesIO()\n",
    "        s3_object = self.s3_resource.Object(self.bucket, f\"{s3_path}/{filename}\")\n",
    "        s3_object.download_fileobj(buffer)\n",
    "        df_temp = pd.read_parquet(buffer)\n",
    "        return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beec089d-e184-4410-a3fc-688e63846ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_disk(dataframe, path, filename):\n",
    "    \"\"\"\n",
    "    Helper function to save the dataframe\n",
    "    as a parquet file to disk.\n",
    "    \"\"\"\n",
    "    dataset_base_path = Path(path)\n",
    "    dataset_base_path.mkdir(parents=True, exist_ok=True)\n",
    "    dataframe.to_parquet(f\"{path}/{filename}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def read_from_disk(path, filename):\n",
    "    \"\"\"\n",
    "    Helper function to read from disk and see if the saved data is the same.\n",
    "    \"\"\"\n",
    "    return pd.read_parquet(f\"{path}/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc34ea4-2022-4823-92d0-906a72fc11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper classes and functions\n",
    "## In your metric notebook, you could just\n",
    "## load the template notebook functions\n",
    "## If your metric requires specific helper\n",
    "## functions, write them here, otherwise if these\n",
    "## functions are general, update the template.\n",
    "## Author: AIOps\n",
    "\n",
    "\n",
    "class TestStatus(Enum):\n",
    "    \"\"\"\n",
    "    Enum to encode what test status each value in testgrid corresponds to\n",
    "\n",
    "    Basically python equivalent of the enum here:\n",
    "    https://github.com/GoogleCloudPlatform/testgrid/blob/a18fe953cf98174c215c43e0258b0515e37c283b/pb/test_status/test_status.proto#L3\n",
    "    \"\"\"\n",
    "\n",
    "    NO_RESULT = 0\n",
    "    PASS = 1\n",
    "    PASS_WITH_ERRORS = 2\n",
    "    PASS_WITH_SKIPS = 3\n",
    "    RUNNING = 4\n",
    "    CATEGORIZED_ABORT = 5\n",
    "    UNKNOWN = 6\n",
    "    CANCEL = 7\n",
    "    BLOCKED = 8\n",
    "    TIMED_OUT = 9\n",
    "    CATEGORIZED_FAIL = 10\n",
    "    BUILD_FAIL = 11\n",
    "    FAIL = 12\n",
    "    FLAKY = 13\n",
    "    TOOL_FAIL = 14\n",
    "    BUILD_PASSED = 15\n",
    "\n",
    "\n",
    "def decode_run_length(x):\n",
    "    \"\"\"\n",
    "    Decodes the run length encoded data into an unrolled form.\n",
    "    Returns a list of values.\n",
    "\n",
    "    E.g. takes in [{\"value\":12, \"count\":3}, {\"value\":1, \"count\":2}]\n",
    "    and gives [12, 12, 12, 1, 1]\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    for run_length in x:\n",
    "        extension = [run_length[\"value\"]] * run_length[\"count\"]\n",
    "        lst.extend(extension)\n",
    "    return lst\n",
    "\n",
    "\n",
    "def testgrid_labelwise_encoding(data, label, overall_only=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Run length encode the dataset and unroll the dataset into a list.\n",
    "    Return flattened list after encoding specified value as\n",
    "    True and rest as False\n",
    "    \"\"\"\n",
    "\n",
    "    percent_label_by_grid_csv = []\n",
    "\n",
    "    for tab in data.keys():\n",
    "\n",
    "        for grid in data[tab].keys():\n",
    "            current_grid = data[tab][grid]\n",
    "\n",
    "            if len(current_grid[\"grid\"]) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                # get all timestamps for this grid (x-axis of grid)\n",
    "                timestamps = [\n",
    "                    datetime.datetime.fromtimestamp(x // 1000)\n",
    "                    for x in current_grid[\"timestamps\"]\n",
    "                ]\n",
    "                # get all test names for this grid (y-axis of grid)\n",
    "                tests = [\n",
    "                    current_grid[\"grid\"][i][\"name\"]\n",
    "                    for i in range(len(current_grid[\"grid\"]))\n",
    "                ]\n",
    "\n",
    "                graphs = [\n",
    "                    current_grid[\"grid\"][i][\"graphs\"]\n",
    "                    for i in range(len(current_grid[\"grid\"]))\n",
    "                ]\n",
    "\n",
    "                # unroll the run-length encoding and set bool for flake or not (x==13)\n",
    "                decoded = [\n",
    "                    (\n",
    "                        np.array(decode_run_length(current_grid[\"grid\"][i][\"statuses\"]))\n",
    "                        == label\n",
    "                    ).tolist()\n",
    "                    for i in range(len(current_grid[\"grid\"]))\n",
    "                ]\n",
    "\n",
    "                # add the timestamp to bool value\n",
    "                decoded = [list(zip(timestamps, g)) for g in decoded]\n",
    "                # add the test, tab and grid name to each entry\n",
    "                # TODO: any ideas for avoiding this quad-loop\n",
    "                # if the label is passed as an arg, add the timestamp, tab,\n",
    "                # grid, tests, graphs metric and the bool values\n",
    "                if label:\n",
    "                    for i, d in enumerate(decoded):\n",
    "                        for j, k in enumerate(d):\n",
    "                            # here we are fetching the test duration values for the tests\n",
    "                            # however,since not all tests contain time duration values,\n",
    "                            # we are only considering the 'Overall' test and fetching the\n",
    "                            # time duration values for this test and setting it to 'None'\n",
    "                            # for all the other tests in each grid\n",
    "                            if overall_only:\n",
    "                                if \"Overall\" in tests[i]:\n",
    "                                    try:\n",
    "                                        test_duration = graphs[i][0][\"values\"][0][j]\n",
    "                                    except IndexError:\n",
    "                                        test_duration = None\n",
    "                                else:\n",
    "                                    test_duration = None\n",
    "                            else:\n",
    "                                try:\n",
    "                                    graphs[i][0].keys()\n",
    "                                    try:\n",
    "                                        graphs[i][0][\"values\"][0][j]\n",
    "                                        test_duration = graphs[i][0][\"values\"][0][j]\n",
    "                                    except IndexError:\n",
    "                                        test_duration = None\n",
    "                                except TypeError:\n",
    "                                    test_duration = None\n",
    "\n",
    "                            decoded[i][j] = (\n",
    "                                k[0],\n",
    "                                tab,\n",
    "                                grid,\n",
    "                                tests[i],\n",
    "                                test_duration,\n",
    "                                k[1],\n",
    "                            )\n",
    "                    # accumulate the results\n",
    "                    percent_label_by_grid_csv.append(decoded)\n",
    "\n",
    "                # if label is 'None', add only the timestamp, tab, grid, tests and test\n",
    "                # duration values\n",
    "                else:\n",
    "                    for i, d in enumerate(decoded):\n",
    "                        for j, k in enumerate(d):\n",
    "                            # here we are fetching the time duration values for the tests\n",
    "                            # however,since not all tests contain time duration values,\n",
    "                            # we are only considering the 'Overall' test and fetching the time duration\n",
    "                            # values for this test in each grid\n",
    "                            if overall_only:\n",
    "                                if \"Overall\" in tests[i]:\n",
    "                                    try:\n",
    "                                        test_duration = graphs[i][0][\"values\"][0][j]\n",
    "                                    except IndexError:\n",
    "                                        print(\n",
    "                                            \"Test duration value does not exist for all \\\n",
    "                                            timestamps for test Overall in grid \",\n",
    "                                            grid,\n",
    "                                            \"in tab \",\n",
    "                                            tab,\n",
    "                                        )\n",
    "                                        test_duration = None\n",
    "                                else:\n",
    "                                    test_duration = None\n",
    "                            else:\n",
    "                                try:\n",
    "                                    graphs[i][0].keys()\n",
    "                                    try:\n",
    "                                        graphs[i][0][\"values\"][0][j]\n",
    "                                        test_duration = graphs[i][0][\"values\"][0][j]\n",
    "                                    except IndexError:\n",
    "                                        test_duration = None\n",
    "                                except TypeError:\n",
    "                                    test_duration = None\n",
    "\n",
    "                            decoded[i][j] = (k[0], tab, grid, tests[i], test_duration)\n",
    "                    percent_label_by_grid_csv.append(decoded)\n",
    "\n",
    "    # output above leaves us with a doubly nested list. Flatten\n",
    "    flat_list = [item for sublist in percent_label_by_grid_csv for item in sublist]\n",
    "    flatter_list = [item for sublist in flat_list for item in sublist]\n",
    "\n",
    "    return flatter_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2532f76-aee5-45d4-b5c6-841952625ecb",
   "metadata": {},
   "source": [
    "## Optimal Stopping point functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38ec11a5-2316-4db4-b8a0-ca896776bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_tests(raw_data, status_code):\n",
    "    \"\"\"\n",
    "    This function takes raw data and status code of the test\n",
    "    and returns a dataframe with all the tests with that status code.\n",
    "    \"\"\"\n",
    "    # Fetch the list of all tests\n",
    "    tests_list = testgrid_labelwise_encoding(raw_data, status_code, overall_only=False)\n",
    "    # Convert to dataframe\n",
    "    tests_df = pd.DataFrame(\n",
    "        tests_list,\n",
    "        columns=[\n",
    "            \"timestamp\",\n",
    "            \"tab\",\n",
    "            \"grid\",\n",
    "            \"test\",\n",
    "            \"test_duration\",\n",
    "            \"failure/passing\",\n",
    "        ],\n",
    "    )\n",
    "    tests_df.head()\n",
    "    # We will drop all the rows having NaN values\n",
    "    tests_df = tests_df.dropna()\n",
    "    tests_df = tests_df[tests_df[\"failure/passing\"]]\n",
    "    return tests_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec099455-3b16-477e-9294-613bb888361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_test_type(df, test):\n",
    "    \"\"\"\n",
    "    This function takes the dataframe with all tests and\n",
    "    filters the dataframe for a specified test name.\n",
    "    \"\"\"\n",
    "    list_test = df[df[\"test\"] == test]\n",
    "    list_test = list_test.reset_index(drop=True)\n",
    "    return list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c95a8b4-99ad-470e-9a78-f7dc6f159d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df, column, pct, pct_lower):\n",
    "    \"\"\"\n",
    "    Function to standardize the features by removing the mean\n",
    "    and scaling to unit variance using StandardScaler library.\n",
    "\n",
    "    Returns standandardized feature, length of the feature\n",
    "    and the original feature.\n",
    "    \"\"\"\n",
    "    sc = StandardScaler()\n",
    "    y = df[column][df[column].notnull()].to_list()\n",
    "    y.sort()\n",
    "    len_y = len(y)\n",
    "    y = y[int(pct_lower * len_y) : int(len_y * pct)]\n",
    "    len_y = len(y)\n",
    "    yy = [[x] for x in y]\n",
    "    sc.fit(yy)\n",
    "    y_std = sc.transform(yy)\n",
    "    y_std = y_std.flatten()\n",
    "    return y_std, len_y, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfd64871-4e40-49c8-83fd-6ea96ee0d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_distribution(df, column, pct, pct_lower):\n",
    "    \"\"\"\n",
    "    This function helps to list out the chi-square statistics for each\n",
    "    distribution and further sorts them to find the best distribution.\n",
    "\n",
    "    Returns a table that contains sorted chi-square values as well as\n",
    "    the parameters such as mu (shape), loc (location) and scale for each\n",
    "    distribution.\n",
    "    \"\"\"\n",
    "    # Set up list of candidate distributions to use\n",
    "    y_std, size, y_org = standardize(df, column, pct, pct_lower)\n",
    "    dist_names = [\n",
    "        \"weibull_min\",\n",
    "        \"norm\",\n",
    "        \"weibull_max\",\n",
    "        \"beta\",\n",
    "        \"invgauss\",\n",
    "        \"uniform\",\n",
    "        \"gamma\",\n",
    "        \"expon\",\n",
    "        \"lognorm\",\n",
    "        \"pearson3\",\n",
    "        \"triang\",\n",
    "    ]\n",
    "\n",
    "    chi_square_statistics = []\n",
    "\n",
    "    # 50 bins\n",
    "    percentile_bins = np.linspace(0, 100, 50)\n",
    "    percentile_cutoffs = np.percentile(y_std, percentile_bins)\n",
    "    observed_frequency, bins = np.histogram(y_std, bins=percentile_cutoffs)\n",
    "    cum_observed_frequency = np.cumsum(observed_frequency)\n",
    "    # Data frame to store results\n",
    "    dist_param = pd.DataFrame()\n",
    "    dist_param[\"Distribution Names\"] = dist_names\n",
    "    param_list = []\n",
    "\n",
    "    # Loop through candidate distributions\n",
    "    for distribution in dist_names:\n",
    "        # Set up distribution and get fitted distribution parameters\n",
    "        dist = getattr(scipy.stats, distribution)\n",
    "        param = dist.fit(y_std)\n",
    "        param_list.append(param)\n",
    "\n",
    "        # Get expected counts in percentile bins\n",
    "        # cdf of fitted distribution across bins\n",
    "        cdf_fitted = dist.cdf(percentile_cutoffs, *param)\n",
    "        expected_frequency = []\n",
    "        for bin in range(len(percentile_bins) - 1):\n",
    "            expected_cdf_area = cdf_fitted[bin + 1] - cdf_fitted[bin]\n",
    "            expected_frequency.append(expected_cdf_area)\n",
    "\n",
    "        # Chi-square Statistics\n",
    "        expected_frequency = np.array(expected_frequency) * size\n",
    "        cum_expected_frequency = np.cumsum(expected_frequency)\n",
    "        ss = scipy.stats.chisquare(\n",
    "            f_obs=cum_observed_frequency, f_exp=cum_expected_frequency\n",
    "        )\n",
    "        chi_square_statistics.append(ss)\n",
    "\n",
    "    # Append results to data frame\n",
    "    dist_param[\"Parameters\"] = param_list\n",
    "    dist_param.set_index(\"Distribution Names\")\n",
    "    # Sort by minimum ch-square statistics\n",
    "    results = pd.DataFrame()\n",
    "    results[\"Distribution\"] = dist_names\n",
    "    results[\"chi_square and p-value\"] = chi_square_statistics\n",
    "    results.sort_values([\"chi_square and p-value\"], inplace=True)\n",
    "\n",
    "    print(\"\\nDistributions listed by Betterment of fit:\")\n",
    "    print(\"............................................\")\n",
    "    print(results)\n",
    "    return dist_param, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf4fc92d-11a2-4da5-9e35-f6db76cf9e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_distribution(dist_param, results):\n",
    "    \"\"\"\n",
    "    This function takes the distribution paramaters and results\n",
    "    from fit_distribution function and finds the best distribution\n",
    "    and returns disribution name and parameters.\n",
    "    \"\"\"\n",
    "    best_dist = results[\"Distribution\"][0]\n",
    "    params = dist_param[dist_param[\"Distribution Names\"] == best_dist][\n",
    "        \"Parameters\"\n",
    "    ].values\n",
    "    params = list(itertools.chain(*params))\n",
    "    return best_dist, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "070bffbf-8a3c-4b10-8a48-b1fbb992ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_stopping_point(\n",
    "    best_dist,\n",
    "    y_std_failing,\n",
    "    y_failing,\n",
    "    parameters_failing,\n",
    "    y_std_passing,\n",
    "    y_passing,\n",
    "    parameters_passing,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function takes the best_distribution,\n",
    "    failing and passing distributions and parameters\n",
    "    and returns an optimal stopping point for the test.\n",
    "    \"\"\"\n",
    "\n",
    "    dist = getattr(scipy.stats, best_dist)\n",
    "\n",
    "    # Obtain the intersection points between the distribution curves\n",
    "    x, y = intersection(\n",
    "        y_failing,\n",
    "        dist.pdf(\n",
    "            y_std_failing,\n",
    "            parameters_failing[0],\n",
    "            parameters_failing[1],\n",
    "            parameters_failing[2],\n",
    "        ),\n",
    "        y_passing,\n",
    "        dist.pdf(\n",
    "            y_std_passing,\n",
    "            parameters_passing[0],\n",
    "            parameters_passing[1],\n",
    "            parameters_passing[2],\n",
    "        ),\n",
    "    )\n",
    "    osp = max(x)\n",
    "    return osp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
