# Table of Contents
- [Research on current industry offerings](#research-on-current-industry-offerings)
- [Data Engineering: Metrics and KPIs for CI](#data-engineering-metrics-and-kpis-for-ci)
  * [TestGrid](#testgrid)
  * [Prow/GCS Artifacts](#prow-gcs-artifacts)
  * [Github](#github)
  * [Bugzilla](#bugzilla)
  * [Telemetry](#telemetry)
- [Machine Learning and Analytics Projects](#machine-learning-and-analytics-projects)
  * [TestGrid Failure Type Classification](#testgrid-failure-type-classification)
  * [Prow Log Templating For Downstream ML Tasks](#prow-log-templating-for-downstream-ml-tasks)
  * [More Projects Coming Soon…](#more-projects-coming-soon-)
- [Automate Notebook Pipelines using Elyra and Kubeflow](#automate-notebook-pipelines-using-elyra-and-kubeflow)

# Research on current industry offerings

Find a curated list of companies involved in AI/ML for CI/CD [here](docs/aiml-cicd-market-research.md).

#  Data Engineering: Metrics and KPIs for CI

Before we attempt to apply any AI or machine learning techniques to improve the CI workflow, it is important that we know how to both quantify and evaluate the current state of the CI workflow. In order to do this we must establish and collect the relevant metrics and key performance indicators (KPIs) needed to measure it. This is a critical first step as it allows us to quantify the state of CI operations, as well as apply the KPIs we will need to evaluate the impact of our AI tools in the future.

There are currently five open datasets that can be used to help us fully describe the CI process: Testgrid, Prow, Github, Telemetry and Bugzilla. This data is currently stored in disparate locations and does not exist in a data science friendly format ready for analysis. Below are our current efforts to collect and prepare each of these datasets for further analysis.

##  TestGrid:

According to the project's [readme](https://github.com/GoogleCloudPlatform/testgrid), TestGrid is a, "highly configurable, interactive dashboard for viewing your test results in a grid!" In other words, it’s an aggregation and visualization platform for CI data. Testgrid primarily reports categorical metrics about which tests passed or failed during specific builds over a configurable time window.

* [List of metrics and KPI's from TestGrid](../notebooks/data-sources/TestGrid/metrics/README.md)
* [Metric template notebook](../notebooks/data-sources/TestGrid/metrics/metric_template.ipynb) and [explainer video](https://youtu.be/ouxtHH2vOHg)
* [TestGrid data access and pre-processing](../notebooks/data-sources/TestGrid/testgrid_EDA.ipynb)
* [Collect raw data notebook](../notebooks/data-sources/TestGrid/metrics/get_raw_data.ipynb)
* Visualization notebook
* [Automated metric pipeline](http://istio-ingressgateway-istio-system.apps.zero.massopen.cloud/pipeline/)
* [Explainer video](https://www.youtube.com/watch?v=lY75bDv6kd4)

## Prow/GCS Artifacts:

TestGrid provides the results of tests, but if we want to triage an issue and see the actual logs generated during the building and testing process, we will need to access the logs generated by [prow](https://prow.ci.openshift.org/) and stored in google cloud storage. This dataset contains all of the log data generated for each build and each job as directories and text files in remote storage.

* [Prow Archive Discovery](../notebooks/data-sources/gcsweb-ci/prow_archive_discovery.ipynb)
* [Video Walkthrough](https://youtu.be/JjFWFaMfUJA)

## Github:

The builds and tests run by the CI process are required because of changes that are happening in the applications code base. The goal of CI is to automatically identify if any of these code changes will cause problems for the deployed application. Therefore, we also include information such as metadata and diff’s about the PR’s associated with the builds run by Prow. This dataset contains a mix of numerical, categorical and textual data types.

* [GitHub PR EDA](../notebooks/data-sources/oc-github-repo/github_PR_EDA.ipynb)
* [Video Walkthrough](https://youtu.be/bUm0jvXaY14)

## Bugzilla:

[Bugzilla](https://bugzilla.redhat.com/) is Red Hat’s bug-tracking system and is used to submit and review defects that have been found in Red Hat distributions. In addition to TestGrid, analyzing bugs related to OpenShift CI can help us get into automated root cause analysis. This is primarily a dataset of human written text.

* [Bugzilla EDA notebook](../notebooks/data-sources/Bugzilla/bugzilla_EDA.ipynb)
* [Video Walkthrough](https://youtu.be/FMXv3l4NsPc)

## Telemetry:

The builds and tests we are interested in analyzing run in the cloud and produce metrics about the resources they are using and report any alerts they experience while running. These metrics are stored in Thanos for users to query. Here we have primarily numerical time series data that describes the underlying state of the cluster running these builds and tests.

* [Telemerty EDA notebook](../notebooks/data-sources/Telemetry/telemetry_EDA.ipynb)
* [Video Walkthrough](https://youtu.be/D38gR-7c2dc)

#  Machine Learning and Analytics Projects

With the data sources made easily accessible and with the necessary metrics and KPIs available to quantify and evaluate the CI workflow we can start to apply some AI and machine learning techniques to help improve the CI workflow. There are many ways in which this could be done given the multimodal, multi-source nature of our data. Instead of defining a single specific problem to solve, our current aim is to use this repository as a hub for multiple machine learning and analytics projects centered around this data for AIOps problems focused on improving CI workflows. Below is a list of the current ML and analytics projects.

## TestGrid Failure Type Classification

Currently, human subject matter experts are able to identify different types of failures by looking at the testgrids. This is, however, a manual process.  This project aims to automate the manual identification process for individual Testgrids. This can be thought of as a classification problem aimed at classifying errors on the testgrids as either flakey tests, infra flakes, install flakes or new test failures.

* [Detailed project description](../notebooks/failure-type-classification/README.md)
* [Failure Type Classification Notebook](../notebooks/failure-type-classification/stage/failure_type_classifier.ipynb)

## Prow Log Templating For Downstream ML Tasks

Logs represent a rich source of information for automated triaging and root cause analysis. Unfortunately, logs are very noisy data types, i.e, two logs that are of the same type but from two different sources may be different enough at a character level that traditional comparison methods are insufficient to capture this similarity. To overcome this issue, we will use the Prow logs made available to us by this project to identify useful methods for learning log templates that denoise log data and help improve performance on downstream ML tasks.

*   Notebook (forthcoming)

## More Projects Coming Soon…

*   [List of potential ML projects](https://github.com/aicoe-aiops/ocp-ci-analysis/issues?q=is%3Aissue+is%3Aopen+%22ML+Request%22+).

# Automate Notebook Pipelines using Elyra and Kubeflow

In order to automate the sequential running of the various notebooks in the project responsible for data collection, metric calculation, ML analysis, we are using Kubeflow Pipelines.  For more information on using Elyra and Kubeflow pipelines to automate the notebook workflows you can go through the following resources.

* [Guide](automating-using-elyra.md)
* [Tutorial Video](https://youtu.be/bh5WpKq3W7Y)
