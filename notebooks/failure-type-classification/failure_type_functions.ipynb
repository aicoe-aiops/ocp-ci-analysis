{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure type functions \n",
    "\n",
    "This notebook contains the helper functions to run `failure_type_classifier.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T20:10:30.227730Z",
     "start_time": "2021-01-26T20:10:29.130177Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "from scipy.signal import convolve2d\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Template Functions\n",
    "\n",
    "These are functions copied from `../data_sources/TestGrid/metrics_template.ipynb` as a workaround to [this issue](https://github.com/elyra-ai/elyra/issues/1734) where functions imported from notebooks must be in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_run_length(x):\n",
    "    \"\"\"\n",
    "    Decodes the run length encoded data into an unrolled form.\n",
    "    Returns a list of values.\n",
    "\n",
    "    E.g. takes in [{\"value\":12, \"count\":3}, {\"value\":1, \"count\":2}]\n",
    "    and gives [12, 12, 12, 1, 1]\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    for run_length in x:\n",
    "        extension = [run_length[\"value\"]] * run_length[\"count\"]\n",
    "        lst.extend(extension)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CephCommunication:\n",
    "    \"\"\"\n",
    "    Class to establish communication with a ceph s3 bucket.\n",
    "    It connects with the bucket and provides methods to read and write data in the parquet format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, s3_endpoint_url, aws_access_key_id, aws_secret_access_key, s3_bucket\n",
    "    ):\n",
    "        self.s3_endpoint_url = s3_endpoint_url\n",
    "        self.aws_access_key_id = aws_access_key_id\n",
    "        self.aws_secret_access_key = aws_secret_access_key\n",
    "        self.s3_resource = boto3.resource(\n",
    "            \"s3\",\n",
    "            endpoint_url=self.s3_endpoint_url,\n",
    "            aws_access_key_id=self.aws_access_key_id,\n",
    "            aws_secret_access_key=self.aws_secret_access_key,\n",
    "        )\n",
    "        self.bucket = s3_bucket\n",
    "        ## Todo: Add try catch\n",
    "\n",
    "    def upload_to_ceph(self, dataframe, s3_path, filename):\n",
    "        \"\"\"\n",
    "        This helper function takes as input the data frame to be uploaded, and the output filename.\n",
    "        It then saves the data frame in the defined ceph bucket.\n",
    "        \"\"\"\n",
    "        parquet_buffer = BytesIO()\n",
    "        dataframe.to_parquet(parquet_buffer)\n",
    "        s3_obj = self.s3_resource.Object(self.bucket, f\"{s3_path}/{filename}\")\n",
    "        status = s3_obj.put(Body=parquet_buffer.getvalue())\n",
    "        return status\n",
    "\n",
    "    def read_from_ceph(self, s3_path, filename):\n",
    "        \"\"\"\n",
    "        Helper function to read from ceph and see if the saved data is correct.\n",
    "        \"\"\"\n",
    "        buffer = BytesIO()\n",
    "        s3_object = self.s3_resource.Object(self.bucket, f\"{s3_path}/{filename}\")\n",
    "        s3_object.download_fileobj(buffer)\n",
    "        df_temp = pd.read_parquet(buffer)\n",
    "        return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_disk(dataframe, path, filename):\n",
    "    \"\"\"\n",
    "    Helper function to save the dataframe\n",
    "    as a parquet file to disk.\n",
    "    \"\"\"\n",
    "    dataset_base_path = Path(path)\n",
    "    dataset_base_path.mkdir(parents=True, exist_ok=True)\n",
    "    dataframe.to_parquet(f\"{path}/{filename}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def read_from_disk(path, filename):\n",
    "    \"\"\"\n",
    "    Helper function to read from disk and see if the saved data is the same.\n",
    "    \"\"\"\n",
    "    return pd.read_parquet(f\"{path}/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flakiness detection functions\n",
    "\n",
    "These functions are used in the `./testgrid_flakiness_detection.ipynb` notebook to determine optimal flake detection calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_flake_calc(test_row):\n",
    "    return (\n",
    "        100\n",
    "        * np.logical_or(np.array(test_row) == 12, np.array(test_row) == 13).sum()\n",
    "        / len(test_row)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_consecutive_failures(test_row, i):\n",
    "    \"\"\"This function calculates number of consecutive failures\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_row : array\n",
    "            array of test runs with 0, 1, 12 values as not run, pass, fail respectively\n",
    "    i : float, int\n",
    "            index in array i\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    result: int\n",
    "            number of consecutive failures from index i\n",
    "    \"\"\"\n",
    "\n",
    "    result = 0\n",
    "    while i < len(test_row) and (test_row[i] == 12 or test_row[i] == 0):\n",
    "        if test_row[i] == 12:\n",
    "            result += 1\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def calc_flakiness_score(test_row, ignore_failures_in_a_row=3):\n",
    "    \"\"\"This function calculates flakiness score as the number of edges divided by total runs.\n",
    "    At google, If the test is failing three times in a row, then only it reported as real failures;\n",
    "    otherwise, it's considered a flaky test.\n",
    "    (https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html)\n",
    "    Hence, we ignored three or more than three consecutive failures\n",
    "    and test cases that are not run while calculating the flakiness score.\n",
    "\n",
    "    We always consider label 13 as an edge.\n",
    "    since currently, each failed test is retry, and if it's passed on a subsequent run it is considered as flaky.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_row : array\n",
    "            array of test runs with 0, 1, 12, 13 values as not run, pass, fail, flaky respectively\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    flakiness: int\n",
    "            Flakiness score lies between 0 and 100; 0 is no flakiness, and 100 is maximum flakiness.\n",
    "    \"\"\"\n",
    "    flips = 0\n",
    "    i = 0\n",
    "    ## ignore not run instances in the starting\n",
    "    while i < len(test_row) and test_row[i] == 0:\n",
    "        i += 1\n",
    "\n",
    "    ## intializing last_passing variable value\n",
    "    if i >= len(test_row):\n",
    "        return 0\n",
    "    elif test_row[i] == 1:\n",
    "        last_passing = True\n",
    "    elif test_row[i] == 13:\n",
    "        last_passing = True\n",
    "        flips += 1\n",
    "    elif test_row[i] == 12:\n",
    "        last_passing = False\n",
    "    else:\n",
    "        last_passing = True\n",
    "    considerd = 1\n",
    "    i += 1\n",
    "\n",
    "    while i < len(test_row):\n",
    "        ## ignoring more than three consecutive failures\n",
    "        ## If the test is consecutively failing for three or more than three runs,\n",
    "        ## we considered did not consider it an edge.\n",
    "        cf = calc_consecutive_failures(test_row, i)\n",
    "        if cf >= ignore_failures_in_a_row:\n",
    "            i = i + cf\n",
    "            if i >= len(test_row):\n",
    "                break\n",
    "\n",
    "        s = test_row[i]\n",
    "        if s == 1:\n",
    "            ## run is pass\n",
    "            considerd += 1\n",
    "            last_passing = True\n",
    "        elif s == 0:\n",
    "            ## not run\n",
    "            pass\n",
    "        elif s == 13:\n",
    "            ## flaky\n",
    "            flips += 1\n",
    "            considerd += 1\n",
    "            last_passing = True\n",
    "        elif s == 12:\n",
    "            ## run is fail\n",
    "            considerd += 1\n",
    "            if last_passing:\n",
    "                flips += 1\n",
    "            last_passing = False\n",
    "        i += 1\n",
    "    if considerd == 0:\n",
    "        return 0\n",
    "    ## multiplying by 2 since flakiness score lies between 0 and 100\n",
    "    flakiness = (flips / (considerd)) * 100\n",
    "    return flakiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_flake_edges(test_array):\n",
    "    \"\"\"This function calculates the number of edges, the transition of a\n",
    "    particular test case from pass to fail.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_array : numpy array\n",
    "            array of test runs with 0, 1, 12, 13 values as not run, pass, fail, flaky respectively\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    flake_edges: numpy array\n",
    "                        array where the value are the starting of the edge\n",
    "\n",
    "    \"\"\"\n",
    "    flake_edges = []  ## array to store edges\n",
    "    ignore_failures_in_a_row = 3\n",
    "    i = 0\n",
    "    valid = 0\n",
    "    while i < len(test_array) and test_array[i] == 0:\n",
    "        i += 1\n",
    "    ## intializing last_passing variable value\n",
    "    if i >= len(test_array):\n",
    "        return 0\n",
    "    elif test_array[i] == 1:\n",
    "        last_passing = True\n",
    "        valid = i\n",
    "    elif test_array[i] == 13:\n",
    "        last_passing = True\n",
    "    elif test_array[i] == 12:\n",
    "        last_passing = False\n",
    "    else:\n",
    "        last_passing = True\n",
    "    i += 1\n",
    "\n",
    "    ## Finding all the edges in our test runs\n",
    "    while i < len(test_array):\n",
    "        ## ignoring more than three consecutive failures\n",
    "        ## If the test is consecutively failing for three or more than three runs,\n",
    "        ## we do not consider it an edge.\n",
    "        cf = calc_consecutive_failures(test_array, i)\n",
    "        if cf >= ignore_failures_in_a_row:\n",
    "            i = i + cf\n",
    "            if i >= len(test_array):\n",
    "                break\n",
    "\n",
    "        s = test_array[i]\n",
    "        if s == 1:\n",
    "            ## run is pass\n",
    "            last_passing = True\n",
    "            valid = i\n",
    "        elif s == 0:\n",
    "            ## not run\n",
    "            pass\n",
    "        elif s == 13:\n",
    "            last_passing = True\n",
    "            flake_edges.append(i)\n",
    "        elif s == 12:\n",
    "            ## run is fail\n",
    "            if last_passing:\n",
    "                flake_edges.append(valid)\n",
    "            last_passing = False\n",
    "        i = i + 1\n",
    "    return flake_edges\n",
    "\n",
    "\n",
    "def flake_edge_end(test_array, flake_edges, index):\n",
    "    \"\"\"This function calculates the end of the edges. Starting of the edge will always be 1 or 13\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_array : numpy array\n",
    "            array of test runs with 0, 1, 12, 13 values as not run, pass, fail, flaky respectively\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    flake_edges: numpy array\n",
    "                        array where the value are the starting of the edge\n",
    "\n",
    "    \"\"\"\n",
    "    flake_end = flake_edges[index]\n",
    "    while test_array[flake_end] != 12 and test_array[flake_end] != 13:\n",
    "        flake_end = flake_end + 1\n",
    "        if flake_end > len(test_array):\n",
    "            break\n",
    "    return flake_end\n",
    "\n",
    "\n",
    "def calc_optimal_flakiness_score(test_array, threshold=30):\n",
    "    \"\"\"Calculate the flakiness score between edges since it will maximize the flakiness score.\n",
    "    Specifically, we calculate the flakiness score between the two farthest edges\n",
    "    which have a flakiness score greater than a threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_array : array\n",
    "            array of test runs with 0, 1, 12, 13 values as not run, pass, fail, flaky respectively\n",
    "    threshold: int default 30\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    modified_test_array: numpy array\n",
    "                        modified test grid where the value of failure due to flake is 13\n",
    "    flake_edges_dict: dictionary\n",
    "            key of the dictionary is a tuple of time period, and value is the flakiness score between the time period\n",
    "    \"\"\"\n",
    "    modified_test_array = test_array.copy()\n",
    "    flake_edges_dict = {}\n",
    "    flake_edges = calc_flake_edges(test_array)\n",
    "    ## flakiness score between the two farthest edges\n",
    "    p = 0\n",
    "    q = 0\n",
    "    while p < len(flake_edges):\n",
    "        possible_flake = False\n",
    "        for q in range(p + 1, len(flake_edges)):\n",
    "            flake_end = flake_edge_end(test_array, flake_edges, q)\n",
    "            curr_flake = calc_flakiness_score(\n",
    "                test_array[flake_edges[p] : flake_end + 1]\n",
    "            )\n",
    "            if curr_flake > threshold:\n",
    "                possible_flake = True\n",
    "                max_flake = curr_flake\n",
    "                max_p = flake_edges[p]\n",
    "                max_q = flake_end\n",
    "            else:\n",
    "                break\n",
    "        p = q\n",
    "        if possible_flake:\n",
    "            for k in range(max_p, flake_end + 1):\n",
    "                if modified_test_array[k] == 12:\n",
    "                    modified_test_array[k] = 13\n",
    "                    curr = k + 1\n",
    "                    while curr < len(modified_test_array):\n",
    "                        if modified_test_array[curr] == 12:\n",
    "                            modified_test_array[curr] = 13\n",
    "                        else:\n",
    "                            break\n",
    "            arr = []\n",
    "            arr.append(max_p)\n",
    "            arr.append(max_q)\n",
    "            flake_edges_dict[tuple(arr)] = max_flake\n",
    "        if p == len(flake_edges) - 1:\n",
    "            break\n",
    "    return modified_test_array, flake_edges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flake_annotation(val_array, flake_score, flake_score_threshold):\n",
    "    # for illustration purposes, we are removing the flaky labels\n",
    "    val_array = [12 if (x == 13) else x for x in val_array]\n",
    "    if flake_score > flake_score_threshold:\n",
    "        return [13 if (x == 12) else x for x in val_array]\n",
    "    else:\n",
    "        return val_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure type classification functions\n",
    "\n",
    "These functions are used in `./failure_type_classifier.ipynb` to detect different types of flakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_infra_flake(data, grid, tab_name, grid_name):\n",
    "    \"\"\"\n",
    "    This function takes a 2d numpy array \"grid\" and uses a diagonal edge detecting\n",
    "    filter to identify time windows in which 'infrastructure flakes' occured.\n",
    "\n",
    "    Returns a list of dates and test indexes\n",
    "    \"\"\"\n",
    "    infra_flakes_found = []\n",
    "\n",
    "    # 2d filter that will have its highest value when convolved with a diagonal pattern.\n",
    "    infra_flake_filter = np.array([[-1, 1], [1, -1]])\n",
    "\n",
    "    # Find the spots on the map where the convolution had its maximum value.\n",
    "    spots = convolve2d(infra_flake_filter, grid, mode=\"valid\")\n",
    "    infra_flakes = np.where(spots == 4)\n",
    "\n",
    "    dates = data[tab_name][grid_name][\"timestamps\"]\n",
    "    infra_flake_dates = np.array(dates)[list([infra_flakes][0][1])]\n",
    "    infra_flake_dates = [\n",
    "        datetime.date.fromtimestamp(x // 1000) for x in infra_flake_dates\n",
    "    ]\n",
    "\n",
    "    infra_flake_tests = list([infra_flakes][0][0])\n",
    "\n",
    "    infra_flakes_found = list(zip(infra_flake_dates, infra_flake_tests))\n",
    "\n",
    "    return infra_flakes_found\n",
    "\n",
    "\n",
    "def detect_install_flake(data, grid, tab_name, grid_name):\n",
    "    \"\"\"\n",
    "    If greater than 90% of tests are not run for 2 or more consecutive days,\n",
    "    then we will record this period as as an install flake.\n",
    "    \"\"\"\n",
    "    install_flakes = []\n",
    "\n",
    "    n_rows, n_cols = grid.shape\n",
    "    grid = pd.DataFrame(grid)\n",
    "    not_run_percent = grid.apply(lambda x: sum(x == 0) / n_rows, axis=0)\n",
    "    install_errors = not_run_percent > 0.90\n",
    "    install_error_streaks = run_length_encode(install_errors)\n",
    "\n",
    "    for i in install_error_streaks:\n",
    "        if i[0] is True and i[1] >= 2:\n",
    "            install_flakes.append((i[2] - i[1], i[2]))\n",
    "\n",
    "    dates = data[tab_name][grid_name][\"timestamps\"]\n",
    "    install_flake_dates = []\n",
    "    if install_flakes:\n",
    "        install_flake_dates = np.array(dates)[list([install_flakes][0][0])]\n",
    "        install_flake_dates = [\n",
    "            datetime.date.fromtimestamp(x // 1000) for x in install_flake_dates\n",
    "        ]\n",
    "\n",
    "    return install_flake_dates\n",
    "\n",
    "\n",
    "def detect_new_test_failures(data, grid, tab_name, grid_name):\n",
    "    \"\"\"\n",
    "    If 6 or more consecutive failures occur, then we will record this period\n",
    "    as a new test failure\n",
    "    \"\"\"\n",
    "    grid = pd.DataFrame(grid)\n",
    "    new_test_failures = grid.apply(single_new_test_failure, axis=1)\n",
    "    none_empties = new_test_failures[new_test_failures.apply(lambda x: len(x)) > 0]\n",
    "\n",
    "    dates = data[tab_name][grid_name][\"timestamps\"]\n",
    "\n",
    "    for i, j in enumerate(none_empties):\n",
    "        none_empties[i] = [np.array(dates)[[x[0], x[1]]] for x in j]\n",
    "\n",
    "        none_empties[i] = [\n",
    "            (\n",
    "                datetime.date.fromtimestamp(x[0] // 1000),\n",
    "                datetime.date.fromtimestamp(x[1] // 1000),\n",
    "            )\n",
    "            for x in none_empties[i]\n",
    "        ]\n",
    "\n",
    "    idx = list(none_empties.index)\n",
    "    new_test_failures = [(idx[i], none_empties[i]) for i in range(len(none_empties))]\n",
    "\n",
    "    return new_test_failures\n",
    "\n",
    "\n",
    "def single_new_test_failure(test):\n",
    "\n",
    "    \"\"\"given a test as an array of values, uses run length encoding to\n",
    "    find occurences of 6 or moe consecutive failures for a test.\"\"\"\n",
    "    new_test_failure = []\n",
    "    rle = run_length_encode(test)\n",
    "\n",
    "    end_of_grid = []\n",
    "    if rle[-1][0] == 0 and rle[-2][0] == -1:\n",
    "        for i, j in reversed(list(enumerate(rle[:-2]))):\n",
    "            if j[0] == 1:\n",
    "                end_of_grid = rle[i:]\n",
    "                break\n",
    "\n",
    "        count = 0\n",
    "        for streak in end_of_grid:\n",
    "            if streak[0] == -1:\n",
    "                count += streak[1]\n",
    "\n",
    "        if count >= 6:\n",
    "            new_test_failure.append((end_of_grid[0][2], end_of_grid[-1][2]))\n",
    "\n",
    "    return new_test_failure\n",
    "\n",
    "\n",
    "def detect_flaky_test(data, grid, tab_name, grid_name):\n",
    "    \"\"\"\n",
    "    Apply run calc_optimal_flakiness_score to out grid\n",
    "    \"\"\"\n",
    "\n",
    "    flaky_tests = []\n",
    "    dates = data[tab_name][grid_name][\"timestamps\"]\n",
    "    for i, j in enumerate(grid):\n",
    "\n",
    "        # use the calc_optimal_flakiness_score function imported from testgrid_flakiness_detection notebook\n",
    "        found_flakes = calc_optimal_flakiness_score(grid[i])\n",
    "        if len(found_flakes[1].keys()) > 0:\n",
    "            times = [np.array(dates)[[x[0], x[1]]] for x in found_flakes[1].keys()]\n",
    "            times = [\n",
    "                (\n",
    "                    datetime.date.fromtimestamp(x[0] // 1000),\n",
    "                    datetime.date.fromtimestamp(x[1] // 1000),\n",
    "                )\n",
    "                for x in times\n",
    "            ]\n",
    "\n",
    "            flaky_tests.append((i, found_flakes[1], times))\n",
    "\n",
    "    return flaky_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_failures(data, grid, tab_name, grid_name):\n",
    "    \"\"\"\n",
    "    This takens in a grid and runs all of our detectors and outputs a report\n",
    "    \"\"\"\n",
    "\n",
    "    failure_report = {}\n",
    "\n",
    "    # use the decode_run_length function imported from TestGrid_EDA notebook\n",
    "    x = np.array(list(pd.DataFrame(grid).statuses.apply(decode_run_length)))\n",
    "\n",
    "    failure_report[\"flaky_tests\"] = detect_flaky_test(data, x, tab_name, grid_name)\n",
    "\n",
    "    x = pd.DataFrame(x).apply(lambda x: [normalize(y) for y in x])\n",
    "    x = np.array(x)\n",
    "\n",
    "    failure_report[\"infra_flake\"] = detect_infra_flake(data, x, tab_name, grid_name)\n",
    "    failure_report[\"install_flake\"] = detect_install_flake(data, x, tab_name, grid_name)\n",
    "    failure_report[\"new_test_failure\"] = detect_new_test_failures(\n",
    "        data, x, tab_name, grid_name\n",
    "    )\n",
    "\n",
    "    return failure_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(results, tab_name, grid_name):\n",
    "    print(\n",
    "        f\"Failure Report for: \\n\\\n",
    "    {tab_name}/{grid_name}\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "\n",
    "    print(\"Flaky Tests:\")\n",
    "    for ft in results[\"flaky_tests\"]:\n",
    "        print(f\"Test number {ft[0]} had flakes at:\")\n",
    "        for i in ft[2]:\n",
    "            print(f\"{i[1]} to {i[0]}\")\n",
    "\n",
    "    print(\"\\b\")\n",
    "    print(\n",
    "        \"Infra Flake:\",\n",
    "    )\n",
    "    for infr in results[\"infra_flake\"]:\n",
    "        print(f\"Test number {infr[1]} had an infra flake at {infr[0]}\")\n",
    "\n",
    "    print(\"\\b\")\n",
    "    print(\"Install Flake:\")\n",
    "    for inst in results[\"install_flake\"]:\n",
    "        print(f\"An install flake started on {inst}\")\n",
    "\n",
    "    print(\"\\b\")\n",
    "    print(\"New Test Failures:\")\n",
    "    for ntf in results[\"new_test_failure\"]:\n",
    "        print(f\"Test number {ntf[0]} had new test failures at:\")\n",
    "        for i in ntf[1]:\n",
    "            print(f\"{i[1]} to {i[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to re-map the values so that the output of the convolution will be more interpretable.\n",
    "def normalize(x):\n",
    "    if x == 1:\n",
    "        return 1\n",
    "    if x == 12:\n",
    "        return -1\n",
    "    if x == 0:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def run_length_encode(x):\n",
    "    \"\"\"run length encoding\"\"\"\n",
    "\n",
    "    rle = []\n",
    "    count = 1\n",
    "    for i, j in enumerate(x):\n",
    "        key = j\n",
    "        if i == len(x) - 1:\n",
    "            rle.append((key, count, i))\n",
    "            break\n",
    "        if key == x[i + 1]:\n",
    "            count += 1\n",
    "        else:\n",
    "            rle.append((key, count, i))\n",
    "            count = 1\n",
    "    return rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary formating of the results dictionary\n",
    "# The results dic contains datetime object and\n",
    "# tuple keys that can't be jsoned. At some point\n",
    "# we'll have to define an output schema.\n",
    "def format_results(results):\n",
    "    for key in results:\n",
    "        results[key] = str(results[key])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
