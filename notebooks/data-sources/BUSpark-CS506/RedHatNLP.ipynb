{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Due 02/28/2021",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT1xSMJ40ewc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb60e847-929a-4672-ffc2-80629889e170"
      },
      "source": [
        "# Web scraping (Kyle, due 02/26/2021)\n",
        "#     BeautifulSoup for web scraping\n",
        "\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import dateutil\n",
        "from dateutil import parser\n",
        "import textblob\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "import requests\n",
        "from textblob import Word\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# upload file\n",
        "#uploaded = files.upload()\n",
        "#read array\n",
        "#df2 = pd.read_csv(io.BytesIO(uploaded['logs - Sheet1.csv']))\n",
        "#turn to flat list\n",
        "#array_pages = df2.to_numpy()\n",
        "#array_pages = np.ndarray.tolist(array_pages)\n",
        "#array_pages = [item for sublist in array_pages for item in sublist]\n",
        "\n",
        "#url core needed to pull\n",
        "website = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com\"\n",
        "base = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/canary-release-openshift-origin-installer-e2e-aws-4.5-cnv/\"\n",
        "ending = \"build-log.txt\"\n",
        "url = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/canary-release-openshift-origin-installer-e2e-aws-4.5-cnv/1300557127638585344/build-log.txt\"\n",
        "page = requests.get(base)    \n",
        "data = page.text\n",
        "soup = BeautifulSoup(data)\n",
        "links = []\n",
        "for link in soup.find_all('a'):\n",
        "    links.append(link.get('href'))\n",
        "links = links[1:-1]\n",
        "\n",
        "final_array = []\n",
        "# create array of urls\n",
        "for x in range(len(links)):\n",
        "  final_array.append(str(website) + str(links[x]) + str(ending))\n",
        "\n",
        "\n",
        "# pull all urls logs and store in 2-d array where array_of_logs[x] is a build-log file and \n",
        "# array_of_logs[x][y] is an individual log line split by new line\n",
        "array_of_logs = []\n",
        "for x in range(len(final_array)):\n",
        "  page = urlopen(final_array[x])\n",
        "  html_bytes = page.read()\n",
        "  array_of_logs.append(str(html_bytes).split('\\\\n'))\n",
        "  \n",
        "# first log\n",
        "print(array_of_logs[0])\n",
        "\n",
        "\n",
        "# Analysis on the log data. Trying to find a framework. API (Ningxiao, Parker, Tianze, Hong)\n",
        "#     Identify limitations with data and potential risks of achieving project goals.\n",
        "\n",
        "\n",
        "# ******* Tianze *******\n",
        "#ignore useless \"Waiting for setup to finish...\"\n",
        "def ignoreWaiting(logs):\n",
        "  for i in range(len(logs)):\n",
        "    if \"Waiting for setup to finish...\" in logs[i]:\n",
        "      logs[i]=\"\"\n",
        "  return logs\n",
        "# ******* Tianze *******\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[\"b'2020/09/26 22:20:37 ci-operator version v20200924-c41f44a\", '2020/09/26 22:20:37 No source defined', '2020/09/26 22:20:37 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.5-ci', '2020/09/26 22:20:37 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-xl3p51qp', '2020/09/26 22:20:37 Running [release-inputs], [images], [release:latest], e2e-aws', '2020/09/26 22:20:37 Creating namespace ci-op-xl3p51qp', '2020/09/26 22:20:37 Setting up pipeline imagestream for the test', '2020/09/26 22:20:37 Created secret e2e-aws-cnv-cluster-profile', '2020/09/26 22:20:37 Created secret pull-secret', '2020/09/26 22:20:37 Created PDB for pods with openshift.io/build.name label', '2020/09/26 22:20:37 Created PDB for pods with created-by-ci label', '2020/09/26 22:20:37 Tagged shared images from ocp/4.5:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-xl3p51qp/stable:${component}', '2020/09/26 22:20:44 Importing release image latest', '2020/09/26 22:21:50 Imported release 4.5.0-0.ci-2020-09-22-154858 created at 2020-09-22 15:56:05 +0000 UTC with 110 images to tag release:latest', '2020/09/26 22:21:50 Acquiring lease for \"aws-quota-slice\"', '2020/09/26 22:21:50 Acquired lease \"c7e13d8f-f2a4-4ba7-91c2-53cd46d5c2eb\" for \"aws-quota-slice\"', '2020/09/26 22:21:50 Executing template e2e-aws', '2020/09/26 22:21:50 Creating or restarting template instance', '2020/09/26 22:21:50 Template instance e2e-aws already deleted, do not need to wait any longer', '2020/09/26 22:21:50 Waiting for template instance to be ready', '2020/09/26 22:21:52 Running pod e2e-aws-cnv', '2020/09/26 22:21:55 Container cli in pod e2e-aws-cnv completed successfully', '2020/09/26 22:21:57 Container cli-tests in pod e2e-aws-cnv completed successfully', '2020/09/26 22:56:18 Container setup in pod e2e-aws-cnv completed successfully', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'Waiting for setup to finish...', 'secret/support created', 'which: no docker in (/tmp/shared:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin:/go/bin)', './hack/deploy.sh', '+ source hack/common.sh', '++ set -e', '++ source hack/defaults', \"+++++ dirname \\\\'hack/defaults[0]\\\\'\", '++++ readlink -e hack/../', '+++ PROJECT_ROOT=/go/src/github.com/kubevirt/hyperconverged-cluster-operator', '+++ source /go/src/github.com/kubevirt/hyperconverged-cluster-operator/hack/config', '++++ KUBEVIRT_VERSION=v0.29.2', '++++ CDI_VERSION=v1.18.0', '++++ NETWORK_ADDONS_VERSION=0.38.0', '++++ SSP_VERSION=v1.0.35', '++++ NMO_VERSION=v0.6.0', '++++ HPPO_VERSION=v0.4.3', '++++ HPP_VERSION=v0.4.0', '++++ CONVERSION_CONTAINER_VERSION=v2.0.0', '++++ VMWARE_CONTAINER_VERSION=v2.0.0-3', '++++ VM_IMPORT_VERSION=v0.0.2', '++++ CONTAINER_REGISTRY=quay.io/kubevirt', '++++ mktemp -d', '+++ TEMP_DIR=/tmp/tmp.rVClBhsVd1', '+++ WAIT_TIMEOUT=450s', '+++ CDI_CONTAINER_REGISTRY=docker.io/kubevirt', '+++ KUBEVIRT_CONTAINER_REGISTRY=docker.io/kubevirt', '+++ NETWORK_ADDONS_CONTAINER_REGISTRY=quay.io/kubevirt', '+++ SSP_CONTAINER_REGISTRY=quay.io/fromani', '+++ CDI_OPERATOR_NAME=cdi-operator', '++++ basename docker.io/kubevirt', '+++ CDI_DOCKER_PREFIX=kubevirt', '+++ CONTROLLER_IMAGE=cdi-controller', '+++ IMPORTER_IMAGE=cdi-importer', '+++ CLONER_IMAGE=cdi-cloner', '+++ APISERVER_IMAGE=cdi-apiserver', '+++ UPLOADPROXY_IMAGE=cdi-uploadproxy', '+++ UPLOADSERVER_IMAGE=cdi-uploadserver', '+++ NETWORK_ADDONS_MULTUS_IMAGE=', '+++ NETWORK_ADDONS_LINUX_BRIDGE_CNI_IMAGE=', '+++ NETWORK_ADDONS_LINUX_BRIDGE_MARKER_IMAGE=', '+++ NETWORK_ADDONS_KUBEMACPOOL_IMAGE=', '+++ NETWORK_ADDONS_NMSTATE_HANDLER_IMAGE=', '+++ NETWORK_ADDONS_OVS_CNI_PLUGIN_IMAGE=', '+++ NETWORK_ADDONS_OVS_CNI_MARKER_IMAGE=', '+++ VM_IMPORT_CONTAINER_REGISTRY=quay.io/kubevirt', '+++ VM_IMPORT_IMAGE=vm-import-operator', '+++ echo docker.io/kubevirt', '+++ grep brew', \"+++ OPERATOR_MANIFESTS=\\\\'https://github.com/kubevirt/kubevirt/releases/download/v0.29.2/kubevirt-operator.yaml\", 'https://github.com/kubevirt/containerized-data-importer/releases/download/v1.18.0/cdi-operator.yaml.j2', 'https://github.com/kubevirt/cluster-network-addons-operator/releases/download/0.38.0/network-addons-config.crd.yaml', 'https://github.com/kubevirt/cluster-network-addons-operator/releases/download/0.38.0/namespace.yaml', 'https://github.com/kubevirt/cluster-network-addons-operator/releases/download/0.38.0/operator.yaml', 'https://github.com/MarSik/kubevirt-ssp-operator/releases/download/v1.0.35/kubevirt-ssp-operator-crd.yaml', 'https://github.com/MarSik/kubevirt-ssp-operator/releases/download/v1.0.35/kubevirt-ssp-operator.yaml', \"https://github.com/kubevirt/vm-import-operator/releases/download/v0.0.2/operator.yaml\\\\'\", \"+++ OPERATOR_CRS=\\\\'https://github.com/kubevirt/kubevirt/releases/download/v0.29.2/kubevirt-cr.yaml\", 'https://github.com/kubevirt/containerized-data-importer/releases/download/v1.18.0/cdi-operator-cr.yaml', 'https://github.com/kubevirt/cluster-network-addons-operator/releases/download/0.38.0/network-addons-config-example.cr.yaml', 'https://github.com/MarSik/kubevirt-ssp-operator/releases/download/v1.0.35/kubevirt-ssp-operator-cr.yaml', \"https://github.com/kubevirt/vm-import-operator/releases/download/v0.0.2/vmimportconfig_cr.yaml\\\\'\", '++ source cluster/kubevirtci.sh', '+++ export KUBEVIRT_PROVIDER=k8s-1.17', '+++ KUBEVIRT_PROVIDER=k8s-1.17', '+++ KUBEVIRTCI_VERSION=9d224d0c22e9ed2ca7588ccf3a258d82e160b195', '+++ KUBEVIRTCI_PATH=/go/src/github.com/kubevirt/hyperconverged-cluster-operator/_kubevirtci', '++ CDI_OPERATOR_URL=https://github.com/kubevirt/containerized-data-importer/releases/download/v1.18.0/cdi-operator.yaml', '++ KUBEVIRT_OPERATOR_URL=https://github.com/kubevirt/kubevirt/releases/download/v0.29.2/kubevirt-operator.yaml', '++ CNA_URL_PREFIX=https://github.com/kubevirt/cluster-network-addons-operator/releases/download/0.38.0', '++ mem_size=5120M', '++ num_nodes=1', '++ KUBEVIRT_PROVIDER=k8s-1.17', '++ BASE_PATH=/go/src/github.com/kubevirt/hyperconverged-cluster-operator', '+++ kubevirtci::path', '+++ echo -n /go/src/github.com/kubevirt/hyperconverged-cluster-operator/_kubevirtci', '++ KUBEVIRTCI_PATH=/go/src/github.com/kubevirt/hyperconverged-cluster-operator/_kubevirtci', '++ CMD=', '++ KUBECTL=', '++ TEST_PATH=tests/func-tests', '++ TEST_OUT_PATH=tests/func-tests/_out', '++ JOB_TYPE=prow', '++ SSP_URL_PREFIX=https://github.com/MarSik/kubevirt-ssp-operator/releases/download/v1.0.35', '++ VM_IMPORT_URL_PREFIX=https://github.com/kubevirt/vm-import-operator/releases/download/v0.0.2', '+++ which kubectl', '++ KUBECTL=/tmp/shared/kubectl', \"++ \\\\'[\\\\' -z \\\\'\\\\' \\\\']\\\\'\", \"++ \\\\'[\\\\' -z /tmp/shared/kubectl \\\\']\\\\'\", '++ CMD=kubectl', '+ HCO_IMAGE=quay.io/kubevirt/hyperconverged-cluster-operator:latest', '+ HCO_NAMESPACE=kubevirt-hyperconverged', '+ HCO_KIND=hyperconvergeds', '+ HCO_RESOURCE_NAME=kubevirt-hyperconverged', '+ CI=', \"+ \\\\'[\\\\' \\\\'\\\\' == CI \\\\']\\\\'\", \"+ \\\\'[\\\\' e2e-aws-cnv == hco-e2e-aws \\\\']\\\\'\", \"+ \\\\'[\\\\' e2e-aws-cnv == e2e-aws-cnv \\\\']\\\\'\", \"+ echo \\\\'deploying on AWS CI\\\\'\", 'deploying on AWS CI', '+ CI=true', '+ rm -rf _out/', '+ cp -r deploy _out/', \"+ \\\\'[\\\\' -n \\\\'registry.svc.ci.openshift.org/ci-op-xl3p51qp/stable:${component}\\\\' \\\\']\\\\'\", '+ component=hyperconverged-cluster-operator', \"++ eval echo \\\\'registry.svc.ci.openshift.org/ci-op-xl3p51qp/stable:${component}\\\\'\", '+++ echo registry.svc.ci.openshift.org/ci-op-xl3p51qp/stable:hyperconverged-cluster-operator', '+ HCO_IMAGE=registry.svc.ci.openshift.org/ci-op-xl3p51qp/stable:hyperconverged-cluster-operator', \"+ sed -i \\\\'s|image: quay.io/kubevirt/hyperconverged-cluster-operator:.*$|image: registry.svc.ci.openshift.org/ci-op-xl3p51qp/stable:hyperconverged-cluster-operator|g\\\\' _out/operator.yaml\", '+ kubectl create ns kubevirt-hyperconverged', '+ true', '+ namespaces=(\"openshift\")', \"+ for namespace in \\\\'${namespaces[@]}\\\\'\", '++ kubectl get ns openshift', '+ [[ NAME        STATUS   AGE', \"openshift   Active   18m == \\\\'\\\\' ]]\", \"+ \\\\'[\\\\' kubectl == oc \\\\']\\\\'\", '++ kubectl config current-context', '+ kubectl config set-context admin --namespace=kubevirt-hyperconverged', 'Context \"admin\" modified.', '+ trap status EXIT', '+ CONTAINER_ERRORED=', '+ kubectl create -f _out/cluster_role.yaml', 'role.rbac.authorization.k8s.io/cluster-network-addons-operator created', 'role.rbac.authorization.k8s.io/kubevirt-operator created', 'role.rbac.authorization.k8s.io/cdi-operator created', 'role.rbac.authorization.k8s.io/hostpath-provisioner-operator created', 'clusterrole.rbac.authorization.k8s.io/hyperconverged-cluster-operator created', 'clusterrole.rbac.authorization.k8s.io/cluster-network-addons-operator created', 'clusterrole.rbac.authorization.k8s.io/kubevirt-operator created', 'clusterrole.rbac.authorization.k8s.io/kubevirt-ssp-operator created', 'clusterrole.rbac.authorization.k8s.io/cdi-operator created', 'clusterrole.rbac.authorization.k8s.io/node-maintenance-operator created', 'clusterrole.rbac.authorization.k8s.io/hostpath-provisioner-operator created', 'clusterrole.rbac.authorization.k8s.io/vm-import-operator created', '+ kubectl create -f _out/service_account.yaml', 'serviceaccount/cdi-operator created', 'serviceaccount/cluster-network-addons-operator created', 'serviceaccount/hostpath-provisioner-operator created', 'serviceaccount/hyperconverged-cluster-operator created', 'serviceaccount/kubevirt-operator created', 'serviceaccount/kubevirt-ssp-operator created', 'serviceaccount/node-maintenance-operator created', 'serviceaccount/vm-import-operator created', '+ kubectl create -f _out/cluster_role_binding.yaml', 'rolebinding.rbac.authorization.k8s.io/cluster-network-addons-operator created', 'rolebinding.rbac.authorization.k8s.io/kubevirt-operator created', 'rolebinding.rbac.authorization.k8s.io/cdi-operator created', 'rolebinding.rbac.authorization.k8s.io/hostpath-provisioner-operator created', 'clusterrolebinding.rbac.authorization.k8s.io/hyperconverged-cluster-operator created', 'clusterrolebinding.rbac.authorization.k8s.io/cluster-network-addons-operator created', 'clusterrolebinding.rbac.authorization.k8s.io/kubevirt-operator created', 'clusterrolebinding.rbac.authorization.k8s.io/kubevirt-ssp-operator created', 'clusterrolebinding.rbac.authorization.k8s.io/cdi-operator created', 'clusterrolebinding.rbac.authorization.k8s.io/node-maintenance-operator created', 'clusterrolebinding.rbac.authorization.k8s.io/hostpath-provisioner-operator created', 'clusterrolebinding.rbac.authorization.k8s.io/vm-import-operator created', '+ kubectl create -f _out/crds/', 'customresourcedefinition.apiextensions.k8s.io/networkaddonsconfigs.networkaddonsoperator.network.kubevirt.io created', 'customresourcedefinition.apiextensions.k8s.io/cdis.cdi.kubevirt.io created', 'customresourcedefinition.apiextensions.k8s.io/hyperconvergeds.hco.kubevirt.io created', 'customresourcedefinition.apiextensions.k8s.io/v2vvmwares.v2v.kubevirt.io created', 'customresourcedefinition.apiextensions.k8s.io/ovirtproviders.v2v.kubevirt.io created', 'customresourcedefinition.apiextensions.k8s.io/hostpathprovisioners.hostpathprovisioner.kubevirt.io created', 'customresourcedefinition.apiextensions.k8s.io/kubevirts.kubevirt.io created', 'customresourcedefinition.apiextensions.k8s.io/nodemaintenances.nodemaintenance.kubevirt.io created', 'customresourcedefinition.apiextensions.k8s.io/kubevirtcommontemplatesbundles.ssp.kubevirt.io created', 'customresourcedefinition.apiextensions.k8s.io/kubevirtmetricsaggregations.ssp.kubevirt.io created', 'customresourcedefinition.apiextensions.k8s.io/kubevirtnodelabellerbundles.ssp.kubevirt.io created', 'customresourcedefinition.apiextensions.k8s.io/kubevirttemplatevalidators.ssp.kubevirt.io created', 'customresourcedefinition.apiextensions.k8s.io/vmimportconfigs.v2v.kubevirt.io created', \"+ \\\\'[\\\\' true \\\\'!=\\\\' true \\\\']\\\\'\", \"+ sed -E \\\\'s|^(\\\\\\\\s*)- name: KVM_EMULATION$|\\\\\\\\1- name: KVM_EMULATION\\\\\", '\\\\\\\\1  value: \"true\"|\\\\\\'', '+ cat _out/operator-ci.yaml', '---', 'apiVersion: apps/v1', 'kind: Deployment', 'metadata:', '  labels:', '    name: hyperconverged-cluster-operator', '  name: hyperconverged-cluster-operator', 'spec:', '  replicas: 1', '  selector:', '    matchLabels:', '      name: hyperconverged-cluster-operator', '  strategy: {}', '  template:', '    metadata:', '      labels:', '        name: hyperconverged-cluster-operator', '    spec:', '      containers:', '      - command:', '        - hyperconverged-cluster-operator', '        env:', '        - name: KVM_EMULATION', '          value: \"true\"', '        - name: OPERATOR_IMAGE', '          value: quay.io/kubevirt/hyperconverged-cluster-operator:1.1.0', '        - name: OPERATOR_NAME', '          value: hyperconverged-cluster-operator', '        - name: OPERATOR_NAMESPACE', '          value: kubevirt-hyperconverged', '        - name: POD_NAME', '          valueFrom:', '            fieldRef:', '              fieldPath: metadata.name', '        - name: WATCH_NAMESPACE', '        - name: CONVERSION_CONTAINER', '          value: quay.io/kubevirt/kubevirt-v2v-conversion:v2.0.0', '        - name: VMWARE_CONTAINER', '          value: quay.io/kubevirt/kubevirt-vmware:v2.0.0-3', '        - name: SMBIOS', '          value: |-', '+ kubectl create -f _out/operator-ci.yaml', '            Family: KubeVirt', '            Manufacturer: KubeVirt', '            Product: None', '        - name: MACHINETYPE', '        - name: HCO_KV_IO_VERSION', '          value: 1.1.0', '        - name: KUBEVIRT_VERSION', '          value: v0.29.2', '        - name: CDI_VERSION', '          value: v1.18.0', '        - name: NETWORK_ADDONS_VERSION', '          value: 0.38.0', '        - name: SSP_VERSION', '          value: v1.0.35', '        - name: NMO_VERSION', '          value: v0.6.0', '        - name: HPPO_VERSION', '          value: v0.4.3', '        - name: VM_IMPORT_VERSION', '          value: v0.0.2', '        image: registry.svc.ci.openshift.org/ci-op-xl3p51qp/stable:hyperconverged-cluster-operator', '        imagePullPolicy: IfNotPresent', '        name: hyperconverged-cluster-operator', '        readinessProbe:', '          exec:', '            command:', '            - stat', '            - /tmp/operator-sdk-ready', '          failureThreshold: 1', '          initialDelaySeconds: 5', '          periodSeconds: 5', '        resources: {}', '      serviceAccountName: hyperconverged-cluster-operator', '---', 'apiVersion: apps/v1', 'kind: Deployment', 'metadata:', '  labels:', '    name: cluster-network-addons-operator', '  name: cluster-network-addons-operator', 'spec:', '  replicas: 1', '  selector:', '    matchLabels:', '      name: cluster-network-addons-operator', '  strategy:', '    type: Recreate', '  template:', '    metadata:', '      labels:', '        name: cluster-network-addons-operator', '    spec:', '      containers:', '      - env:', '        - name: MULTUS_IMAGE', '          value: nfvpe/multus:v3.4.1', '        - name: LINUX_BRIDGE_IMAGE', '          value: quay.io/kubevirt/cni-default-plugins:v0.8.1', '        - name: LINUX_BRIDGE_MARKER_IMAGE', '          value: quay.io/kubevirt/bridge-marker:0.2.0', '        - name: NMSTATE_HANDLER_IMAGE', '          value: quay.io/nmstate/kubernetes-nmstate-handler:v0.20.0', '        - name: OVS_CNI_IMAGE', '          value: quay.io/kubevirt/ovs-cni-plugin:v0.11.0', '        - name: OVS_MARKER_IMAGE', '          value: quay.io/kubevirt/ovs-cni-marker:v0.11.0', '        - name: KUBEMACPOOL_IMAGE', '          value: quay.io/kubevirt/kubemacpool:v0.14.0', '        - name: MACVTAP_CNI_IMAGE', '          value: quay.io/kubevirt/macvtap-cni:v0.2.0', '        - name: OPERATOR_IMAGE', '          value: quay.io/kubevirt/cluster-network-addons-operator:0.38.0', '        - name: OPERATOR_NAME', '          value: cluster-network-addons-operator', '        - name: OPERATOR_VERSION', '          value: 0.38.0', '        - name: OPERATOR_NAMESPACE', '          valueFrom:', '            fieldRef:', '              fieldPath: metadata.namespace', '        - name: OPERAND_NAMESPACE', '          valueFrom:', '            fieldRef:', '              fieldPath: metadata.namespace', '        - name: POD_NAME', '          valueFrom:', '            fieldRef:', '              fieldPath: metadata.name', '        - name: WATCH_NAMESPACE', '        image: quay.io/kubevirt/cluster-network-addons-operator:0.38.0', '        imagePullPolicy: IfNotPresent', '        name: cluster-network-addons-operator', '        resources: {}', '      serviceAccountName: cluster-network-addons-operator', '---', 'apiVersion: apps/v1', 'kind: Deployment', 'metadata:', '  labels:', '    name: virt-operator', '  name: virt-operator', 'spec:', '  replicas: 2', '  selector:', '    matchLabels:', '      kubevirt.io: virt-operator', '  strategy:', '    type: RollingUpdate', '  template:', '    metadata:', '      annotations:', '        scheduler.alpha.kubernetes.io/critical-pod: \"\"', '      labels:', '        kubevirt.io: virt-operator', '        prometheus.kubevirt.io: \"\"', '      name: virt-operator', '    spec:', '      affinity:', '        podAntiAffinity:', '          preferredDuringSchedulingIgnoredDuringExecution:', '          - podAffinityTerm:', '              labelSelector:', '                matchExpressions:', '                - key: kubevirt.io', '                  operator: In', '                  values:', '                  - virt-operator', '              topologyKey: kubernetes.io/hostname', '            weight: 1', '      containers:', '      - command:', '        - virt-operator', '        - --port', '        - \"8443\"', '        - -v', '        - \"2\"', '        env:', '        - name: OPERATOR_IMAGE', '          value: docker.io/kubevirt/virt-operator:v0.29.2', '        - name: WATCH_NAMESPACE', '          valueFrom:', '            fieldRef:', \"              fieldPath: metadata.annotations[\\\\'olm.targetNamespaces\\\\']\", '        image: docker.io/kubevirt/virt-operator:v0.29.2', '        imagePullPolicy: IfNotPresent', '        name: virt-operator', '        ports:', '        - containerPort: 8443', '          name: metrics', '          protocol: TCP', '        - containerPort: 8444', '          name: webhooks', '          protocol: TCP', '        readinessProbe:', '          httpGet:', '            path: /metrics', '            port: 8443', '            scheme: HTTPS', '          initialDelaySeconds: 5', '          timeoutSeconds: 10', '        resources: {}', '        volumeMounts:', '        - mountPath: /etc/virt-operator/certificates', '          name: kubevirt-operator-certs', '          readOnly: true', '      priorityClassName: kubevirt-cluster-critical', '      securityContext:', '        runAsNonRoot: true', '      serviceAccountName: kubevirt-operator', '      tolerations:', '      - key: CriticalAddonsOnly', '        operator: Exists', '      volumes:', '      - name: kubevirt-operator-certs', '        secret:', '          optional: true', '          secretName: kubevirt-operator-certs', '---', 'apiVersion: apps/v1', 'kind: Deployment', 'metadata:', '  labels:', '    name: kubevirt-ssp-operator', '  name: kubevirt-ssp-operator', 'spec:', '  replicas: 1', '  selector:', '    matchLabels:', '      name: kubevirt-ssp-operator', '  strategy: {}', '  template:', '    metadata:', '      labels:', '        name: kubevirt-ssp-operator', '    spec:', '      containers:', '      - env:', '        - name: POD_NAME', '          valueFrom:', '            fieldRef:', '              fieldPath: metadata.name', '        - name: IMAGE_REFERENCE', '          value: quay.io/fromani/kubevirt-ssp-operator-container:v1.0.35', '        - name: WATCH_NAMESPACE', '        - name: KVM_INFO_TAG', '        - name: VALIDATOR_TAG', '        - name: VIRT_LAUNCHER_TAG', '        - name: NODE_LABELLER_TAG', '        - name: CPU_PLUGIN_TAG', '        - name: IMAGE_NAME_PREFIX', '        - name: OPERATOR_NAME', '          value: kubevirt-ssp-operator', '        image: quay.io/fromani/kubevirt-ssp-operator-container:v1.0.35', '        imagePullPolicy: Always', '        name: kubevirt-ssp-operator', '        ports:', '        - containerPort: 60000', '          name: metrics', '        resources: {}', '      serviceAccountName: kubevirt-ssp-operator', '---', 'apiVersion: apps/v1', 'kind: Deployment', 'metadata:', '  labels:', '    name: cdi-operator', '  name: cdi-operator', 'spec:', '  replicas: 1', '  selector:', '    matchLabels:', '      name: cdi-operator', '      operator.cdi.kubevirt.io: \"\"', '  strategy: {}', '  template:', '    metadata:', '      labels:', '        name: cdi-operator', '        operator.cdi.kubevirt.io: \"\"', '    spec:', '      containers:', '      - env:', '        - name: DEPLOY_CLUSTER_RESOURCES', '          value: \"true\"', '        - name: OPERATOR_VERSION', '          value: v1.18.0', '        - name: CONTROLLER_IMAGE', '          value: docker.io/kubevirt/cdi-controller:v1.18.0', '        - name: IMPORTER_IMAGE', '          value: docker.io/kubevirt/cdi-importer:v1.18.0', '        - name: CLONER_IMAGE', '          value: docker.io/kubevirt/cdi-cloner:v1.18.0', '        - name: APISERVER_IMAGE', '          value: docker.io/kubevirt/cdi-apiserver:v1.18.0', '        - name: UPLOAD_SERVER_IMAGE', '          value: docker.io/kubevirt/cdi-uploadserver:v1.18.0', '        - name: UPLOAD_PROXY_IMAGE', '          value: docker.io/kubevirt/cdi-uploadproxy:v1.18.0', '        - name: VERBOSITY', '          value: \"1\"', '        - name: PULL_POLICY', '          value: IfNotPresent', '        image: docker.io/kubevirt/cdi-operator:v1.18.0', '        imagePullPolicy: IfNotPresent', '        name: cdi-operator', '        ports:', '        - containerPort: 60000', '          name: metrics', '          protocol: TCP', '        resources: {}', '      securityContext:', '        runAsNonRoot: true', '      serviceAccountName: cdi-operator', '---', 'apiVersion: apps/v1', 'kind: Deployment', 'metadata:', '  labels:', '    name: node-maintenance-operator', '  name: node-maintenance-operator', 'spec:', '  replicas: 1', '  selector:', '    matchLabels:', '      name: node-maintenance-operator', '  strategy: {}', '  template:', '    metadata:', '      labels:', '        name: node-maintenance-operator', '    spec:', '      affinity:', '        nodeAffinity:', '          requiredDuringSchedulingIgnoredDuringExecution:', '            nodeSelectorTerms:', '            - matchExpressions:', '              - key: node-role.kubernetes.io/master', '                operator: Exists', '      containers:', '      - env:', '        - name: WATCH_NAMESPACE', '          valueFrom:', '            fieldRef:', '              fieldPath: metadata.namespace', '        - name: POD_NAME', '          valueFrom:', '            fieldRef:', '              fieldPath: metadata.name', '        - name: OPERATOR_NAME', '          value: node-maintenance-operator', '        image: quay.io/kubevirt/node-maintenance-operator:v0.6.0', '        imagePullPolicy: Always', '        name: node-maintenance-operator', '        resources: {}', '      serviceAccountName: node-maintenance-operator', '      tolerations:', '      - effect: NoSchedule', '        key: node-role.kubernetes.io/master', '---', 'apiVersion: apps/v1', 'kind: Deployment', 'metadata:', '  labels:', '    name: hostpath-provisioner-operator', '  name: hostpath-provisioner-operator', 'spec:', '  replicas: 1', '  selector:', '    matchLabels:', '      name: hostpath-provisioner-operator', '      operator.hostpath-provisioner.kubevirt.io: \"\"', '  strategy: {}', '  template:', '    metadata:', '      labels:', '        name: hostpath-provisioner-operator', '        operator.hostpath-provisioner.kubevirt.io: \"\"', '    spec:', '      containers:', '      - env:', '        - name: WATCH_NAMESPACE', '          valueFrom:', '            fieldRef:', '              fieldPath: metadata.namespace', '        - name: POD_NAME', '          valueFrom:', '            fieldRef:', '              fieldPath: metadata.name', '        - name: OPERATOR_NAME', '          value: hostpath-provisioner-operator', '        - name: PROVISIONER_IMAGE', '          value: quay.io/kubevirt/hostpath-provisioner:v0.4.0', '        - name: PULL_POLICY', '          value: IfNotPresent', '        image: quay.io/kubevirt/hostpath-provisioner-operator:v0.4.3', '        imagePullPolicy: IfNotPresent', '        name: hostpath-provisioner-operator', '        resources: {}', '      serviceAccountName: hostpath-provisioner-operator', '---', 'apiVersion: apps/v1', 'kind: Deployment', 'metadata:', '  labels:', '    name: vm-import-operator', '  name: vm-import-operator', 'spec:', '  replicas: 1', '  selector:', '    matchLabels:', '      name: vm-import-operator', '      operator.v2v.kubevirt.io: \"\"', '  strategy: {}', '  template:', '    metadata:', '      labels:', '        name: vm-import-operator', '        operator.v2v.kubevirt.io: \"\"', '    spec:', '      containers:', '      - env:', '        - name: DEPLOY_CLUSTER_RESOURCES', '          value: \"true\"', '        - name: OPERATOR_VERSION', '          value: v0.0.2', '        - name: CONTROLLER_IMAGE', '          value: quay.io/kubevirt/vm-import-controller:v0.0.2', '        - name: PULL_POLICY', '          value: IfNotPresent', '        - name: WATCH_NAMESPACE', '        - name: POD_NAME', '          valueFrom:', '            fieldRef:', '              fieldPath: metadata.name', '        image: quay.io/kubevirt/vm-import-operator:v0.0.2', '        imagePullPolicy: IfNotPresent', '        name: vm-import-operator', '        resources: {}', '      securityContext:', '        runAsNonRoot: true', '      serviceAccountName: vm-import-operator', 'deployment.apps/hyperconverged-cluster-operator created', 'deployment.apps/cluster-network-addons-operator created', 'deployment.apps/virt-operator created', 'deployment.apps/kubevirt-ssp-operator created', 'deployment.apps/cdi-operator created', 'deployment.apps/node-maintenance-operator created', 'deployment.apps/hostpath-provisioner-operator created', 'deployment.apps/vm-import-operator created', '+ sleep 20', '+ kubectl wait deployment/hyperconverged-cluster-operator --for=condition=Available --timeout=1080s', 'deployment.apps/hyperconverged-cluster-operator condition met', '+ for op in cdi-operator cluster-network-addons-operator kubevirt-ssp-operator node-maintenance-operator vm-import-operator', '+ kubectl wait deployment/cdi-operator --for=condition=Available --timeout=540s', 'deployment.apps/cdi-operator condition met', '+ for op in cdi-operator cluster-network-addons-operator kubevirt-ssp-operator node-maintenance-operator vm-import-operator', '+ kubectl wait deployment/cluster-network-addons-operator --for=condition=Available --timeout=540s', 'deployment.apps/cluster-network-addons-operator condition met', '+ for op in cdi-operator cluster-network-addons-operator kubevirt-ssp-operator node-maintenance-operator vm-import-operator', '+ kubectl wait deployment/kubevirt-ssp-operator --for=condition=Available --timeout=540s', 'deployment.apps/kubevirt-ssp-operator condition met', '+ for op in cdi-operator cluster-network-addons-operator kubevirt-ssp-operator node-maintenance-operator vm-import-operator', '+ kubectl wait deployment/node-maintenance-operator --for=condition=Available --timeout=540s', 'deployment.apps/node-maintenance-operator condition met', '+ for op in cdi-operator cluster-network-addons-operator kubevirt-ssp-operator node-maintenance-operator vm-import-operator', '+ kubectl wait deployment/vm-import-operator --for=condition=Available --timeout=540s', 'deployment.apps/vm-import-operator condition met', '+ kubectl create -f _out/hco.cr.yaml', 'hyperconverged.hco.kubevirt.io/kubevirt-hyperconverged created', '+ sleep 10', '+ timeout 30m bash -c -- \\\\\\'until kubectl get -n kubevirt-hyperconverged hyperconvergeds kubevirt-hyperconverged -o go-template=\\\\\\'\\\\\\\\\\\\\\'\\\\\\'{{ range .status.conditions }}{{ if eq .type \"Available\" }}{{ .status }}{{ end }}{{ end }}\\\\\\'\\\\\\\\\\\\\\'\\\\\\' | grep True; do sleep 1; done\\\\\\'', 'True', '+ kubectl get -n kubevirt-hyperconverged hyperconvergeds kubevirt-hyperconverged -o \\\\\\'go-template={{ range .status.conditions }}{{ .type }}{{ \"\\\\\\\\t\" }}{{ .status }}{{ \"\\\\\\\\t\" }}{{ .message }}{{ \"\\\\', '\" }}{{ end }}\\\\\\'', 'ReconcileComplete\\\\tTrue\\\\tReconcile completed successfully', 'Available\\\\tTrue\\\\tReconcile completed successfully', 'Progressing\\\\tFalse\\\\tReconcile completed successfully', 'Degraded\\\\tFalse\\\\tReconcile completed successfully', 'Upgradeable\\\\tTrue\\\\tReconcile completed successfully', '+ for dep in cdi-apiserver cdi-deployment cdi-uploadproxy virt-api virt-controller', '+ kubectl wait deployment/cdi-apiserver --for=condition=Available --timeout=360s', 'deployment.apps/cdi-apiserver condition met', '+ for dep in cdi-apiserver cdi-deployment cdi-uploadproxy virt-api virt-controller', '+ kubectl wait deployment/cdi-deployment --for=condition=Available --timeout=360s', 'deployment.apps/cdi-deployment condition met', '+ for dep in cdi-apiserver cdi-deployment cdi-uploadproxy virt-api virt-controller', '+ kubectl wait deployment/cdi-uploadproxy --for=condition=Available --timeout=360s', 'deployment.apps/cdi-uploadproxy condition met', '+ for dep in cdi-apiserver cdi-deployment cdi-uploadproxy virt-api virt-controller', '+ kubectl wait deployment/virt-api --for=condition=Available --timeout=360s', 'deployment.apps/virt-api condition met', '+ for dep in cdi-apiserver cdi-deployment cdi-uploadproxy virt-api virt-controller', '+ kubectl wait deployment/virt-controller --for=condition=Available --timeout=360s', 'deployment.apps/virt-controller condition met', 'SUCCESS', \"+ \\\\'[\\\\' -z \\\\'\\\\' \\\\']\\\\'\", '+ echo SUCCESS', '+ exit 0', '+ status', '+ kubectl get hco -n kubevirt-hyperconverged -o yaml', 'apiVersion: v1', 'items:', '- apiVersion: hco.kubevirt.io/v1alpha1', '  kind: HyperConverged', '  metadata:', '    creationTimestamp: \"2020-09-26T22:56:57Z\"', '    finalizers:', '    - hyperconvergeds.hco.kubevirt.io', '    generation: 1', '    managedFields:', '    - apiVersion: hco.kubevirt.io/v1alpha1', '      fieldsType: FieldsV1', '      fieldsV1:', '        f:spec: {}', '      manager: kubectl', '      operation: Update', '      time: \"2020-09-26T22:56:57Z\"', '    - apiVersion: hco.kubevirt.io/v1alpha1', '      fieldsType: FieldsV1', '      fieldsV1:', '        f:metadata:', '          f:finalizers:', '            .: {}', '            v:\"hyperconvergeds.hco.kubevirt.io\": {}', '        f:status:', '          .: {}', '          f:conditions: {}', '          f:relatedObjects: {}', '          f:versions: {}', '      manager: hyperconverged-cluster-operator', '      operation: Update', '      time: \"2020-09-26T22:59:26Z\"', '    name: kubevirt-hyperconverged', '    namespace: kubevirt-hyperconverged', '    resourceVersion: \"26759\"', '    selfLink: /apis/hco.kubevirt.io/v1alpha1/namespaces/kubevirt-hyperconverged/hyperconvergeds/kubevirt-hyperconverged', '    uid: e2f29e5a-53b2-450e-8c76-656277cd887a', '  spec: {}', '  status:', '    conditions:', '    - lastHeartbeatTime: \"2020-09-26T22:59:26Z\"', '      lastTransitionTime: \"2020-09-26T22:56:58Z\"', '      message: Reconcile completed successfully', '      reason: ReconcileCompleted', '      status: \"True\"', '      type: ReconcileComplete', '    - lastHeartbeatTime: \"2020-09-26T22:59:26Z\"', '      lastTransitionTime: \"2020-09-26T22:59:26Z\"', '      message: Reconcile completed successfully', '      reason: ReconcileCompleted', '      status: \"True\"', '      type: Available', '    - lastHeartbeatTime: \"2020-09-26T22:59:26Z\"', '      lastTransitionTime: \"2020-09-26T22:59:26Z\"', '      message: Reconcile completed successfully', '      reason: ReconcileCompleted', '      status: \"False\"', '      type: Progressing', '    - lastHeartbeatTime: \"2020-09-26T22:59:26Z\"', '      lastTransitionTime: \"2020-09-26T22:58:11Z\"', '      message: Reconcile completed successfully', '      reason: ReconcileCompleted', '      status: \"False\"', '      type: Degraded', '    - lastHeartbeatTime: \"2020-09-26T22:59:26Z\"', '      lastTransitionTime: \"2020-09-26T22:59:26Z\"', '      message: Reconcile completed successfully', '      reason: ReconcileCompleted', '      status: \"True\"', '      type: Upgradeable', '    relatedObjects:', '    - apiVersion: scheduling.k8s.io/v1', '      kind: PriorityClass', '      name: kubevirt-cluster-critical', '      resourceVersion: \"23988\"', '      uid: bc0c51a9-81e3-4caa-8d00-b684c22012ba', '    - apiVersion: v1', '      kind: ConfigMap', '      name: kubevirt-config', '      namespace: kubevirt-hyperconverged', '      resourceVersion: \"23989\"', '      uid: 01231030-c0d7-4191-a19c-25ba0e8cc74e', '    - apiVersion: v1', '      kind: ConfigMap', '      name: kubevirt-storage-class-defaults', '      namespace: kubevirt-hyperconverged', '      resourceVersion: \"23990\"', '      uid: 37aae361-4d68-4629-9d53-7aabb218e263', '    - apiVersion: kubevirt.io/v1alpha3', '      kind: KubeVirt', '      name: kubevirt-kubevirt-hyperconverged', '      namespace: kubevirt-hyperconverged', '      resourceVersion: \"26756\"', '      uid: e852e27d-7c0f-4395-8c74-139e1280e401', '    - apiVersion: cdi.kubevirt.io/v1alpha1', '      kind: CDI', '      name: cdi-kubevirt-hyperconverged', '      resourceVersion: \"25550\"', '      uid: a3376ac7-d538-4b4a-b0da-1ab8eee40888', '    - apiVersion: networkaddonsoperator.network.kubevirt.io/v1alpha1', '      kind: NetworkAddonsConfig', '      name: cluster', '      resourceVersion: \"26566\"', '      uid: e4e358ab-3813-41fc-9dda-740470afd5d7', '    - apiVersion: ssp.kubevirt.io/v1', '      kind: KubevirtCommonTemplatesBundle', '      name: common-templates-kubevirt-hyperconverged', '      namespace: openshift', '      resourceVersion: \"26675\"', '      uid: 443fb1b8-fe95-4eb6-95d2-f4dacb1d1da8', '    - apiVersion: ssp.kubevirt.io/v1', '      kind: KubevirtNodeLabellerBundle', '      name: node-labeller-kubevirt-hyperconverged', '      namespace: kubevirt-hyperconverged', '      resourceVersion: \"25257\"', '      uid: 40479c0a-9f75-427d-864a-5778a95c5b36', '    - apiVersion: ssp.kubevirt.io/v1', '      kind: KubevirtTemplateValidator', '      name: template-validator-kubevirt-hyperconverged', '      namespace: kubevirt-hyperconverged', '      resourceVersion: \"25198\"', '      uid: d530b77f-8263-40e9-8c65-5b1935def74d', '    - apiVersion: ssp.kubevirt.io/v1', '      kind: KubevirtMetricsAggregation', '      name: metrics-aggregation-kubevirt-hyperconverged', '      namespace: kubevirt-hyperconverged', '      resourceVersion: \"24846\"', '      uid: 85399c1d-e618-4979-9831-107bbe88584c', '    - apiVersion: v1', '      kind: ConfigMap', '      name: v2v-vmware', '      namespace: kubevirt-hyperconverged', '      resourceVersion: \"23999\"', '      uid: 2d9695c3-4571-4529-a148-b91b9ec1b099', '    - apiVersion: v2v.kubevirt.io/v1alpha1', '      kind: VMImportConfig', '      name: vmimport-kubevirt-hyperconverged', '      resourceVersion: \"25761\"', '      uid: bebc5dde-e31e-40d7-b8a4-2af7858e1256', '    versions:', '    - name: operator', '      version: 1.1.0', 'kind: List', 'metadata:', '  resourceVersion: \"\"', '  selfLink: \"\"', '+ kubectl get pods -n kubevirt-hyperconverged', 'NAME                                                  READY   STATUS             RESTARTS   AGE', 'bridge-marker-5cgj2                                   1/1     Running            0          2m41s', 'bridge-marker-6gxrq                                   1/1     Running            0          2m41s', 'bridge-marker-6l9lg                                   1/1     Running            0          2m41s', 'bridge-marker-6ttjz                                   1/1     Running            0          2m41s', 'bridge-marker-bn8vh                                   1/1     Running            0          2m41s', 'bridge-marker-gp95h                                   1/1     Running            0          2m41s', 'cdi-apiserver-66c5fb4595-jz8jz                        1/1     Running            0          2m45s', 'cdi-deployment-7f6c498dcf-fk4ff                       1/1     Running            0          2m46s', 'cdi-operator-748565fbd8-l7z5n                         1/1     Running            0          3m12s', 'cdi-uploadproxy-748db7f7c4-kgtvl                      1/1     Running            0          2m46s', 'cluster-network-addons-operator-85ff784c67-ml5d2      1/1     Running            0          3m12s', 'hostpath-provisioner-operator-6bbf5ffd9c-vx2k8        1/1     Running            0          3m11s', 'hyperconverged-cluster-operator-59fd8f5765-m5sgp      1/1     Running            0          3m12s', 'kube-cni-linux-bridge-plugin-46j5b                    1/1     Running            0          2m41s', 'kube-cni-linux-bridge-plugin-4gmts                    1/1     Running            0          2m41s', 'kube-cni-linux-bridge-plugin-96jpq                    1/1     Running            0          2m41s', 'kube-cni-linux-bridge-plugin-vlgg2                    1/1     Running            0          2m41s', 'kube-cni-linux-bridge-plugin-wjvvb                    1/1     Running            0          2m41s', 'kube-cni-linux-bridge-plugin-wvj4q                    1/1     Running            0          2m41s', 'kubemacpool-mac-controller-manager-76596b6f8f-njl5v   1/1     Running            0          2m42s', 'kubemacpool-mac-controller-manager-76596b6f8f-pqzvz   0/1     Running            0          2m42s', 'kubevirt-node-labeller-q2dhw                          1/1     Running            0          2m4s', 'kubevirt-node-labeller-qpgj8                          1/1     Running            0          2m4s', 'kubevirt-node-labeller-xsjz5                          1/1     Running            0          2m4s', 'kubevirt-ssp-operator-55d9699848-hkn85                1/1     Running            0          3m12s', 'nmstate-handler-4qssj                                 1/1     Running            0          2m40s', 'nmstate-handler-kvwdl                                 1/1     Running            0          2m40s', 'nmstate-handler-l5dcp                                 1/1     Running            0          2m40s', 'nmstate-handler-s48bq                                 1/1     Running            0          2m40s', 'nmstate-handler-z54nk                                 1/1     Running            0          2m40s', 'nmstate-handler-zq9bl                                 1/1     Running            0          2m40s', 'node-maintenance-operator-68fb7b4889-dcfdn            1/1     Running            0          3m11s', 'ovs-cni-amd64-6rmvn                                   2/2     Running            0          2m40s', 'ovs-cni-amd64-8z6x2                                   2/2     Running            0          2m40s', 'ovs-cni-amd64-f5jw2                                   2/2     Running            0          2m40s', 'ovs-cni-amd64-f96kj                                   2/2     Running            0          2m40s', 'ovs-cni-amd64-rb8wd                                   2/2     Running            0          2m40s', 'ovs-cni-amd64-vn2xn                                   2/2     Running            0          2m40s', 'virt-api-7cf789bcb8-2b85v                             1/1     Running            0          106s', 'virt-api-7cf789bcb8-dm4qg                             1/1     Running            0          106s', 'virt-controller-7557668d6b-95kz4                      1/1     Running            0          70s', 'virt-controller-7557668d6b-h2trd                      1/1     Running            0          70s', 'virt-handler-495rk                                    1/1     Running            0          70s', 'virt-handler-6gptn                                    1/1     Running            0          70s', 'virt-handler-85wgv                                    1/1     Running            0          70s', 'virt-operator-6cbcb47b78-9v4tf                        1/1     Running            0          2m30s', 'virt-operator-6cbcb47b78-bkvmj                        1/1     Running            0          2m30s', 'virt-template-validator-b7bcb65d4-gpv92               0/1     CrashLoopBackOff   4          2m5s', 'virt-template-validator-b7bcb65d4-z2xct               0/1     CrashLoopBackOff   3          2m5s', 'vm-import-controller-6c64995476-vtcfb                 1/1     Running            2          2m47s', 'vm-import-operator-6df99df7cd-9chmq                   1/1     Running            0          3m11s', '+ kubectl get hco kubevirt-hyperconverged -n kubevirt-hyperconverged \\\\\\'-o=jsonpath={range .status.conditions[*]}{.type}{\"\\\\\\\\t\"}{.status}{\"\\\\\\\\t\"}{.message}{\"\\\\', '\"}{end}\\\\\\'', 'ReconcileComplete\\\\tTrue\\\\tReconcile completed successfully', 'Available\\\\tTrue\\\\tReconcile completed successfully', 'Progressing\\\\tFalse\\\\tReconcile completed successfully', 'Degraded\\\\tFalse\\\\tReconcile completed successfully', 'Upgradeable\\\\tTrue\\\\tReconcile completed successfully', \"++ kubectl get pods -n kubevirt-hyperconverged \\\\'--field-selector=status.phase!=Running\\\\' -o custom-columns=:metadata.name\", 'which: no docker in (/tmp/shared:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin:/go/bin)', 'eval ./hack/build-tests.sh', 'go: github.com/kubevirt/cluster-network-addons-operator@v0.3.1-0.20200527095331-9cc2867ac8dc requires', '\\\\tgithub.com/operator-framework/operator-sdk@v0.12.0 requires', '\\\\tgithub.com/operator-framework/operator-registry@v1.1.1 requires', '\\\\tbitbucket.org/ww/goautoneg@v0.0.0-20120707110453-75cd24fc2f2c: reading https://api.bitbucket.org/2.0/repositories/ww/goautoneg?fields=scm: 404 Not Found', 'make: *** [build-functest] Error 1', '2020/09/26 22:59:52 Container test in pod e2e-aws-cnv failed, exit code 2, reason Error', '2020/09/26 23:14:27 Copied 112.27MB of artifacts from e2e-aws-cnv to /logs/artifacts/e2e-aws', '2020/09/26 23:14:28 Releasing lease for \"aws-quota-slice\"', '2020/09/26 23:14:28 No custom metadata found and prow metadata already exists. Not updating the metadata.', '2020/09/26 23:14:28 Ran for 53m50s', 'error: some steps failed:', '  * could not run steps: step e2e-aws failed: template pod \"e2e-aws-cnv\" failed: the pod ci-op-xl3p51qp/e2e-aws-cnv failed after 52m26s (failed containers: test): ContainerFailed one or more containers exited', '', 'Container test exited with code 2, reason Error', '---', 'd6b-h2trd                      1/1     Running            0          70s', 'virt-handler-495rk                                    1/1     Running            0          70s', 'virt-handler-6gptn                                    1/1     Running            0          70s', 'virt-handler-85wgv                                    1/1     Running            0          70s', 'virt-operator-6cbcb47b78-9v4tf                        1/1     Running            0          2m30s', 'virt-operator-6cbcb47b78-bkvmj                        1/1     Running            0          2m30s', 'virt-template-validator-b7bcb65d4-gpv92               0/1     CrashLoopBackOff   4          2m5s', 'virt-template-validator-b7bcb65d4-z2xct               0/1     CrashLoopBackOff   3          2m5s', 'vm-import-controller-6c64995476-vtcfb                 1/1     Running            2          2m47s', 'vm-import-operator-6df99df7cd-9chmq                   1/1     Running            0          3m11s', '+ kubectl get hco kubevirt-hyperconverged -n kubevirt-hyperconverged \\\\\\'-o=jsonpath={range .status.conditions[*]}{.type}{\"\\\\\\\\t\"}{.status}{\"\\\\\\\\t\"}{.message}{\"\\\\', '\"}{end}\\\\\\'', 'ReconcileComplete\\\\tTrue\\\\tReconcile completed successfully', 'Available\\\\tTrue\\\\tReconcile completed successfully', 'Progressing\\\\tFalse\\\\tReconcile completed successfully', 'Degraded\\\\tFalse\\\\tReconcile completed successfully', 'Upgradeable\\\\tTrue\\\\tReconcile completed successfully', \"++ kubectl get pods -n kubevirt-hyperconverged \\\\'--field-selector=status.phase!=Running\\\\' -o custom-columns=:metadata.name\", 'which: no docker in (/tmp/shared:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin:/go/bin)', 'eval ./hack/build-tests.sh', 'go: github.com/kubevirt/cluster-network-addons-operator@v0.3.1-0.20200527095331-9cc2867ac8dc requires', '\\\\tgithub.com/operator-framework/operator-sdk@v0.12.0 requires', '\\\\tgithub.com/operator-framework/operator-registry@v1.1.1 requires', '\\\\tbitbucket.org/ww/goautoneg@v0.0.0-20120707110453-75cd24fc2f2c: reading https://api.bitbucket.org/2.0/repositories/ww/goautoneg?fields=scm: 404 Not Found', 'make: *** [build-functest] Error 1', '---', '2020/09/26 23:14:28 could not load result reporting options: failed to read file \"\": open : no such file or directory', \"'\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaYWmzJoL1NL",
        "outputId": "e78f3086-6e78-431e-ca6a-a179efbbe5d8"
      },
      "source": [
        "#url core needed to pull\n",
        "website2 = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com\"\n",
        "base2 = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/release-openshift-ocp-installer-e2e-aws-serial-4.1/\"\n",
        "ending2 = \"build-log.txt\"\n",
        "page2 = requests.get(base2)    \n",
        "data2 = page2.text\n",
        "soup2 = BeautifulSoup(data2)\n",
        "links2 = []\n",
        "for link2 in soup2.find_all('a'):\n",
        "    links2.append(link2.get('href'))\n",
        "links2 = links2[1:-1]\n",
        "\n",
        "final_array2 = []\n",
        "# create array of urls\n",
        "for x in range(len(links2)):\n",
        "  final_array2.append(str(website2) + str(links2[x]) + str(ending2))\n",
        "\n",
        "\n",
        "# pull all urls logs and store in 2-d array where array_of_logs[x] is a build-log file and \n",
        "# array_of_logs[x][y] is an individual log line split by new line\n",
        "array_of_logs2 = []\n",
        "for x in range(len(final_array2)):\n",
        "  page2 = urlopen(final_array2[x])\n",
        "  html_bytes2 = page2.read()\n",
        "  array_of_logs2.append(str(html_bytes2).split('\\\\n'))\n",
        "  \n",
        "# first log\n",
        "print(array_of_logs2[0])\n",
        "\n",
        "\n",
        "# Analysis on the log data. Trying to find a framework. API (Ningxiao, Parker, Tianze, Hong)\n",
        "#     Identify limitations with data and potential risks of achieving project goals.\n",
        "\n",
        "\n",
        "# ******* Tianze *******\n",
        "#ignore useless \"Waiting for setup to finish...\"\n",
        "def ignoreWaiting(logs):\n",
        "  for i in range(len(logs)):\n",
        "    if \"Waiting for setup to finish...\" in logs[i]:\n",
        "      logs[i]=\"\"\n",
        "  return logs\n",
        "# ******* Tianze *******\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"b'2020/09/26 21:56:38 ci-operator version v20200924-c41f44a\", '2020/09/26 21:56:38 No source defined', '2020/09/26 21:56:38 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1', '2020/09/26 21:56:38 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-l3h5nrkw', '2020/09/26 21:56:38 Running [release-inputs], [images], [release:latest], e2e-aws-serial', '2020/09/26 21:56:38 Creating namespace ci-op-l3h5nrkw', '2020/09/26 21:56:38 Setting up pipeline imagestream for the test', '2020/09/26 21:56:38 Created secret e2e-aws-serial-cluster-profile', '2020/09/26 21:56:38 Created secret pull-secret', '2020/09/26 21:56:38 Created PDB for pods with openshift.io/build.name label', '2020/09/26 21:56:38 Created PDB for pods with created-by-ci label', '2020/09/26 21:56:38 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-l3h5nrkw/stable:${component}', '2020/09/26 21:56:41 Importing release image latest', '2020/09/26 21:57:35 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest', '2020/09/26 21:57:35 Acquiring lease for \"aws-quota-slice\"', '2020/09/26 21:57:35 Acquired lease \"c4382d1d-f6fa-4857-a727-96c202c58026\" for \"aws-quota-slice\"', '2020/09/26 21:57:35 Executing template e2e-aws-serial', '2020/09/26 21:57:36 Creating or restarting template instance', '2020/09/26 21:57:36 Template instance e2e-aws-serial already deleted, do not need to wait any longer', '2020/09/26 21:57:36 Waiting for template instance to be ready', '2020/09/26 21:57:38 Running pod e2e-aws-serial', '2020/09/26 22:29:56 Container setup in pod e2e-aws-serial completed successfully', '2020/09/26 23:44:10 Copied 105.19MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial', '2020/09/26 23:44:10 Releasing lease for \"aws-quota-slice\"', '2020/09/26 23:44:10 No custom metadata found and prow metadata already exists. Not updating the metadata.', '2020/09/26 23:44:10 Ran for 1h47m31s', '2020/09/26 23:44:10 could not load result reporting options: failed to read file \"\": open : no such file or directory', \"'\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRyol0yU4HP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "2622a09d-6a9c-4a36-e2ef-f8d95ee07c83"
      },
      "source": [
        "# ******* PARKER *******\n",
        "#  helper function detecting if a string is a date / timestamp\n",
        "def is_date(str):\n",
        "  try:\n",
        "    dateutil.parser.parse(str)\n",
        "    return True\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "# logs variable contains all parsed logs\n",
        "logs = []\n",
        "for i in range(len(array_of_logs2)):\n",
        "  #  removing newline characters\n",
        "  array_of_logs2[i] = str(array_of_logs[i]).splitlines()\n",
        "  array_of_logs2[i] = str(array_of_logs[i]).replace('\\\\n', ' ')\n",
        "  \n",
        "\n",
        "# removes leading 'b from log\n",
        "  array_of_logs2[i] = array_of_logs[i][0][2:]\n",
        "# splitting each section as its own index (for parsing)\n",
        "  array_of_logs2[i] = str(array_of_logs[i]).split(' ')\n",
        "\n",
        "#  tmp is log without timestamps\n",
        "  tmp = []\n",
        "  for j in range(len(array_of_logs2[i])):\n",
        "    if is_date(array_of_logs2[i][j]) == False:\n",
        "      tmp.append(array_of_logs2[i][j])\n",
        "    else:\n",
        "      continue\n",
        "  logs.append(tmp)\n",
        "\n",
        "# removes whitespace characters and keeps root words\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "for log in logs:\n",
        "  log[:] = [stemmer.stem(x) for x in log if x != '']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e28e3adf42fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_of_logs2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m#  removing newline characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0marray_of_logs2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_of_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0marray_of_logs2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_of_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtFxnlMOmp3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb212c72-e3a1-4b6e-962f-358ac9fa7062"
      },
      "source": [
        "import drain3\n",
        "from drain3 import TemplateMiner\n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "from drain3.kafka_persistence import KafkaPersistence\n",
        "\n",
        "template_miner = TemplateMiner(None)\n",
        "i = 0\n",
        "array_of_logs2 = array_of_logs2[:100]\n",
        "while True:\n",
        "    if i >= len(array_of_logs2):\n",
        "      break\n",
        "    log_line = ' '.join(array_of_logs2[i])\n",
        "    i += 1\n",
        "    if log_line == 'q':\n",
        "        break\n",
        "    result = template_miner.add_log_message(log_line)\n",
        "    result_json = json.dumps(result)\n",
        "    print(result_json)\n",
        "\n",
        "print(\"Clusters:\")\n",
        "for cluster in template_miner.drain.clusters:\n",
        "    print(cluster)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config file not found: drain3.ini\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 1, \"cluster_size\": 1, \"template_mined\": \"b'2020/09/26 21:56:38 ci-operator version v20200924-c41f44a 2020/09/26 21:56:38 No source defined 2020/09/26 21:56:38 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/09/26 21:56:38 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-l3h5nrkw 2020/09/26 21:56:38 Running [release-inputs], [images], [release:latest], e2e-aws-serial 2020/09/26 21:56:38 Creating namespace ci-op-l3h5nrkw 2020/09/26 21:56:38 Setting up pipeline imagestream for the test 2020/09/26 21:56:38 Created secret e2e-aws-serial-cluster-profile 2020/09/26 21:56:38 Created secret pull-secret 2020/09/26 21:56:38 Created PDB for pods with openshift.io/build.name label 2020/09/26 21:56:38 Created PDB for pods with created-by-ci label 2020/09/26 21:56:38 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-l3h5nrkw/stable:${component} 2020/09/26 21:56:41 Importing release image latest 2020/09/26 21:57:35 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/09/26 21:57:35 Acquiring lease for \\\"aws-quota-slice\\\" 2020/09/26 21:57:35 Acquired lease \\\"c4382d1d-f6fa-4857-a727-96c202c58026\\\" for \\\"aws-quota-slice\\\" 2020/09/26 21:57:35 Executing template e2e-aws-serial 2020/09/26 21:57:36 Creating or restarting template instance 2020/09/26 21:57:36 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/09/26 21:57:36 Waiting for template instance to be ready 2020/09/26 21:57:38 Running pod e2e-aws-serial 2020/09/26 22:29:56 Container setup in pod e2e-aws-serial completed successfully 2020/09/26 23:44:10 Copied 105.19MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/09/26 23:44:10 Releasing lease for \\\"aws-quota-slice\\\" 2020/09/26 23:44:10 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/09/26 23:44:10 Ran for 1h47m31s 2020/09/26 23:44:10 could not load result reporting options: failed to read file \\\"\\\": open : no such file or directory '\", \"cluster_count\": 1}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 1, \"cluster_size\": 2, \"template_mined\": \"<*> <*> ci-operator version v20200924-c41f44a <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], [images], [release:latest], e2e-aws-serial <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> <*> <*> could not load result reporting options: failed to read file \\\"\\\": open : no such file or directory '\", \"cluster_count\": 1}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 2, \"cluster_size\": 1, \"template_mined\": \"b'2020/09/28 21:58:42 ci-operator version v20200928-c450a36 2020/09/28 21:58:42 No source defined 2020/09/28 21:58:42 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/09/28 21:58:42 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-485l57ir 2020/09/28 21:58:42 Running [release-inputs], [images], [release:latest], e2e-aws-serial 2020/09/28 21:58:42 Creating namespace ci-op-485l57ir 2020/09/28 21:58:42 Setting up pipeline imagestream for the test 2020/09/28 21:58:42 Created secret e2e-aws-serial-cluster-profile 2020/09/28 21:58:42 Created secret pull-secret 2020/09/28 21:58:42 Created PDB for pods with openshift.io/build.name label 2020/09/28 21:58:42 Created PDB for pods with created-by-ci label 2020/09/28 21:58:42 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-485l57ir/stable:${component} 2020/09/28 21:58:45 Importing release image latest 2020/09/28 21:59:58 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/09/28 21:59:58 Acquiring lease for \\\"aws-quota-slice\\\" 2020/09/28 21:59:58 Acquired lease \\\"864adb84-1287-4fbf-9a0c-adfeb4c73ddb\\\" for \\\"aws-quota-slice\\\" 2020/09/28 21:59:58 Executing template e2e-aws-serial 2020/09/28 21:59:58 Creating or restarting template instance 2020/09/28 21:59:58 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/09/28 21:59:58 Waiting for template instance to be ready 2020/09/28 22:00:00 Running pod e2e-aws-serial 2020/09/28 22:31:33 Container setup in pod e2e-aws-serial completed successfully 2020/09/29 00:02:59 error: unable to gather container logs: [error: Unable to retrieve logs from pod container setup: container \\\"setup\\\" in pod \\\"e2e-aws-serial\\\" is terminated, error: Unable to retrieve logs from pod container teardown: container \\\"teardown\\\" in pod \\\"e2e-aws-serial\\\" is terminated, error: Unable to retrieve logs from pod container test: container \\\"test\\\" in pod \\\"e2e-aws-serial\\\" is terminated] 2020/09/29 00:02:59 error: unable to retrieve artifacts from pod e2e-aws-serial: could not read gzipped artifacts: unable to upgrade connection: container not found (\\\"artifacts\\\") 2020/09/29 00:02:59 Releasing lease for \\\"aws-quota-slice\\\" 2020/09/29 00:02:59 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/09/29 00:02:59 Ran for 2h4m17s 2020/09/29 00:02:59 could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 2}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 3, \"cluster_size\": 1, \"template_mined\": \"b'2020/09/29 21:59:35 ci-operator version v20200929-3687fb5 2020/09/29 21:59:35 No source defined 2020/09/29 21:59:35 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/09/29 21:59:35 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-ct0f2jrf 2020/09/29 21:59:35 Running [release-inputs], [images], [release:latest], e2e-aws-serial 2020/09/29 21:59:35 Creating namespace ci-op-ct0f2jrf 2020/09/29 21:59:35 Setting up pipeline imagestream for the test 2020/09/29 21:59:35 Created secret e2e-aws-serial-cluster-profile 2020/09/29 21:59:35 Created secret pull-secret 2020/09/29 21:59:35 Created PDB for pods with openshift.io/build.name label 2020/09/29 21:59:35 Created PDB for pods with created-by-ci label 2020/09/29 21:59:35 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-ct0f2jrf/stable:${component} 2020/09/29 21:59:37 Importing release image latest 2020/09/29 22:00:28 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/09/29 22:00:28 Acquiring lease for \\\"aws-quota-slice\\\" 2020/09/29 22:00:28 Acquired lease \\\"53162864-5217-47c3-b07a-f56cf02c18e1\\\" for \\\"aws-quota-slice\\\" 2020/09/29 22:00:28 Executing template e2e-aws-serial 2020/09/29 22:00:28 Creating or restarting template instance 2020/09/29 22:00:28 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/09/29 22:00:28 Waiting for template instance to be ready 2020/09/29 22:00:30 Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-west-2 (zones: us-west-2a us-west-2b) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=warning msg=\\\"Found override for ReleaseImage. Please be warned, this is not advised\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Openshift Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Master Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Worker Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Common Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Creating infrastructure resources...\\\" level=error level=error msg=\\\"Error: Error applying plan:\\\" level=error level=error msg=\\\"1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* module.bootstrap.aws_iam_role.bootstrap: 1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* aws_iam_role.bootstrap: Error creating IAM Role ci-op-ct0f2jrf-7bc5c-fdj2p-bootstrap-role: timeout while waiting for state to become \\\\'success\\\\' (timeout: 30s)\\\" level=error level=error level=error level=error level=error level=error msg=\\\"Terraform does not automatically rollback in the face of errors.\\\" level=error msg=\\\"Instead, your Terraform state file has been partially updated with\\\" level=error msg=\\\"any resources that successfully completed. Please address the error\\\" level=error msg=\\\"above and apply again to incrementally change your infrastructure.\\\" level=error level=error level=fatal msg=\\\"failed to fetch Cluster: failed to generate asset \\\\\\\\\\\"Cluster\\\\\\\\\\\": failed to create cluster: failed to apply using Terraform\\\" 2020/09/29 22:09:36 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2020/09/29 22:14:52 Copied 6.95MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/09/29 22:14:52 Releasing lease for \\\"aws-quota-slice\\\" 2020/09/29 22:14:52 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/09/29 22:14:52 Ran for 15m17s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-ct0f2jrf/e2e-aws-serial failed after 14m21s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-west-2 (zones: us-west-2a us-west-2b) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=warning msg=\\\"Found override for ReleaseImage. Please be warned, this is not advised\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Openshift Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Master Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Worker Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Common Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Creating infrastructure resources...\\\" level=error level=error msg=\\\"Error: Error applying plan:\\\" level=error level=error msg=\\\"1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* module.bootstrap.aws_iam_role.bootstrap: 1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* aws_iam_role.bootstrap: Error creating IAM Role ci-op-ct0f2jrf-7bc5c-fdj2p-bootstrap-role: timeout while waiting for state to become \\\\'success\\\\' (timeout: 30s)\\\" level=error level=error level=error level=error level=error level=error msg=\\\"Terraform does not automatically rollback in the face of errors.\\\" level=error msg=\\\"Instead, your Terraform state file has been partially updated with\\\" level=error msg=\\\"any resources that successfully completed. Please address the error\\\" level=error msg=\\\"above and apply again to incrementally change your infrastructure.\\\" level=error level=error level=fatal msg=\\\"failed to fetch Cluster: failed to generate asset \\\\\\\\\\\"Cluster\\\\\\\\\\\": failed to create cluster: failed to apply using Terraform\\\" --- 2020/09/29 22:14:52 could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 3}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 4, \"cluster_size\": 1, \"template_mined\": \"b'2020/09/30 22:00:30 ci-operator version v20200930-8970be3 2020/09/30 22:00:30 No source defined 2020/09/30 22:00:30 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/09/30 22:00:30 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-lgihjhzt 2020/09/30 22:00:30 Running [release-inputs], [images], [release:latest], e2e-aws-serial 2020/09/30 22:00:30 Creating namespace ci-op-lgihjhzt 2020/09/30 22:00:30 Setting up pipeline imagestream for the test 2020/09/30 22:00:30 Created secret e2e-aws-serial-cluster-profile 2020/09/30 22:00:30 Created secret pull-secret 2020/09/30 22:00:30 Created PDB for pods with openshift.io/build.name label 2020/09/30 22:00:30 Created PDB for pods with created-by-ci label 2020/09/30 22:00:30 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-lgihjhzt/stable:${component} 2020/09/30 22:00:32 Importing release image latest 2020/09/30 22:01:25 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/09/30 22:01:25 Acquiring lease for \\\"aws-quota-slice\\\" 2020/09/30 22:01:25 Acquired lease \\\"0574c731-0389-4b44-8a63-e0ce1b9fe94c\\\" for \\\"aws-quota-slice\\\" 2020/09/30 22:01:25 Executing template e2e-aws-serial 2020/09/30 22:01:25 Creating or restarting template instance 2020/09/30 22:01:25 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/09/30 22:01:25 Waiting for template instance to be ready 2020/09/30 22:01:27 Running pod e2e-aws-serial 2020/09/30 22:34:12 warning: failed to update lease \\\"0574c731-0389-4b44-8a63-e0ce1b9fe94c\\\": [Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=0574c731-0389-4b44-8a63-e0ce1b9fe94c&owner=ci-op-lgihjhzt-7bc5c&state=leased\\\": net/http: TLS handshake timeout, Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=0574c731-0389-4b44-8a63-e0ce1b9fe94c&owner=ci-op-lgihjhzt-7bc5c&state=leased\\\": dial tcp: i/o timeout] 2020/09/30 22:36:15 Container setup in pod e2e-aws-serial completed successfully 2020/09/30 22:40:38 warning: failed to update lease \\\"0574c731-0389-4b44-8a63-e0ce1b9fe94c\\\": Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=0574c731-0389-4b44-8a63-e0ce1b9fe94c&owner=ci-op-lgihjhzt-7bc5c&state=leased\\\": net/http: TLS handshake timeout 2020/10/01 00:06:38 Copied 119.25MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/01 00:06:38 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/01 00:06:38 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/01 00:06:38 Ran for 2h6m8s 2020/10/01 00:06:38 could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 4}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 5, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/01 22:01:09 ci-operator version v20201001-619f3c5 2020/10/01 22:01:09 No source defined 2020/10/01 22:01:09 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/01 22:01:09 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-zft6bcr8 2020/10/01 22:01:09 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/01 22:01:09 Creating namespace ci-op-zft6bcr8 2020/10/01 22:01:10 Setting up pipeline imagestream for the test 2020/10/01 22:01:10 Created secret e2e-aws-serial-cluster-profile 2020/10/01 22:01:10 Created secret pull-secret 2020/10/01 22:01:10 Created PDB for pods with openshift.io/build.name label 2020/10/01 22:01:10 Created PDB for pods with created-by-ci label 2020/10/01 22:01:10 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-zft6bcr8/stable:${component} 2020/10/01 22:01:12 Importing release image latest 2020/10/01 22:02:05 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/01 22:02:05 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/01 22:02:06 Acquired lease \\\"3ea7794c-938d-4132-b988-fdf81a308741\\\" for \\\"aws-quota-slice\\\" 2020/10/01 22:02:06 Executing template e2e-aws-serial 2020/10/01 22:02:06 Creating or restarting template instance 2020/10/01 22:02:06 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/01 22:02:06 Waiting for template instance to be ready 2020/10/01 22:02:08 Running pod e2e-aws-serial 2020/10/01 22:45:31 Container setup in pod e2e-aws-serial completed successfully 2020/10/01 23:59:31 Copied 113.32MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/01 23:59:31 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/01 23:59:31 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/01 23:59:31 Ran for 1h58m21s 2020/10/01 23:59:31 could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 5}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 6, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/02 22:02:18 ci-operator version v20201002-9e95fb1 2020/10/02 22:02:18 No source defined 2020/10/02 22:02:18 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/02 22:02:18 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-8b90spl7 2020/10/02 22:02:18 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/02 22:02:18 Creating namespace ci-op-8b90spl7 2020/10/02 22:02:19 Setting up pipeline imagestream for the test 2020/10/02 22:02:19 Created secret e2e-aws-serial-cluster-profile 2020/10/02 22:02:19 Created secret pull-secret 2020/10/02 22:02:19 Created PDB for pods with openshift.io/build.name label 2020/10/02 22:02:19 Created PDB for pods with created-by-ci label 2020/10/02 22:02:19 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-8b90spl7/stable:${component} 2020/10/02 22:02:21 Importing release image latest 2020/10/02 22:03:32 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/02 22:03:32 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/02 22:03:32 Acquired lease \\\"42f2c087-ff9e-4455-be19-eaae74b6271e\\\" for \\\"aws-quota-slice\\\" 2020/10/02 22:03:32 Executing template e2e-aws-serial 2020/10/02 22:03:32 Creating or restarting template instance 2020/10/02 22:03:32 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/02 22:03:32 Waiting for template instance to be ready 2020/10/02 22:03:35 Running pod e2e-aws-serial 2020/10/02 22:37:35 Container setup in pod e2e-aws-serial completed successfully 2020/10/02 22:49:01 warning: failed to update lease \\\"42f2c087-ff9e-4455-be19-eaae74b6271e\\\": Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\\\": net/http: TLS handshake timeout 2020/10/02 22:54:07 warning: failed to update lease \\\"42f2c087-ff9e-4455-be19-eaae74b6271e\\\": Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\\\": net/http: TLS handshake timeout 2020/10/02 22:58:10 warning: failed to update lease \\\"42f2c087-ff9e-4455-be19-eaae74b6271e\\\": Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\\\": net/http: TLS handshake timeout 2020/10/02 23:03:30 warning: failed to update lease \\\"42f2c087-ff9e-4455-be19-eaae74b6271e\\\": Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\\\": net/http: TLS handshake timeout 2020/10/02 23:06:49 warning: failed to update lease \\\"42f2c087-ff9e-4455-be19-eaae74b6271e\\\": Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\\\": net/http: TLS handshake timeout 2020/10/02 23:08:46 warning: failed to update lease \\\"42f2c087-ff9e-4455-be19-eaae74b6271e\\\": Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\\\": net/http: TLS handshake timeout 2020/10/02 23:11:02 warning: failed to update lease \\\"42f2c087-ff9e-4455-be19-eaae74b6271e\\\": Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\\\": net/http: TLS handshake timeout 2020/10/02 23:15:30 warning: failed to update lease \\\"42f2c087-ff9e-4455-be19-eaae74b6271e\\\": Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\\\": net/http: TLS handshake timeout 2020/10/02 23:17:40 warning: failed to update lease \\\"42f2c087-ff9e-4455-be19-eaae74b6271e\\\": Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\\\": net/http: TLS handshake timeout 2020/10/03 00:08:45 Copied 120.71MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/03 00:08:45 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/03 00:08:45 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/03 00:08:45 Ran for 2h6m26s 2020/10/03 00:08:45 could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 6}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 7, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/03 22:11:41 ci-operator version v20201002-9e95fb1 2020/10/03 22:11:41 No source defined 2020/10/03 22:11:41 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/03 22:11:41 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-ztmfi4lp 2020/10/03 22:11:41 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/03 22:11:41 Creating namespace ci-op-ztmfi4lp 2020/10/03 22:11:41 Setting up pipeline imagestream for the test 2020/10/03 22:11:41 Created secret e2e-aws-serial-cluster-profile 2020/10/03 22:11:41 Created secret pull-secret 2020/10/03 22:11:41 Created PDB for pods with openshift.io/build.name label 2020/10/03 22:11:41 Created PDB for pods with created-by-ci label 2020/10/03 22:11:41 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-ztmfi4lp/stable:${component} 2020/10/03 22:11:43 Importing release image latest 2020/10/03 22:12:47 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/03 22:12:47 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/03 22:12:47 Acquired lease \\\"9354bf8f-a2f6-4692-9d2d-a87a6ea02c40\\\" for \\\"aws-quota-slice\\\" 2020/10/03 22:12:47 Executing template e2e-aws-serial 2020/10/03 22:12:47 Creating or restarting template instance 2020/10/03 22:12:48 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/03 22:12:48 Waiting for template instance to be ready 2020/10/03 22:12:50 Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-1 (zones: us-east-1b us-east-1c) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=warning msg=\\\"Found override for ReleaseImage. Please be warned, this is not advised\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Master Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Openshift Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Worker Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Common Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Creating infrastructure resources...\\\" level=info msg=\\\"Waiting up to 30m0s for the Kubernetes API at https://api.ci-op-ztmfi4lp-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443...\\\" level=info msg=\\\"API v1.13.4-138-g41dc99c up\\\" level=info msg=\\\"Waiting up to 30m0s for bootstrapping to complete...\\\" level=info msg=\\\"Destroying the bootstrap resources...\\\" level=info msg=\\\"Waiting up to 30m0s for the cluster at https://api.ci-op-ztmfi4lp-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 to initialize...\\\" level=fatal msg=\\\"failed to initialize the cluster: Multiple errors are preventing progress:\\\\ * Cluster operator monitoring is reporting a failure: Failed to rollout the stack. Error: running task Updating Prometheus-k8s failed: waiting for Prometheus Route to become ready failed: waiting for RouteReady of prometheus-k8s: the server is currently unable to handle the request (get routes.route.openshift.io prometheus-k8s)\\\\ * Cluster operator openshift-apiserver has not yet reported success: timed out waiting for the condition\\\" 2020/10/03 23:04:48 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2020/10/03 23:29:18 Copied 43.26MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/03 23:29:18 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/03 23:29:19 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/03 23:29:19 Ran for 1h17m37s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-ztmfi4lp/e2e-aws-serial failed after 1h16m22s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-1 (zones: us-east-1b us-east-1c) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=warning msg=\\\"Found override for ReleaseImage. Please be warned, this is not advised\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Master Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Openshift Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Worker Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Common Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Creating infrastructure resources...\\\" level=info msg=\\\"Waiting up to 30m0s for the Kubernetes API at https://api.ci-op-ztmfi4lp-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443...\\\" level=info msg=\\\"API v1.13.4-138-g41dc99c up\\\" level=info msg=\\\"Waiting up to 30m0s for bootstrapping to complete...\\\" level=info msg=\\\"Destroying the bootstrap resources...\\\" level=info msg=\\\"Waiting up to 30m0s for the cluster at https://api.ci-op-ztmfi4lp-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 to initialize...\\\" level=fatal msg=\\\"failed to initialize the cluster: Multiple errors are preventing progress:\\\\ * Cluster operator monitoring is reporting a failure: Failed to rollout the stack. Error: running task Updating Prometheus-k8s failed: waiting for Prometheus Route to become ready failed: waiting for RouteReady of prometheus-k8s: the server is currently unable to handle the request (get routes.route.openshift.io prometheus-k8s)\\\\ * Cluster operator openshift-apiserver has not yet reported success: timed out waiting for the condition\\\" --- 2020/10/03 23:29:19 could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 7}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 8, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/04 22:03:44 ci-operator version v20201002-9e95fb1 2020/10/04 22:03:44 No source defined 2020/10/04 22:03:44 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/04 22:03:44 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-i4b12fn5 2020/10/04 22:03:44 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/04 22:03:44 Creating namespace ci-op-i4b12fn5 2020/10/04 22:03:45 Setting up pipeline imagestream for the test 2020/10/04 22:03:45 Created secret e2e-aws-serial-cluster-profile 2020/10/04 22:03:45 Created secret pull-secret 2020/10/04 22:03:45 Created PDB for pods with openshift.io/build.name label 2020/10/04 22:03:45 Created PDB for pods with created-by-ci label 2020/10/04 22:03:45 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-i4b12fn5/stable:${component} 2020/10/04 22:03:47 Importing release image latest 2020/10/04 22:04:42 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/04 22:04:42 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/04 22:04:42 Acquired lease \\\"47a8c32b-7b73-451c-8b7c-e979b97fea83\\\" for \\\"aws-quota-slice\\\" 2020/10/04 22:04:42 Executing template e2e-aws-serial 2020/10/04 22:04:42 Creating or restarting template instance 2020/10/04 22:04:42 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/04 22:04:43 Waiting for template instance to be ready 2020/10/04 22:04:45 Running pod e2e-aws-serial I1004 22:52:49.516914 17 request.go:621] Throttling request took 2m9.389732511s, request: GET:https://172.30.0.1:443/api/v1/namespaces/ci-op-i4b12fn5/pods/e2e-aws-serial 2020/10/04 22:53:47 Container setup in pod e2e-aws-serial completed successfully 2020/10/05 00:20:13 Copied 126.39MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/05 00:20:13 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/05 00:20:13 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/05 00:20:13 Ran for 2h16m29s 2020/10/05 00:20:13 could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 8}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 5, \"cluster_size\": 2, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> <*> <*> could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 8}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 9, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/06 22:04:28 ci-operator version v20201006-ef0331a 2020/10/06 22:04:28 No source defined 2020/10/06 22:04:28 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/06 22:04:28 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-i0y349zs 2020/10/06 22:04:28 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/06 22:04:28 Creating namespace ci-op-i0y349zs 2020/10/06 22:04:28 Setting up pipeline imagestream for the test 2020/10/06 22:04:28 Created secret e2e-aws-serial-cluster-profile 2020/10/06 22:04:28 Created secret pull-secret 2020/10/06 22:04:29 Created PDB for pods with openshift.io/build.name label 2020/10/06 22:04:29 Created PDB for pods with created-by-ci label 2020/10/06 22:04:29 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-i0y349zs/stable:${component} 2020/10/06 22:04:31 Importing release image latest 2020/10/06 22:05:24 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/06 22:05:24 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/06 22:05:24 Acquired lease \\\"0c6bba54-1615-4ab3-af76-928fddc42f2a\\\" for \\\"aws-quota-slice\\\" 2020/10/06 22:05:24 Executing template e2e-aws-serial 2020/10/06 22:05:24 Creating or restarting template instance 2020/10/06 22:05:24 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/06 22:05:24 Waiting for template instance to be ready 2020/10/06 22:05:26 Running pod e2e-aws-serial 2020/10/06 22:31:24 Container setup in pod e2e-aws-serial completed successfully 2020/10/06 23:39:17 Container test in pod e2e-aws-serial completed successfully 2020/10/06 23:44:56 Container teardown in pod e2e-aws-serial completed successfully 2020/10/06 23:44:56 Pod e2e-aws-serial succeeded after 1h39m30s 2020/10/06 23:45:46 Copied 105.31MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/06 23:47:10 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/06 23:54:10 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/06 23:54:25 Ran for 1h49m56s 2020/10/06 23:54:25 could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 9}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 5, \"cluster_size\": 3, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> <*> <*> could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 9}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 10, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/08 22:06:31 ci-operator version v20201008-bd69a8a 2020/10/08 22:06:31 No source defined 2020/10/08 22:06:31 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/08 22:06:31 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-t0j93dv1 2020/10/08 22:06:31 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/08 22:06:31 Creating namespace ci-op-t0j93dv1 2020/10/08 22:06:31 Setting up pipeline imagestream for the test 2020/10/08 22:06:31 Created secret e2e-aws-serial-cluster-profile 2020/10/08 22:06:31 Created secret pull-secret 2020/10/08 22:06:31 Created PDB for pods with openshift.io/build.name label 2020/10/08 22:06:31 Created PDB for pods with created-by-ci label 2020/10/08 22:06:31 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-t0j93dv1/stable:${component} 2020/10/08 22:06:34 Importing release image latest 2020/10/08 22:06:35 Executing pod \\\"release-images-latest-cli\\\" 2020/10/08 22:06:52 Executing pod \\\"release-images-latest\\\" 2020/10/08 22:07:42 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/08 22:07:42 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/08 22:07:44 Acquired lease \\\"aa794ad8-fbab-44d7-9c7f-c5f632d23536\\\" for \\\"aws-quota-slice\\\" 2020/10/08 22:07:44 Executing template e2e-aws-serial 2020/10/08 22:07:48 Creating or restarting template instance 2020/10/08 22:07:48 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/08 22:07:48 Waiting for template instance to be ready 2020/10/08 22:07:50 Running pod e2e-aws-serial 2020/10/08 22:41:37 Container setup in pod e2e-aws-serial completed successfully 2020/10/08 23:19:47 warning: failed to update lease \\\"aa794ad8-fbab-44d7-9c7f-c5f632d23536\\\": Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=aa794ad8-fbab-44d7-9c7f-c5f632d23536&owner=ci-op-t0j93dv1-7bc5c&state=leased\\\": net/http: TLS handshake timeout I1008 23:26:40.659217 16 request.go:621] Throttling request took 1.315662144s, request: GET:https://172.30.0.1:443/api/v1/namespaces/ci-op-t0j93dv1 2020/10/08 23:34:12 warning: failed to update lease \\\"aa794ad8-fbab-44d7-9c7f-c5f632d23536\\\": Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=aa794ad8-fbab-44d7-9c7f-c5f632d23536&owner=ci-op-t0j93dv1-7bc5c&state=leased\\\": net/http: TLS handshake timeout 2020/10/08 23:38:13 warning: failed to update lease \\\"aa794ad8-fbab-44d7-9c7f-c5f632d23536\\\": Post \\\"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=aa794ad8-fbab-44d7-9c7f-c5f632d23536&owner=ci-op-t0j93dv1-7bc5c&state=leased\\\": net/http: TLS handshake timeout 2020/10/09 00:11:39 Copied 126.52MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/09 00:11:39 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/09 00:11:39 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/09 00:11:39 Ran for 2h5m8s 2020/10/09 00:11:39 could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 10}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 8, \"cluster_size\": 2, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> to <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> <*> <*> could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 10}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 11, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/10 22:07:34 ci-operator version v20201009-5784e03 2020/10/10 22:07:34 No source defined 2020/10/10 22:07:34 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/10 22:07:34 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-qx14fdfg 2020/10/10 22:07:34 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/10 22:07:34 Creating namespace ci-op-qx14fdfg 2020/10/10 22:07:34 Setting up pipeline imagestream for the test 2020/10/10 22:07:34 Created secret e2e-aws-serial-cluster-profile 2020/10/10 22:07:34 Created secret pull-secret 2020/10/10 22:07:34 Created PDB for pods with openshift.io/build.name label 2020/10/10 22:07:34 Created PDB for pods with created-by-ci label 2020/10/10 22:07:34 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-qx14fdfg/stable:${component} 2020/10/10 22:07:36 Importing release image latest 2020/10/10 22:07:37 Executing pod \\\"release-images-latest-cli\\\" 2020/10/10 22:07:44 Executing pod \\\"release-images-latest\\\" 2020/10/10 22:08:33 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/10 22:08:33 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/10 22:08:34 Acquired lease \\\"c5f904bd-9c28-4d45-8565-06eb1478137c\\\" for \\\"aws-quota-slice\\\" 2020/10/10 22:08:34 Executing template e2e-aws-serial 2020/10/10 22:08:34 Creating or restarting template instance 2020/10/10 22:08:34 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/10 22:08:34 Waiting for template instance to be ready 2020/10/10 22:08:36 Running pod e2e-aws-serial 2020/10/10 22:42:05 Container setup in pod e2e-aws-serial completed successfully I1010 23:17:42.661737 14 request.go:621] Throttling request took 4.919880382s, request: GET:https://172.30.0.1:443/api/v1/namespaces/ci-op-qx14fdfg 2020/10/10 23:52:04 Container test in pod e2e-aws-serial completed successfully 2020/10/10 23:58:02 Copied 113.21MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/10 23:58:02 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/10 23:58:02 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/10 23:58:02 Ran for 1h50m28s 2020/10/10 23:58:02 could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 11}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 8, \"cluster_size\": 3, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> to <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> <*> <*> could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 11}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 12, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/12 22:09:34 ci-operator version v20201009-5784e03 2020/10/12 22:09:34 No source defined 2020/10/12 22:09:34 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/12 22:09:34 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-qqz5nvq1 2020/10/12 22:09:34 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/12 22:09:34 Creating namespace ci-op-qqz5nvq1 2020/10/12 22:09:34 Setting up pipeline imagestream for the test 2020/10/12 22:09:34 Created secret e2e-aws-serial-cluster-profile 2020/10/12 22:09:34 Created secret pull-secret 2020/10/12 22:09:34 Created PDB for pods with openshift.io/build.name label 2020/10/12 22:09:34 Created PDB for pods with created-by-ci label 2020/10/12 22:09:34 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-qqz5nvq1/stable:${component} 2020/10/12 22:09:36 Importing release image latest 2020/10/12 22:09:37 Executing pod \\\"release-images-latest-cli\\\" 2020/10/12 22:09:43 Executing pod \\\"release-images-latest\\\" 2020/10/12 22:10:30 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/12 22:10:30 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/12 22:10:30 Acquired lease \\\"6fdc4895-3ff1-45be-9ef5-8a250b3bfba8\\\" for \\\"aws-quota-slice\\\" 2020/10/12 22:10:30 Executing template e2e-aws-serial 2020/10/12 22:10:30 Creating or restarting template instance 2020/10/12 22:10:30 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/12 22:10:30 Waiting for template instance to be ready 2020/10/12 22:10:32 Running pod e2e-aws-serial I1012 22:29:42.215381 15 request.go:621] Throttling request took 2.550226459s, request: GET:https://172.30.0.1:443/api/v1/namespaces/ci-op-qqz5nvq1 Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-west-1 (zones: us-west-1a us-west-1b) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=warning msg=\\\"Found override for ReleaseImage. Please be warned, this is not advised\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Common Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Master Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Worker Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Openshift Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Creating infrastructure resources...\\\" level=info msg=\\\"Waiting up to 30m0s for the Kubernetes API at https://api.ci-op-qqz5nvq1-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443...\\\" level=info msg=\\\"API v1.13.4-138-g41dc99c up\\\" level=info msg=\\\"Waiting up to 30m0s for bootstrapping to complete...\\\" level=info msg=\\\"Destroying the bootstrap resources...\\\" level=info msg=\\\"Waiting up to 30m0s for the cluster at https://api.ci-op-qqz5nvq1-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 to initialize...\\\" level=fatal msg=\\\"failed to initialize the cluster: Some cluster operators are still updating: authentication, console: timed out waiting for the condition\\\" 2020/10/12 22:59:35 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2020/10/12 23:05:17 Copied 51.00MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/12 23:05:17 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/12 23:05:17 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/12 23:05:17 Ran for 55m43s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-qqz5nvq1/e2e-aws-serial failed after 54m39s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-west-1 (zones: us-west-1a us-west-1b) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=warning msg=\\\"Found override for ReleaseImage. Please be warned, this is not advised\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Common Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Master Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Worker Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Openshift Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Creating infrastructure resources...\\\" level=info msg=\\\"Waiting up to 30m0s for the Kubernetes API at https://api.ci-op-qqz5nvq1-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443...\\\" level=info msg=\\\"API v1.13.4-138-g41dc99c up\\\" level=info msg=\\\"Waiting up to 30m0s for bootstrapping to complete...\\\" level=info msg=\\\"Destroying the bootstrap resources...\\\" level=info msg=\\\"Waiting up to 30m0s for the cluster at https://api.ci-op-qqz5nvq1-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 to initialize...\\\" level=fatal msg=\\\"failed to initialize the cluster: Some cluster operators are still updating: authentication, console: timed out waiting for the condition\\\" --- 2020/10/12 23:05:17 could not load result reporting options: mandatory flag -report-password-file is unset '\", \"cluster_count\": 12}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 13, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/13 22:10:08 ci-operator version v20201013-e136392 2020/10/13 22:10:08 No source defined 2020/10/13 22:10:08 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/13 22:10:08 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-xh6icmvm 2020/10/13 22:10:08 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/13 22:10:08 Creating namespace ci-op-xh6icmvm 2020/10/13 22:10:08 Setting up pipeline imagestream for the test 2020/10/13 22:10:08 Created secret e2e-aws-serial-cluster-profile 2020/10/13 22:10:08 Created secret pull-secret 2020/10/13 22:10:08 Created PDB for pods with openshift.io/build.name label 2020/10/13 22:10:08 Created PDB for pods with created-by-ci label 2020/10/13 22:10:09 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-xh6icmvm/stable:${component} 2020/10/13 22:10:11 Importing release image latest 2020/10/13 22:10:11 Executing pod \\\"release-images-latest-cli\\\" 2020/10/13 22:10:20 Executing pod \\\"release-images-latest\\\" 2020/10/13 22:11:07 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/13 22:11:07 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/13 22:14:45 Acquired lease \\\"a70ec978-4528-4ef7-9631-4792e1e2c901\\\" for \\\"aws-quota-slice\\\" 2020/10/13 22:14:45 Executing template e2e-aws-serial 2020/10/13 22:14:45 Creating or restarting template instance 2020/10/13 22:14:45 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/13 22:14:45 Waiting for template instance to be ready 2020/10/13 22:14:47 Running pod e2e-aws-serial 2020/10/13 22:45:26 Container setup in pod e2e-aws-serial completed successfully secret/support created started: (0/1/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m37s) 2020-10-13T22:48:15 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/2/79) \\\"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m48s) 2020-10-13T22:50:02 \\\"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/3/79) \\\"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m6s) 2020-10-13T22:51:08 \\\"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/4/79) \\\"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\\\" skip [github.com/openshift/origin/test/extended/images/signatures.go:25]: disable because containers/image: https://github.com/containers/image/pull/570 skipped: (9.8s) 2020-10-13T22:51:18 \\\"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\\\" started: (0/5/79) \\\"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (2m58s) 2020-10-13T22:54:16 \\\"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/6/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T22:54:45 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/7/79) \\\"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/apps/daemon_set.go:378]: Requires at least 2 nodes (not -1) skipped: (11.8s) 2020-10-13T22:54:57 \\\"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/8/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" Oct 13 22:54:58.214: INFO: >>> kubeConfig: /tmp/admin.kubeconfig Oct 13 22:54:58.218: INFO: Waiting up to 30m0s for all (but 100) nodes to be schedulable Oct 13 22:54:58.393: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace \\\\'kube-system\\\\' to be running and ready Oct 13 22:54:58.484: INFO: 0 / 0 pods in namespace \\\\'kube-system\\\\' are running and ready (0 seconds elapsed) Oct 13 22:54:58.485: INFO: expected 0 pod replicas in namespace \\\\'kube-system\\\\', 0 are Running and Ready. Oct 13 22:54:58.485: INFO: Waiting up to 5m0s for all daemonsets in namespace \\\\'kube-system\\\\' to start Oct 13 22:54:58.514: INFO: e2e test version: v1.13.4-138-g41dc99c Oct 13 22:54:58.538: INFO: kube-apiserver version: v1.13.4-138-g41dc99c [BeforeEach] [Top Level] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/test/extended/util/test.go:69 [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:153 STEP: Creating a kubernetes client Oct 13 22:54:58.539: INFO: >>> kubeConfig: /tmp/admin.kubeconfig STEP: Building a namespace api object, basename sched-priority Oct 13 22:55:00.084: INFO: About to run a Kube e2e test, ensuring namespace is privileged Oct 13 22:55:00.423: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled. STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:71 Oct 13 22:55:00.448: INFO: Waiting up to 1m0s for all nodes to be ready Oct 13 22:56:00.841: INFO: Waiting for terminating namespaces to be deleted... Oct 13 22:56:00.868: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace \\\\'kube-system\\\\' to be running and ready Oct 13 22:56:00.945: INFO: 0 / 0 pods in namespace \\\\'kube-system\\\\' are running and ready (0 seconds elapsed) Oct 13 22:56:00.945: INFO: expected 0 pod replicas in namespace \\\\'kube-system\\\\', 0 are Running and Ready. [It] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:209 Oct 13 22:56:00.945: INFO: ComputeCpuMemFraction for node: ip-10-0-131-112.us-east-2.compute.internal Oct 13 22:56:00.997: INFO: Pod for on the node: tuned-ttkbd, Cpu: 10, Mem: 20971520 Oct 13 22:56:00.997: INFO: Pod for on the node: downloads-795f496c64-w8fkj, Cpu: 10, Mem: 52428800 Oct 13 22:56:00.997: INFO: Pod for on the node: dns-default-kszjp, Cpu: 110, Mem: 283115520 Oct 13 22:56:00.997: INFO: Pod for on the node: node-ca-gdcq5, Cpu: 10, Mem: 10485760 Oct 13 22:56:00.997: INFO: Pod for on the node: machine-config-daemon-w5j8c, Cpu: 20, Mem: 52428800 Oct 13 22:56:00.997: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Oct 13 22:56:00.997: INFO: Pod for on the node: grafana-649f787944-8znfk, Cpu: 200, Mem: 314572800 Oct 13 22:56:00.997: INFO: Pod for on the node: node-exporter-h68hq, Cpu: 110, Mem: 230686720 Oct 13 22:56:00.997: INFO: Pod for on the node: prometheus-adapter-b6fb584c8-877np, Cpu: 100, Mem: 209715200 Oct 13 22:56:00.997: INFO: Pod for on the node: multus-n7fm4, Cpu: 100, Mem: 209715200 Oct 13 22:56:00.997: INFO: Pod for on the node: olm-operators-kt5zd, Cpu: 100, Mem: 209715200 Oct 13 22:56:00.997: INFO: Pod for on the node: ovs-clnlv, Cpu: 200, Mem: 419430400 Oct 13 22:56:00.997: INFO: Pod for on the node: sdn-rgvn4, Cpu: 100, Mem: 209715200 Oct 13 22:56:00.997: INFO: Node: ip-10-0-131-112.us-east-2.compute.internal, totalRequestedCpuResource: 1170, cpuAllocatableMil: 3500, cpuFraction: 0.3342857142857143 Oct 13 22:56:00.997: INFO: Node: ip-10-0-131-112.us-east-2.compute.internal, totalRequestedMemResource: 2327838720, memAllocatableVal: 16181800960, memFraction: 0.14385535489864287 Oct 13 22:56:00.997: INFO: ComputeCpuMemFraction for node: ip-10-0-141-37.us-east-2.compute.internal Oct 13 22:56:01.043: INFO: Pod for on the node: tuned-bd4jd, Cpu: 10, Mem: 20971520 Oct 13 22:56:01.043: INFO: Pod for on the node: dns-default-xvq2v, Cpu: 110, Mem: 283115520 Oct 13 22:56:01.043: INFO: Pod for on the node: image-registry-7d8ddf69f5-s48h6, Cpu: 100, Mem: 268435456 Oct 13 22:56:01.043: INFO: Pod for on the node: node-ca-2xl87, Cpu: 10, Mem: 10485760 Oct 13 22:56:01.043: INFO: Pod for on the node: router-default-7cf4f88fff-hfcjh, Cpu: 100, Mem: 268435456 Oct 13 22:56:01.043: INFO: Pod for on the node: machine-config-daemon-nzbx2, Cpu: 20, Mem: 52428800 Oct 13 22:56:01.043: INFO: Pod for on the node: certified-operators-84864f8bc-l7nk8, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.043: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Oct 13 22:56:01.043: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-4qkjl, Cpu: 300, Mem: 629145600 Oct 13 22:56:01.043: INFO: Pod for on the node: node-exporter-q79sr, Cpu: 110, Mem: 230686720 Oct 13 22:56:01.043: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Oct 13 22:56:01.043: INFO: Pod for on the node: multus-plc4l, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.043: INFO: Pod for on the node: ovs-m4gxq, Cpu: 200, Mem: 419430400 Oct 13 22:56:01.043: INFO: Pod for on the node: sdn-n8flh, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.043: INFO: Node: ip-10-0-141-37.us-east-2.compute.internal, totalRequestedCpuResource: 1160, cpuAllocatableMil: 3500, cpuFraction: 0.3314285714285714 Oct 13 22:56:01.043: INFO: Node: ip-10-0-141-37.us-east-2.compute.internal, totalRequestedMemResource: 2497708032, memAllocatableVal: 16181792768, memFraction: 0.1543529859645277 Oct 13 22:56:01.043: INFO: ComputeCpuMemFraction for node: ip-10-0-157-185.us-east-2.compute.internal Oct 13 22:56:01.096: INFO: Pod for on the node: tuned-wjtqw, Cpu: 10, Mem: 20971520 Oct 13 22:56:01.096: INFO: Pod for on the node: downloads-795f496c64-s4bjk, Cpu: 10, Mem: 52428800 Oct 13 22:56:01.096: INFO: Pod for on the node: dns-default-jlm7s, Cpu: 110, Mem: 283115520 Oct 13 22:56:01.096: INFO: Pod for on the node: node-ca-d7jqn, Cpu: 10, Mem: 10485760 Oct 13 22:56:01.096: INFO: Pod for on the node: router-default-7cf4f88fff-mqlsj, Cpu: 100, Mem: 268435456 Oct 13 22:56:01.096: INFO: Pod for on the node: machine-config-daemon-wcqlt, Cpu: 20, Mem: 52428800 Oct 13 22:56:01.096: INFO: Pod for on the node: community-operators-656f6798cf-4mvzl, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.096: INFO: Pod for on the node: redhat-operators-68dffc88bb-nzxj7, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.096: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Oct 13 22:56:01.096: INFO: Pod for on the node: node-exporter-62j7m, Cpu: 110, Mem: 230686720 Oct 13 22:56:01.097: INFO: Pod for on the node: prometheus-adapter-b6fb584c8-wgl8s, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.097: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Oct 13 22:56:01.097: INFO: Pod for on the node: prometheus-operator-5d4588dd6-klcm2, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.097: INFO: Pod for on the node: telemeter-client-78b5484ddf-lm764, Cpu: 210, Mem: 440401920 Oct 13 22:56:01.097: INFO: Pod for on the node: multus-n7zg8, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.097: INFO: Pod for on the node: ovs-5rj98, Cpu: 200, Mem: 419430400 Oct 13 22:56:01.097: INFO: Pod for on the node: sdn-hvg9q, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.097: INFO: Node: ip-10-0-157-185.us-east-2.compute.internal, totalRequestedCpuResource: 1280, cpuAllocatableMil: 3500, cpuFraction: 0.3657142857142857 Oct 13 22:56:01.097: INFO: Node: ip-10-0-157-185.us-east-2.compute.internal, totalRequestedMemResource: 2722103296, memAllocatableVal: 16181800960, memFraction: 0.16822004563823284 Oct 13 22:56:01.143: INFO: Waiting for running... Oct 13 22:56:11.226: INFO: Waiting for running... Oct 13 22:56:21.309: INFO: Waiting for running... STEP: Compute Cpu, Mem Fraction after create balanced pods. Oct 13 22:56:31.360: INFO: ComputeCpuMemFraction for node: ip-10-0-131-112.us-east-2.compute.internal Oct 13 22:56:31.522: INFO: Pod for on the node: 43abe79a-0da7-11eb-af38-0a58ac107a4c-0, Cpu: 580, Mem: 5763061760 Oct 13 22:56:31.522: INFO: Pod for on the node: tuned-ttkbd, Cpu: 10, Mem: 20971520 Oct 13 22:56:31.522: INFO: Pod for on the node: downloads-795f496c64-w8fkj, Cpu: 10, Mem: 52428800 Oct 13 22:56:31.522: INFO: Pod for on the node: dns-default-kszjp, Cpu: 110, Mem: 283115520 Oct 13 22:56:31.522: INFO: Pod for on the node: node-ca-gdcq5, Cpu: 10, Mem: 10485760 Oct 13 22:56:31.522: INFO: Pod for on the node: machine-config-daemon-w5j8c, Cpu: 20, Mem: 52428800 Oct 13 22:56:31.522: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Oct 13 22:56:31.522: INFO: Pod for on the node: grafana-649f787944-8znfk, Cpu: 200, Mem: 314572800 Oct 13 22:56:31.522: INFO: Pod for on the node: node-exporter-h68hq, Cpu: 110, Mem: 230686720 Oct 13 22:56:31.522: INFO: Pod for on the node: prometheus-adapter-b6fb584c8-877np, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.522: INFO: Pod for on the node: multus-n7fm4, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.522: INFO: Pod for on the node: olm-operators-kt5zd, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.522: INFO: Pod for on the node: ovs-clnlv, Cpu: 200, Mem: 419430400 Oct 13 22:56:31.522: INFO: Pod for on the node: sdn-rgvn4, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.522: INFO: Node: ip-10-0-131-112.us-east-2.compute.internal, totalRequestedCpuResource: 1750, cpuAllocatableMil: 3500, cpuFraction: 0.5 Oct 13 22:56:31.522: INFO: Node: ip-10-0-131-112.us-east-2.compute.internal, totalRequestedMemResource: 8090900480, memAllocatableVal: 16181800960, memFraction: 0.5 STEP: Compute Cpu, Mem Fraction after create balanced pods. Oct 13 22:56:31.522: INFO: ComputeCpuMemFraction for node: ip-10-0-141-37.us-east-2.compute.internal Oct 13 22:56:31.589: INFO: Pod for on the node: 49b091f9-0da7-11eb-af38-0a58ac107a4c-0, Cpu: 590, Mem: 5593188352 Oct 13 22:56:31.589: INFO: Pod for on the node: tuned-bd4jd, Cpu: 10, Mem: 20971520 Oct 13 22:56:31.589: INFO: Pod for on the node: dns-default-xvq2v, Cpu: 110, Mem: 283115520 Oct 13 22:56:31.589: INFO: Pod for on the node: image-registry-7d8ddf69f5-s48h6, Cpu: 100, Mem: 268435456 Oct 13 22:56:31.589: INFO: Pod for on the node: node-ca-2xl87, Cpu: 10, Mem: 10485760 Oct 13 22:56:31.589: INFO: Pod for on the node: router-default-7cf4f88fff-hfcjh, Cpu: 100, Mem: 268435456 Oct 13 22:56:31.589: INFO: Pod for on the node: machine-config-daemon-nzbx2, Cpu: 20, Mem: 52428800 Oct 13 22:56:31.589: INFO: Pod for on the node: certified-operators-84864f8bc-l7nk8, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.589: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Oct 13 22:56:31.589: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-4qkjl, Cpu: 300, Mem: 629145600 Oct 13 22:56:31.589: INFO: Pod for on the node: node-exporter-q79sr, Cpu: 110, Mem: 230686720 Oct 13 22:56:31.589: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Oct 13 22:56:31.589: INFO: Pod for on the node: multus-plc4l, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.589: INFO: Pod for on the node: ovs-m4gxq, Cpu: 200, Mem: 419430400 Oct 13 22:56:31.589: INFO: Pod for on the node: sdn-n8flh, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.589: INFO: Node: ip-10-0-141-37.us-east-2.compute.internal, totalRequestedCpuResource: 1750, cpuAllocatableMil: 3500, cpuFraction: 0.5 Oct 13 22:56:31.589: INFO: Node: ip-10-0-141-37.us-east-2.compute.internal, totalRequestedMemResource: 8090896384, memAllocatableVal: 16181792768, memFraction: 0.5 STEP: Compute Cpu, Mem Fraction after create balanced pods. Oct 13 22:56:31.589: INFO: ComputeCpuMemFraction for node: ip-10-0-157-185.us-east-2.compute.internal Oct 13 22:56:31.635: INFO: Pod for on the node: 4fb330dc-0da7-11eb-af38-0a58ac107a4c-0, Cpu: 470, Mem: 5368797184 Oct 13 22:56:31.635: INFO: Pod for on the node: tuned-wjtqw, Cpu: 10, Mem: 20971520 Oct 13 22:56:31.635: INFO: Pod for on the node: downloads-795f496c64-s4bjk, Cpu: 10, Mem: 52428800 Oct 13 22:56:31.635: INFO: Pod for on the node: dns-default-jlm7s, Cpu: 110, Mem: 283115520 Oct 13 22:56:31.635: INFO: Pod for on the node: node-ca-d7jqn, Cpu: 10, Mem: 10485760 Oct 13 22:56:31.635: INFO: Pod for on the node: router-default-7cf4f88fff-mqlsj, Cpu: 100, Mem: 268435456 Oct 13 22:56:31.635: INFO: Pod for on the node: machine-config-daemon-wcqlt, Cpu: 20, Mem: 52428800 Oct 13 22:56:31.635: INFO: Pod for on the node: community-operators-656f6798cf-4mvzl, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.635: INFO: Pod for on the node: redhat-operators-68dffc88bb-nzxj7, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.635: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Oct 13 22:56:31.635: INFO: Pod for on the node: node-exporter-62j7m, Cpu: 110, Mem: 230686720 Oct 13 22:56:31.635: INFO: Pod for on the node: prometheus-adapter-b6fb584c8-wgl8s, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.635: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Oct 13 22:56:31.635: INFO: Pod for on the node: prometheus-operator-5d4588dd6-klcm2, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.635: INFO: Pod for on the node: telemeter-client-78b5484ddf-lm764, Cpu: 210, Mem: 440401920 Oct 13 22:56:31.635: INFO: Pod for on the node: multus-n7zg8, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.635: INFO: Pod for on the node: ovs-5rj98, Cpu: 200, Mem: 419430400 Oct 13 22:56:31.635: INFO: Pod for on the node: sdn-hvg9q, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.635: INFO: Node: ip-10-0-157-185.us-east-2.compute.internal, totalRequestedCpuResource: 1750, cpuAllocatableMil: 3500, cpuFraction: 0.5 Oct 13 22:56:31.635: INFO: Node: ip-10-0-157-185.us-east-2.compute.internal, totalRequestedMemResource: 8090900480, memAllocatableVal: 16181800960, memFraction: 0.5 STEP: Trying to apply 10 taint on the nodes except first one. STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-55dfaa88-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-55dfaaa1-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-55ebd6cc-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-55ebd701-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-55f7c123-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-55f7c159-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5603d0a3-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5603d0ff-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-560fd502-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-560fd544-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-561bec54-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-561bec90-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56286285-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-562862b3-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56347606-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5634763f-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56408805-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56408839-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-564c8cea-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-564c8d25-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-565918ca-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-565918fb-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-566525d4-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5665260c-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56712fae-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56712fe8-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-567daa3d-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-567daa78-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5689b24f-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5689b287-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5695ac8e-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5695acc4-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56a2252d-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56a22561-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56ae8709-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56ae873c-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56baabbb-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56baabf2-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56c6e82d-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56c6e865-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: Create a pod without any tolerations STEP: Pod should prefer scheduled to the node don\\\\'t have the taint. STEP: Trying to apply 10 taint on the first node. STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5cda1465-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5cda147f-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5ce60e2f-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5ce60e66-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5cf21790-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5cf217c0-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5cfe333f-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5cfe3413-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5d0a6153-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d0a618c-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5d1676d7-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d167709-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5d228235-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d228264-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5d2ea2cd-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d2ea2fa-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5d3aafcd-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d3aaff9-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5d46ca82-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d46cab3-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: Create a pod that tolerates all the taints of the first node. STEP: Pod should prefer scheduled to the node that pod can tolerate. STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-5d46ca82-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d46cab3-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-5d3aafcd-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d3aaff9-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-5d2ea2cd-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d2ea2fa-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-5d228235-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d228264-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-5d1676d7-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d167709-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-5d0a6153-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d0a618c-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-5cfe333f-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5cfe3413-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-5cf21790-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5cf217c0-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-5ce60e2f-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5ce60e66-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-5cda1465-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5cda147f-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-56c6e82d-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56c6e865-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-56baabbb-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56baabf2-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-56ae8709-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56ae873c-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-56a2252d-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56a22561-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-5695ac8e-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5695acc4-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-5689b24f-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5689b287-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-567daa3d-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-567daa78-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-56712fae-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56712fe8-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-566525d4-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5665260c-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-565918ca-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-565918fb-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-564c8cea-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-564c8d25-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-56408805-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56408839-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-56347606-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5634763f-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-56286285-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-562862b3-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-561bec54-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-561bec90-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-560fd502-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-560fd544-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-5603d0a3-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5603d0ff-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-55f7c123-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-55f7c159-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-55ebd6cc-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-55ebd701-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\\\'t have the taint kubernetes.io/e2e-taint-key-55dfaa88-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-55dfaaa1-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:154 Oct 13 22:56:56.621: INFO: Waiting up to 3m0s for all (but 100) nodes to be ready STEP: Destroying namespace \\\"e2e-tests-sched-priority-6bh77\\\" for this suite. Oct 13 23:06:56.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Oct 13 23:07:57.862: INFO: Couldn\\\\'t delete ns: \\\"e2e-tests-sched-priority-6bh77\\\": the server was unable to return a response in the time allotted, but may still be processing the request (&errors.StatusError{ErrStatus:v1.Status{TypeMeta:v1.TypeMeta{Kind:\\\"\\\", APIVersion:\\\"\\\"}, ListMeta:v1.ListMeta{SelfLink:\\\"\\\", ResourceVersion:\\\"\\\", Continue:\\\"\\\"}, Status:\\\"Failure\\\", Message:\\\"the server was unable to return a response in the time allotted, but may still be processing the request\\\", Reason:\\\"Timeout\\\", Details:(*v1.StatusDetails)(0xc001efd020), Code:504}}) [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:68 Oct 13 23:07:57.867: INFO: Running AfterSuite actions on all nodes Oct 13 23:07:57.867: INFO: Running AfterSuite actions on node 1 fail [k8s.io/kubernetes/test/e2e/framework/framework.go:345]: Oct 13 23:07:57.862: Couldn\\\\'t delete ns: \\\"e2e-tests-sched-priority-6bh77\\\": the server was unable to return a response in the time allotted, but may still be processing the request (&errors.StatusError{ErrStatus:v1.Status{TypeMeta:v1.TypeMeta{Kind:\\\"\\\", APIVersion:\\\"\\\"}, ListMeta:v1.ListMeta{SelfLink:\\\"\\\", ResourceVersion:\\\"\\\", Continue:\\\"\\\"}, Status:\\\"Failure\\\", Message:\\\"the server was unable to return a response in the time allotted, but may still be processing the request\\\", Reason:\\\"Timeout\\\", Details:(*v1.StatusDetails)(0xc001efd020), Code:504}}) Oct 13 22:58:02.943 W clusteroperator/console changed Progressing to True: ProgressingSynchronizationError: Progressing: oauth client for console does not exist and cannot be created Oct 13 22:58:02.952 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"\\\" to \\\"Degraded: oauth client for console does not exist and cannot be created\\\",Progressing changed from False to True (\\\"Progressing: oauth client for console does not exist and cannot be created\\\") Oct 13 22:59:02.975 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"Degraded: oauth client for console does not exist and cannot be created\\\" to \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\",Progressing message changed from \\\"Progressing: oauth client for console does not exist and cannot be created\\\" to \\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\" Oct 13 22:59:40.168 W clusteroperator/openshift-apiserver changed Available to False: AvailableMultiple: Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.authorization.openshift.io is not ready: 0\\\\ Available: v1.oauth.openshift.io is not ready: 0\\\\ Available: v1.project.openshift.io is not ready: 0\\\\ Available: v1.security.openshift.io is not ready: 0 Oct 13 22:59:40.178 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available changed from True to False (\\\"Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.authorization.openshift.io is not ready: 0\\\\ Available: v1.oauth.openshift.io is not ready: 0\\\\ Available: v1.project.openshift.io is not ready: 0\\\\ Available: v1.security.openshift.io is not ready: 0\\\") Oct 13 23:00:01.369 E clusteroperator/monitoring changed Degraded to True: UpdatingconfigurationsharingFailed: Failed to rollout the stack. Error: running task Updating configuration sharing failed: failed to retrieve Grafana host: getting Route object failed: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io grafana) Oct 13 23:00:01.369 W clusteroperator/monitoring changed Available to False Oct 13 23:00:01.390 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:00:03.023 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\" to \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\",Progressing message changed from \\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\" to \\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\" Oct 13 23:00:03.097 W clusteroperator/console changed Progressing to False Oct 13 23:00:03.101 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\" to \\\"\\\",Progressing changed from True to False (\\\"\\\") Oct 13 23:00:20.262 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \\\"\\\" to \\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\" Oct 13 23:01:03.117 W clusteroperator/console changed Progressing to True: ProgressingSynchronizationError: Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console) Oct 13 23:01:03.125 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"\\\" to \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\",Progressing changed from False to True (\\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\") Oct 13 23:01:09.039 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (9 times) Oct 13 23:01:10.012 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (6 times) Oct 13 23:01:11.011 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (6 times) Oct 13 23:01:12.176 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (6 times) Oct 13 23:01:12.369 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (9 times) Oct 13 23:01:12.508 W clusteroperator/monitoring changed Progressing to False Oct 13 23:01:12.531 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:01:12.574 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (9 times) Oct 13 23:01:21.492 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\" to \\\"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" Oct 13 23:02:03.170 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\" to \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\",Progressing message changed from \\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\" to \\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\" (2 times) Oct 13 23:02:18.622 W clusteroperator/monitoring changed Progressing to False Oct 13 23:02:18.642 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:02:21.528 E clusteroperator/authentication changed Degraded to True: MultipleConditionsMatching: RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client) Oct 13 23:02:21.533 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from False to True (\\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\") Oct 13 23:02:52.456 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \\\"Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.authorization.openshift.io is not ready: 0\\\\ Available: v1.oauth.openshift.io is not ready: 0\\\\ Available: v1.project.openshift.io is not ready: 0\\\\ Available: v1.security.openshift.io is not ready: 0\\\" to \\\"Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.build.openshift.io is not ready: 0\\\\ Available: v1.quota.openshift.io is not ready: 0\\\\ Available: v1.route.openshift.io is not ready: 0\\\\ Available: v1.security.openshift.io is not ready: 0\\\\ Available: v1.template.openshift.io is not ready: 0\\\" Oct 13 23:03:03.212 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\" to \\\"Degraded: oauth client for console does not exist and cannot be created\\\",Progressing message changed from \\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\" to \\\"Progressing: oauth client for console does not exist and cannot be created\\\" Oct 13 23:03:22.744 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" to \\\"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" Oct 13 23:03:23.767 W clusteroperator/monitoring changed Progressing to False Oct 13 23:03:23.788 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:04:22.761 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \\\"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" to \\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" Oct 13 23:04:44.727 W clusteroperator/monitoring changed Progressing to False Oct 13 23:04:44.747 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:05:23.991 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" to \\\"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" (2 times) Oct 13 23:05:25.224 W clusteroperator/authentication changed Degraded to False Oct 13 23:05:25.229 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from True to False (\\\"\\\") Oct 13 23:05:32.733 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \\\"Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.build.openshift.io is not ready: 0\\\\ Available: v1.quota.openshift.io is not ready: 0\\\\ Available: v1.route.openshift.io is not ready: 0\\\\ Available: v1.security.openshift.io is not ready: 0\\\\ Available: v1.template.openshift.io is not ready: 0\\\" to \\\"Available: v1.authorization.openshift.io is not ready: 0\\\\ Available: v1.build.openshift.io is not ready: 0\\\\ Available: v1.security.openshift.io is not ready: 0\\\\ Available: v1.template.openshift.io is not ready: 0\\\\ Available: v1.user.openshift.io is not ready: 0\\\" Oct 13 23:06:47.587 W clusteroperator/monitoring changed Progressing to False Oct 13 23:06:47.610 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:07:30.608 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \\\"\\\" to \\\"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" failed: (13m1s) 2020-10-13T23:07:57 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/9/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (4m49s) 2020-10-13T23:12:46 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/10/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (30.8s) 2020-10-13T23:13:17 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/11/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (12.8s) 2020-10-13T23:13:30 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/12/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m23s) 2020-10-13T23:14:52 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/13/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-10-13T23:15:21 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/14/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T23:15:50 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/15/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (31.3s) 2020-10-13T23:16:21 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/16/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (15.3s) 2020-10-13T23:16:36 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/17/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (49.3s) 2020-10-13T23:17:26 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/18/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m43s) 2020-10-13T23:20:09 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/19/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m27s) 2020-10-13T23:22:36 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/20/79) \\\"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (58.1s) 2020-10-13T23:23:34 \\\"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/21/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (38.1s) 2020-10-13T23:24:12 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/22/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-10-13T23:24:41 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/23/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.1s) 2020-10-13T23:24:55 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/24/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.3s) 2020-10-13T23:25:09 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/25/79) \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m47s) 2020-10-13T23:26:56 \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/26/79) \\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m2s) 2020-10-13T23:27:58 \\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/27/79) \\\"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\\\" passed: (53.5s) 2020-10-13T23:28:52 \\\"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\\\" started: (1/28/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (11.5s) 2020-10-13T23:29:03 \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/29/79) \\\"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m32s) 2020-10-13T23:30:35 \\\"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/30/79) \\\"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\\\" passed: (29.2s) 2020-10-13T23:31:04 \\\"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\\\" started: (1/31/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (11.9s) 2020-10-13T23:31:16 \\\"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/32/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-10-13T23:31:45 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/33/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m32s) 2020-10-13T23:33:17 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/34/79) \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (54s) 2020-10-13T23:34:11 \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/35/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m33s) 2020-10-13T23:36:44 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/36/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T23:37:13 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/37/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m53s) 2020-10-13T23:39:06 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/38/79) \\\"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:193]: Could not get controller-manager metrics - skipping skipped: (53.7s) 2020-10-13T23:40:00 \\\"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/39/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (32.1s) 2020-10-13T23:40:32 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/40/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.1s) 2020-10-13T23:40:46 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/41/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T23:41:15 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/42/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.3s) 2020-10-13T23:41:29 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/43/79) \\\"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m10s) 2020-10-13T23:42:39 \\\"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/44/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29s) 2020-10-13T23:43:08 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/45/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-10-13T23:43:36 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/46/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-10-13T23:44:05 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/47/79) \\\"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (11.7s) 2020-10-13T23:44:17 \\\"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/48/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m15s) 2020-10-13T23:46:32 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/49/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T23:47:01 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/50/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (11.5s) 2020-10-13T23:47:12 \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/51/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.7s) 2020-10-13T23:47:41 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/52/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (26.6s) 2020-10-13T23:48:08 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/53/79) \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (25.1s) 2020-10-13T23:48:33 \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/54/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (12.8s) 2020-10-13T23:48:46 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/55/79) \\\"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m47s) 2020-10-13T23:50:32 \\\"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/56/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m53s) 2020-10-13T23:52:25 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/57/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m4s) 2020-10-13T23:53:29 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/58/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (23.7s) 2020-10-13T23:53:52 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/59/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T23:54:21 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/60/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T23:54:50 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/61/79) \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m46s) 2020-10-13T23:56:36 \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/62/79) \\\"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m52s) 2020-10-13T23:58:28 \\\"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/63/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.1s) 2020-10-13T23:58:58 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/64/79) \\\"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:81]: Environment does not support getting controller-manager metrics - skipping skipped: (11.6s) 2020-10-13T23:59:09 \\\"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/65/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.2s) 2020-10-13T23:59:23 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/66/79) \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m35s) 2020-10-14T00:01:59 \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/67/79) \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (27.1s) 2020-10-14T00:02:26 \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/68/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.5s) 2020-10-14T00:02:40 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/69/79) \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (21.7s) 2020-10-14T00:03:02 \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/70/79) \\\"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/scheduling/taint_based_evictions.go:63]: Requires at least 2 nodes (not -1) skipped: (11.8s) 2020-10-14T00:03:14 \\\"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/71/79) \\\"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:247]: Could not get controller-manager metrics - skipping skipped: (19.7s) 2020-10-14T00:03:34 \\\"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/72/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.5s) 2020-10-14T00:03:48 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/73/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (11.8s) 2020-10-14T00:04:00 \\\"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/74/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-10-14T00:04:29 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/75/79) \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (57.8s) 2020-10-14T00:05:26 \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/76/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (31.2s) 2020-10-14T00:05:58 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/77/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m49s) 2020-10-14T00:07:47 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/78/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.2s) 2020-10-14T00:08:16 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/79/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-14T00:08:45 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" Timeline: Oct 13 22:51:09.831 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (5 times) Oct 13 22:51:10.678 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (5 times) Oct 13 22:51:11.532 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (5 times) Oct 13 22:51:11.827 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (8 times) Oct 13 22:51:11.992 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (8 times) Oct 13 22:51:12.185 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (8 times) Oct 13 22:54:57.400 - 780s I test=\\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" running Oct 13 22:58:02.943 W clusteroperator/console changed Progressing to True: ProgressingSynchronizationError: Progressing: oauth client for console does not exist and cannot be created Oct 13 22:58:02.952 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"\\\" to \\\"Degraded: oauth client for console does not exist and cannot be created\\\",Progressing changed from False to True (\\\"Progressing: oauth client for console does not exist and cannot be created\\\") Oct 13 22:59:02.975 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"Degraded: oauth client for console does not exist and cannot be created\\\" to \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\",Progressing message changed from \\\"Progressing: oauth client for console does not exist and cannot be created\\\" to \\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\" Oct 13 22:59:40.168 W clusteroperator/openshift-apiserver changed Available to False: AvailableMultiple: Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.authorization.openshift.io is not ready: 0\\\\ Available: v1.oauth.openshift.io is not ready: 0\\\\ Available: v1.project.openshift.io is not ready: 0\\\\ Available: v1.security.openshift.io is not ready: 0 Oct 13 22:59:40.178 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available changed from True to False (\\\"Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.authorization.openshift.io is not ready: 0\\\\ Available: v1.oauth.openshift.io is not ready: 0\\\\ Available: v1.project.openshift.io is not ready: 0\\\\ Available: v1.security.openshift.io is not ready: 0\\\") Oct 13 23:00:01.369 W clusteroperator/monitoring changed Available to False Oct 13 23:00:01.369 E clusteroperator/monitoring changed Degraded to True: UpdatingconfigurationsharingFailed: Failed to rollout the stack. Error: running task Updating configuration sharing failed: failed to retrieve Grafana host: getting Route object failed: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io grafana) Oct 13 23:00:01.390 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:00:03.023 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\" to \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\",Progressing message changed from \\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\" to \\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\" Oct 13 23:00:03.097 W clusteroperator/console changed Progressing to False Oct 13 23:00:03.101 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\" to \\\"\\\",Progressing changed from True to False (\\\"\\\") Oct 13 23:00:20.262 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \\\"\\\" to \\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\" Oct 13 23:01:03.117 W clusteroperator/console changed Progressing to True: ProgressingSynchronizationError: Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console) Oct 13 23:01:03.125 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"\\\" to \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\",Progressing changed from False to True (\\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\") Oct 13 23:01:09.039 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (9 times) Oct 13 23:01:10.012 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (6 times) Oct 13 23:01:11.011 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (6 times) Oct 13 23:01:12.176 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (6 times) Oct 13 23:01:12.369 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (9 times) Oct 13 23:01:12.508 W clusteroperator/monitoring changed Progressing to False Oct 13 23:01:12.531 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:01:12.574 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (9 times) Oct 13 23:01:21.492 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\" to \\\"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" Oct 13 23:02:03.170 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\" to \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\",Progressing message changed from \\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\\\" to \\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\" (2 times) Oct 13 23:02:18.622 W clusteroperator/monitoring changed Progressing to False Oct 13 23:02:18.642 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:02:21.528 E clusteroperator/authentication changed Degraded to True: MultipleConditionsMatching: RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client) Oct 13 23:02:21.533 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from False to True (\\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\") Oct 13 23:02:52.456 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \\\"Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.authorization.openshift.io is not ready: 0\\\\ Available: v1.oauth.openshift.io is not ready: 0\\\\ Available: v1.project.openshift.io is not ready: 0\\\\ Available: v1.security.openshift.io is not ready: 0\\\" to \\\"Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.build.openshift.io is not ready: 0\\\\ Available: v1.quota.openshift.io is not ready: 0\\\\ Available: v1.route.openshift.io is not ready: 0\\\\ Available: v1.security.openshift.io is not ready: 0\\\\ Available: v1.template.openshift.io is not ready: 0\\\" Oct 13 23:03:03.212 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\" to \\\"Degraded: oauth client for console does not exist and cannot be created\\\",Progressing message changed from \\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\" to \\\"Progressing: oauth client for console does not exist and cannot be created\\\" Oct 13 23:03:22.744 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" to \\\"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" Oct 13 23:03:23.767 W clusteroperator/monitoring changed Progressing to False Oct 13 23:03:23.788 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:04:22.761 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \\\"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" to \\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" Oct 13 23:04:44.727 W clusteroperator/monitoring changed Progressing to False Oct 13 23:04:44.747 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:05:23.991 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" to \\\"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" (2 times) Oct 13 23:05:25.224 W clusteroperator/authentication changed Degraded to False Oct 13 23:05:25.229 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from True to False (\\\"\\\") Oct 13 23:05:32.733 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \\\"Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.build.openshift.io is not ready: 0\\\\ Available: v1.quota.openshift.io is not ready: 0\\\\ Available: v1.route.openshift.io is not ready: 0\\\\ Available: v1.security.openshift.io is not ready: 0\\\\ Available: v1.template.openshift.io is not ready: 0\\\" to \\\"Available: v1.authorization.openshift.io is not ready: 0\\\\ Available: v1.build.openshift.io is not ready: 0\\\\ Available: v1.security.openshift.io is not ready: 0\\\\ Available: v1.template.openshift.io is not ready: 0\\\\ Available: v1.user.openshift.io is not ready: 0\\\" Oct 13 23:06:47.587 W clusteroperator/monitoring changed Progressing to False Oct 13 23:06:47.610 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:07:30.608 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \\\"\\\" to \\\"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\" Oct 13 23:07:57.886 I test=\\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" failed Oct 13 23:08:05.116 W clusteroperator/monitoring changed Progressing to False Oct 13 23:08:05.139 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:08:13.006 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \\\"Available: v1.authorization.openshift.io is not ready: 0\\\\ Available: v1.build.openshift.io is not ready: 0\\\\ Available: v1.security.openshift.io is not ready: 0\\\\ Available: v1.template.openshift.io is not ready: 0\\\\ Available: v1.user.openshift.io is not ready: 0\\\" to \\\"Available: v1.build.openshift.io is not ready: 0\\\\ Available: v1.image.openshift.io is not ready: 0\\\\ Available: v1.oauth.openshift.io is not ready: 0\\\\ Available: v1.quota.openshift.io is not ready: 0\\\\ Available: v1.route.openshift.io is not ready: 0\\\" Oct 13 23:08:30.618 E clusteroperator/authentication changed Degraded to True: MultipleConditionsMatching: RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client) Oct 13 23:08:30.626 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from False to True (\\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\\\") (2 times) Oct 13 23:09:17.292 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \\\"Available: v1.build.openshift.io is not ready: 0\\\\ Available: v1.image.openshift.io is not ready: 0\\\\ Available: v1.oauth.openshift.io is not ready: 0\\\\ Available: v1.quota.openshift.io is not ready: 0\\\\ Available: v1.route.openshift.io is not ready: 0\\\" to \\\"Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.oauth.openshift.io is not ready: 0\\\" Oct 13 23:09:31.682 W clusteroperator/authentication changed Degraded to False Oct 13 23:09:31.690 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from True to False (\\\"\\\") (2 times) Oct 13 23:10:03.456 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"Degraded: oauth client for console does not exist and cannot be created\\\" to \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\",Progressing message changed from \\\"Progressing: oauth client for console does not exist and cannot be created\\\" to \\\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\" Oct 13 23:10:08.177 W clusteroperator/monitoring changed Progressing to False Oct 13 23:10:08.198 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:10:21.567 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \\\"Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.oauth.openshift.io is not ready: 0\\\" to \\\"Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.template.openshift.io is not ready: 0\\\" Oct 13 23:10:37.884 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \\\"\\\" to \\\"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-challenging-client)\\\" Oct 13 23:11:03.546 W clusteroperator/console changed Progressing to False Oct 13 23:11:03.555 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\\\" to \\\"\\\",Progressing changed from True to False (\\\"\\\") (2 times) Oct 13 23:11:09.005 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (10 times) Oct 13 23:11:09.363 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (10 times) Oct 13 23:11:10.443 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (7 times) Oct 13 23:11:11.408 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (7 times) Oct 13 23:11:12.478 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (7 times) Oct 13 23:11:12.656 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (10 times) Oct 13 23:11:19.322 W clusteroperator/monitoring changed Progressing to False Oct 13 23:11:19.344 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:11:37.891 E clusteroperator/authentication changed Degraded to True: MultipleConditionsMatching: RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-challenging-client) Oct 13 23:11:37.898 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from False to True (\\\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-challenging-client)\\\") Oct 13 23:12:03.713 W clusteroperator/console changed Progressing to True: ProgressingSynchronizationError: Progressing: oauth client for console does not exist and cannot be created Oct 13 23:12:03.723 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"\\\" to \\\"Degraded: oauth client for console does not exist and cannot be created\\\",Progressing changed from False to True (\\\"Progressing: oauth client for console does not exist and cannot be created\\\") (2 times) Oct 13 23:12:03.775 W clusteroperator/console changed Progressing to False Oct 13 23:12:03.781 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \\\"Degraded: oauth client for console does not exist and cannot be created\\\" to \\\"\\\",Progressing changed from True to False (\\\"\\\") Oct 13 23:12:29.877 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \\\"Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.template.openshift.io is not ready: 0\\\" to \\\"Available: v1.apps.openshift.io is not ready: 0\\\\ Available: v1.authorization.openshift.io is not ready: 0\\\\ Available: v1.project.openshift.io is not ready: 0\\\\ Available: v1.route.openshift.io is not ready: 0\\\" Oct 13 23:12:30.357 W clusteroperator/openshift-apiserver changed Available to True Oct 13 23:12:30.374 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available changed from False to True (\\\"\\\") (3 times) Oct 13 23:12:30.773 W clusteroperator/authentication changed Degraded to False Oct 13 23:12:30.791 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from True to False (\\\"\\\") (3 times) Oct 13 23:12:32.204 W ns/openshift-machine-config-operator pod/machine-config-daemon-w5j8c node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 13 23:12:32.216 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-w5j8c Oct 13 23:12:32.216 I ns/openshift-machine-config-operator pod/machine-config-daemon-w5j8c Stopping container machine-config-daemon Oct 13 23:12:32.216 W ns/openshift-image-registry pod/node-ca-gdcq5 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:12:32.216 I ns/openshift-image-registry pod/node-ca-gdcq5 Stopping container node-ca Oct 13 23:12:32.226 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-gdcq5 Oct 13 23:12:32.235 I ns/openshift-console pod/downloads-795f496c64-w8fkj Marking for deletion Pod openshift-console/downloads-795f496c64-w8fkj Oct 13 23:12:32.239 W ns/openshift-console pod/downloads-795f496c64-w8fkj node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:12:32.243 I ns/openshift-image-registry pod/node-ca-gdcq5 Marking for deletion Pod openshift-image-registry/node-ca-gdcq5 Oct 13 23:12:32.253 W ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:12:32.253 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np Marking for deletion Pod openshift-monitoring/prometheus-adapter-b6fb584c8-877np Oct 13 23:12:32.253 W ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:12:32.254 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 0s Oct 13 23:12:32.256 I ns/openshift-console pod/downloads-795f496c64-w8fkj Stopping container download-server Oct 13 23:12:32.263 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:12:32.274 I ns/openshift-monitoring pod/alertmanager-main-0 Marking for deletion Pod openshift-monitoring/alertmanager-main-0 Oct 13 23:12:32.274 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np Stopping container prometheus-adapter Oct 13 23:12:32.283 I ns/openshift-monitoring pod/grafana-649f787944-8znfk Stopping container grafana-proxy Oct 13 23:12:32.283 I ns/openshift-machine-config-operator pod/machine-config-daemon-w5j8c Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-w5j8c Oct 13 23:12:32.287 I ns/openshift-monitoring pod/grafana-649f787944-8znfk Stopping container grafana Oct 13 23:12:32.289 I ns/openshift-monitoring pod/grafana-649f787944-8znfk Marking for deletion Pod openshift-monitoring/grafana-649f787944-8znfk Oct 13 23:12:32.295 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy Oct 13 23:12:32.297 I ns/openshift-monitoring pod/alertmanager-main-0 Cancelling deletion of Pod openshift-monitoring/alertmanager-main-0 Oct 13 23:12:32.300 I ns/openshift-console pod/downloads-795f496c64-97mxr node/ created Oct 13 23:12:32.305 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader Oct 13 23:12:32.311 I ns/openshift-console replicaset/downloads-795f496c64 Created pod: downloads-795f496c64-97mxr Oct 13 23:12:32.315 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager Oct 13 23:12:32.318 I ns/openshift-console pod/downloads-795f496c64-97mxr Successfully assigned openshift-console/downloads-795f496c64-97mxr to ip-10-0-141-37.us-east-2.compute.internal Oct 13 23:12:32.319 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-94mld node/ created Oct 13 23:12:32.331 I ns/openshift-monitoring replicaset/prometheus-adapter-b6fb584c8 Created pod: prometheus-adapter-b6fb584c8-94mld Oct 13 23:12:32.333 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy (2 times) Oct 13 23:12:32.346 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-94mld Successfully assigned openshift-monitoring/prometheus-adapter-b6fb584c8-94mld to ip-10-0-141-37.us-east-2.compute.internal Oct 13 23:12:32.363 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 node/ created Oct 13 23:12:32.368 I ns/openshift-monitoring replicaset/grafana-649f787944 Created pod: grafana-649f787944-jm8n9 Oct 13 23:12:32.392 I ns/openshift-monitoring pod/alertmanager-main-0 node/ created Oct 13 23:12:32.397 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Successfully assigned openshift-monitoring/grafana-649f787944-jm8n9 to ip-10-0-141-37.us-east-2.compute.internal Oct 13 23:12:32.410 I ns/openshift-monitoring statefulset/alertmanager-main create Pod alertmanager-main-0 in StatefulSet alertmanager-main successful (2 times) Oct 13 23:12:32.410 I ns/openshift-monitoring pod/alertmanager-main-0 Successfully assigned openshift-monitoring/alertmanager-main-0 to ip-10-0-157-185.us-east-2.compute.internal Oct 13 23:12:32.431 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader (2 times) Oct 13 23:12:32.612 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager (2 times) Oct 13 23:12:34.015 W ns/openshift-image-registry pod/node-ca-gdcq5 node/ip-10-0-131-112.us-east-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 13 23:12:34.015 W ns/openshift-image-registry pod/node-ca-gdcq5 node/ip-10-0-131-112.us-east-2.compute.internal container=node-ca container stopped being ready Oct 13 23:12:34.329 W ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np node/ip-10-0-131-112.us-east-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 13 23:12:34.329 W ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np node/ip-10-0-131-112.us-east-2.compute.internal container=prometheus-adapter container stopped being ready Oct 13 23:12:35.130 W ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 13 23:12:35.130 W ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal container=grafana-proxy container stopped being ready Oct 13 23:12:35.130 W ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal container=grafana container stopped being ready Oct 13 23:12:35.532 E ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal container=grafana container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 13 23:12:35.532 E ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal container=grafana-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 13 23:12:36.533 W ns/openshift-image-registry pod/node-ca-gdcq5 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:12:37.026 W clusteroperator/monitoring changed Progressing to False Oct 13 23:12:37.048 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:12:37.844 W ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np node/ip-10-0-131-112.us-east-2.compute.internal pod has been pending longer than a minute Oct 13 23:12:37.844 W ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal pod has been pending longer than a minute Oct 13 23:12:38.126 W ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:12:38.225 I ns/openshift-image-registry pod/node-ca-gb5x2 node/ created Oct 13 23:12:38.232 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-gb5x2 Oct 13 23:12:38.235 I ns/openshift-image-registry pod/node-ca-gb5x2 Successfully assigned openshift-image-registry/node-ca-gb5x2 to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:12:38.540 W ns/openshift-machine-config-operator pod/machine-config-daemon-w5j8c node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:12:38.549 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 node/ created Oct 13 23:12:38.554 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-lghv9 Oct 13 23:12:38.560 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 Successfully assigned openshift-machine-config-operator/machine-config-daemon-lghv9 to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:12:39.851 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b9d07e8eed67a34b50a3afaa36cd4b9f052697894e64b5c203d0675d12ab5ec\\\" already present on machine Oct 13 23:12:40.031 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager Oct 13 23:12:40.062 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager Oct 13 23:12:40.067 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Oct 13 23:12:40.135 W ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:12:40.253 I ns/openshift-monitoring pod/alertmanager-main-0 Created container config-reloader Oct 13 23:12:40.277 I ns/openshift-monitoring pod/alertmanager-main-0 Started container config-reloader Oct 13 23:12:40.285 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Oct 13 23:12:40.300 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 13 23:12:40.414 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 Created container machine-config-daemon Oct 13 23:12:40.438 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 Started container machine-config-daemon Oct 13 23:12:40.448 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager-proxy Oct 13 23:12:40.477 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager-proxy Oct 13 23:12:42.662 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-94mld Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\\\" Oct 13 23:12:43.396 I ns/openshift-console pod/downloads-795f496c64-97mxr Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e7c14a3e743a80bf16d36154e823343f7d50f54dbcb914daab6013381bad8fc5\\\" already present on machine Oct 13 23:12:43.543 I ns/openshift-console pod/downloads-795f496c64-97mxr Created container download-server Oct 13 23:12:43.568 I ns/openshift-console pod/downloads-795f496c64-97mxr Started container download-server Oct 13 23:12:43.771 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c229ac6674c217359c243e4621aae565c2af46caaf231a077b7f7e3d6f07ef5a\\\" Oct 13 23:12:45.079 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-94mld Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\\\" Oct 13 23:12:45.223 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-94mld Created container prometheus-adapter Oct 13 23:12:45.245 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-94mld Started container prometheus-adapter Oct 13 23:12:47.782 I ns/openshift-image-registry pod/node-ca-gb5x2 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 13 23:12:47.907 I ns/openshift-image-registry pod/node-ca-gb5x2 Created container node-ca Oct 13 23:12:47.931 I ns/openshift-image-registry pod/node-ca-gb5x2 Started container node-ca Oct 13 23:12:49.666 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c229ac6674c217359c243e4621aae565c2af46caaf231a077b7f7e3d6f07ef5a\\\" Oct 13 23:12:49.845 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Created container grafana Oct 13 23:12:49.868 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Started container grafana Oct 13 23:12:49.875 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Oct 13 23:12:50.018 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Created container grafana-proxy Oct 13 23:12:50.047 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Started container grafana-proxy Oct 13 23:13:02.466 W clusteroperator/monitoring changed Available to True: RollOutDone: Successfully rolled out the stack. Oct 13 23:13:02.466 W clusteroperator/monitoring changed Degraded to False Oct 13 23:13:02.466 W clusteroperator/monitoring changed Progressing to False Oct 13 23:13:03.079 E ns/openshift-console pod/downloads-795f496c64-w8fkj node/ip-10-0-131-112.us-east-2.compute.internal container=download-server container exited with code 137 (Error): Oct 13 23:13:08.126 W ns/openshift-console pod/downloads-795f496c64-w8fkj node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:18:30.044 W ns/openshift-image-registry pod/node-ca-gb5x2 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:18:30.045 W ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 13 23:18:30.057 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-gb5x2 Oct 13 23:18:30.057 I ns/openshift-image-registry pod/node-ca-gb5x2 Stopping container node-ca Oct 13 23:18:30.057 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-lghv9 Oct 13 23:18:30.057 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 Stopping container machine-config-daemon Oct 13 23:18:30.063 I ns/openshift-image-registry pod/node-ca-gb5x2 Marking for deletion Pod openshift-image-registry/node-ca-gb5x2 Oct 13 23:18:30.067 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-lghv9 Oct 13 23:18:38.128 W ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:18:38.146 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-qwphm Oct 13 23:18:38.147 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm node/ created Oct 13 23:18:38.151 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm Successfully assigned openshift-machine-config-operator/machine-config-daemon-qwphm to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:18:38.151 W ns/openshift-image-registry pod/node-ca-gb5x2 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:18:38.172 I ns/openshift-image-registry pod/node-ca-hgsz8 node/ created Oct 13 23:18:38.176 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-hgsz8 Oct 13 23:18:38.183 I ns/openshift-image-registry pod/node-ca-hgsz8 Successfully assigned openshift-image-registry/node-ca-hgsz8 to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:18:40.236 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 13 23:18:40.362 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm Created container machine-config-daemon Oct 13 23:18:40.399 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm Started container machine-config-daemon Oct 13 23:18:48.482 I ns/openshift-image-registry pod/node-ca-hgsz8 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 13 23:18:48.611 I ns/openshift-image-registry pod/node-ca-hgsz8 Created container node-ca Oct 13 23:18:48.633 I ns/openshift-image-registry pod/node-ca-hgsz8 Started container node-ca Oct 13 23:21:09.970 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (8 times) Oct 13 23:21:11.024 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (8 times) Oct 13 23:21:11.888 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (8 times) Oct 13 23:21:12.114 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (11 times) Oct 13 23:21:12.329 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (11 times) Oct 13 23:21:12.531 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (11 times) Oct 13 23:21:13.104 W ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 13 23:21:13.105 W ns/openshift-image-registry pod/node-ca-hgsz8 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:21:13.115 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-qwphm Oct 13 23:21:13.115 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm Stopping container machine-config-daemon Oct 13 23:21:13.115 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-hgsz8 Oct 13 23:21:13.118 I ns/openshift-image-registry pod/node-ca-hgsz8 Stopping container node-ca Oct 13 23:21:13.121 I ns/openshift-image-registry pod/node-ca-hgsz8 Marking for deletion Pod openshift-image-registry/node-ca-hgsz8 Oct 13 23:21:13.124 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-qwphm Oct 13 23:21:14.929 W ns/openshift-image-registry pod/node-ca-hgsz8 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:21:15.031 W ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:22:28.192 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 node/ created Oct 13 23:22:28.201 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-4wlc2 Oct 13 23:22:28.205 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 Successfully assigned openshift-machine-config-operator/machine-config-daemon-4wlc2 to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:22:28.215 I ns/openshift-image-registry pod/node-ca-vghrz node/ created Oct 13 23:22:28.219 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-vghrz Oct 13 23:22:28.225 I ns/openshift-image-registry pod/node-ca-vghrz Successfully assigned openshift-image-registry/node-ca-vghrz to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:22:28.855 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 13 23:22:28.982 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 Created container machine-config-daemon Oct 13 23:22:29.016 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 Started container machine-config-daemon Oct 13 23:22:36.245 I ns/openshift-image-registry pod/node-ca-vghrz Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 13 23:22:36.364 I ns/openshift-image-registry pod/node-ca-vghrz Created container node-ca Oct 13 23:22:36.386 I ns/openshift-image-registry pod/node-ca-vghrz Started container node-ca Oct 13 23:31:09.762 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (9 times) Oct 13 23:31:10.585 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (9 times) Oct 13 23:31:11.373 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (9 times) Oct 13 23:31:11.572 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (12 times) Oct 13 23:31:11.742 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (12 times) Oct 13 23:31:11.917 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (12 times) Oct 13 23:35:15.472 W ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 13 23:35:15.474 W ns/openshift-image-registry pod/node-ca-vghrz node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:35:15.484 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 Stopping container machine-config-daemon Oct 13 23:35:15.484 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-4wlc2 Oct 13 23:35:15.487 I ns/openshift-image-registry pod/node-ca-vghrz Marking for deletion Pod openshift-image-registry/node-ca-vghrz Oct 13 23:35:15.487 I ns/openshift-image-registry pod/node-ca-vghrz Stopping container node-ca Oct 13 23:35:15.488 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-vghrz Oct 13 23:35:15.490 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-4wlc2 Oct 13 23:35:17.004 W ns/openshift-image-registry pod/node-ca-vghrz node/ip-10-0-131-112.us-east-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 13 23:35:17.004 W ns/openshift-image-registry pod/node-ca-vghrz node/ip-10-0-131-112.us-east-2.compute.internal container=node-ca container stopped being ready Oct 13 23:35:22.844 W ns/openshift-image-registry pod/node-ca-vghrz node/ip-10-0-131-112.us-east-2.compute.internal pod has been pending longer than a minute Oct 13 23:35:28.124 W ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:35:28.145 W ns/openshift-image-registry pod/node-ca-vghrz node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:36:20.563 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 node/ created Oct 13 23:36:20.575 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-2mdk8 Oct 13 23:36:20.581 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 Successfully assigned openshift-machine-config-operator/machine-config-daemon-2mdk8 to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:36:20.588 I ns/openshift-image-registry pod/node-ca-4276f node/ created Oct 13 23:36:20.596 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4276f Oct 13 23:36:20.598 I ns/openshift-image-registry pod/node-ca-4276f Successfully assigned openshift-image-registry/node-ca-4276f to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:36:21.236 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 13 23:36:21.359 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 Created container machine-config-daemon Oct 13 23:36:21.384 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 Started container machine-config-daemon Oct 13 23:36:28.162 I ns/openshift-image-registry pod/node-ca-4276f Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 13 23:36:28.277 I ns/openshift-image-registry pod/node-ca-4276f Created container node-ca Oct 13 23:36:28.308 I ns/openshift-image-registry pod/node-ca-4276f Started container node-ca Oct 13 23:39:48.299 W persistentvolume/pvc-4a791b95-0dad-11eb-8499-0282592b2e9e Error deleting EBS volume \\\"vol-07767b7983fe690d7\\\" since volume is currently attached to \\\"i-04986068922e9122c\\\" Oct 13 23:41:09.815 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (10 times) Oct 13 23:41:10.006 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (13 times) Oct 13 23:41:10.171 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (13 times) Oct 13 23:41:10.429 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (13 times) Oct 13 23:41:11.261 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (10 times) Oct 13 23:41:12.100 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (10 times) Oct 13 23:44:08.883 I ns/kube-system pod/pod0-system-node-critical node/ created Oct 13 23:44:08.895 I ns/kube-system pod/pod0-system-node-critical Successfully assigned kube-system/pod0-system-node-critical to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:44:08.910 I ns/kube-system pod/pod1-system-cluster-critical node/ created Oct 13 23:44:08.918 I ns/kube-system pod/pod1-system-cluster-critical Successfully assigned kube-system/pod1-system-cluster-critical to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:44:08.939 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 0s Oct 13 23:44:08.942 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:44:08.970 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 0s Oct 13 23:44:08.974 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:46:11.911 W ns/kube-system pod/pod0-system-node-critical Unable to mount volumes for pod \\\"pod0-system-node-critical_kube-system(fcea83e2-0dad-11eb-b1a9-02de12e903fe)\\\": timeout expired waiting for volumes to attach or mount for pod \\\"kube-system\\\"/\\\"pod0-system-node-critical\\\". list of unmounted volumes=[default-token-wn8pl]. list of unattached volumes=[default-token-wn8pl] Oct 13 23:46:11.933 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \\\"pod1-system-cluster-critical_kube-system(fceef1c6-0dad-11eb-b1a9-02de12e903fe)\\\": timeout expired waiting for volumes to attach or mount for pod \\\"kube-system\\\"/\\\"pod1-system-cluster-critical\\\". list of unmounted volumes=[default-token-wn8pl]. list of unattached volumes=[default-token-wn8pl] Oct 13 23:48:23.491 I ns/openshift-image-registry pod/node-ca-4276f Stopping container node-ca Oct 13 23:48:23.491 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 Stopping container machine-config-daemon Oct 13 23:48:23.491 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4276f Oct 13 23:48:23.491 W ns/openshift-image-registry pod/node-ca-4276f node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:48:23.491 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-2mdk8 Oct 13 23:48:23.491 W ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 13 23:48:23.494 I ns/openshift-image-registry pod/node-ca-4276f Marking for deletion Pod openshift-image-registry/node-ca-4276f Oct 13 23:48:23.498 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-2mdk8 Oct 13 23:48:28.125 W ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:48:28.144 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-7dqvc Oct 13 23:48:28.145 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc node/ created Oct 13 23:48:28.148 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc Successfully assigned openshift-machine-config-operator/machine-config-daemon-7dqvc to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:48:28.149 W ns/openshift-image-registry pod/node-ca-4276f node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:48:28.169 I ns/openshift-image-registry pod/node-ca-pbk85 node/ created Oct 13 23:48:28.173 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-pbk85 Oct 13 23:48:28.178 I ns/openshift-image-registry pod/node-ca-pbk85 Successfully assigned openshift-image-registry/node-ca-pbk85 to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:48:29.603 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 13 23:48:29.711 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc Created container machine-config-daemon Oct 13 23:48:29.733 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc Started container machine-config-daemon Oct 13 23:48:37.414 I ns/openshift-image-registry pod/node-ca-pbk85 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 13 23:48:37.540 I ns/openshift-image-registry pod/node-ca-pbk85 Created container node-ca Oct 13 23:48:37.565 I ns/openshift-image-registry pod/node-ca-pbk85 Started container node-ca Oct 13 23:51:09.730 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (11 times) Oct 13 23:51:10.542 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (11 times) Oct 13 23:51:11.549 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (11 times) Oct 13 23:51:11.737 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (14 times) Oct 13 23:51:11.900 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (14 times) Oct 13 23:51:12.091 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (14 times) Oct 13 23:51:46.921 I ns/kube-system pod/critical-pod node/ created Oct 13 23:51:46.929 W ns/kube-system pod/critical-pod 0/6 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 3 node(s) had taints that the pod didn\\\\'t tolerate. Oct 13 23:51:46.950 W ns/kube-system pod/critical-pod 0/6 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 3 node(s) had taints that the pod didn\\\\'t tolerate. (2 times) Oct 13 23:51:58.137 I ns/kube-system pod/critical-pod Successfully assigned kube-system/critical-pod to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:52:05.975 I ns/kube-system pod/critical-pod Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Oct 13 23:52:06.110 I ns/kube-system pod/critical-pod Created container critical-pod Oct 13 23:52:06.129 I ns/kube-system pod/critical-pod Started container critical-pod Oct 13 23:52:07.115 W ns/kube-system pod/critical-pod node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 0s Oct 13 23:52:07.118 W ns/kube-system pod/critical-pod node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:52:07.691 I ns/kube-system pod/critical-pod Pod sandbox changed, it will be killed and re-created. Oct 13 23:52:17.295 W ns/kube-system pod/critical-pod Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_critical-pod_kube-system_0dede5b8-0daf-11eb-8499-0282592b2e9e_1(ef346ef22eb280f6b4369c549be3d57fff13fb2d286c3e9fe48380dd01f49636): Multus: Err adding pod to network \\\"openshift-sdn\\\": cannot set \\\"openshift-sdn\\\" ifname to \\\"eth0\\\": no netns: failed to Statfs \\\"/proc/131341/ns/net\\\": no such file or directory Oct 13 23:55:54.651 W ns/openshift-image-registry pod/node-ca-pbk85 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:55:54.654 W ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 13 23:55:54.667 I ns/openshift-image-registry pod/node-ca-pbk85 Stopping container node-ca Oct 13 23:55:54.667 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-7dqvc Oct 13 23:55:54.667 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc Stopping container machine-config-daemon Oct 13 23:55:54.669 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-7dqvc Oct 13 23:55:54.670 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-pbk85 Oct 13 23:55:54.674 I ns/openshift-image-registry pod/node-ca-pbk85 Marking for deletion Pod openshift-image-registry/node-ca-pbk85 Oct 13 23:55:56.213 W ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:55:56.231 W ns/openshift-image-registry pod/node-ca-pbk85 node/ip-10-0-131-112.us-east-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 13 23:55:56.231 W ns/openshift-image-registry pod/node-ca-pbk85 node/ip-10-0-131-112.us-east-2.compute.internal container=node-ca container stopped being ready Oct 13 23:56:07.844 W ns/openshift-image-registry pod/node-ca-pbk85 node/ip-10-0-131-112.us-east-2.compute.internal pod has been pending longer than a minute Oct 13 23:56:08.145 W ns/openshift-image-registry pod/node-ca-pbk85 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:56:28.198 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf node/ created Oct 13 23:56:28.204 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-rxxtf Oct 13 23:56:28.207 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf Successfully assigned openshift-machine-config-operator/machine-config-daemon-rxxtf to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:56:28.217 I ns/openshift-image-registry pod/node-ca-5w4xw node/ created Oct 13 23:56:28.222 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-5w4xw Oct 13 23:56:28.227 I ns/openshift-image-registry pod/node-ca-5w4xw Successfully assigned openshift-image-registry/node-ca-5w4xw to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:56:28.881 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 13 23:56:29.032 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf Created container machine-config-daemon Oct 13 23:56:29.064 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf Started container machine-config-daemon Oct 13 23:56:36.440 I ns/openshift-image-registry pod/node-ca-5w4xw Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 13 23:56:36.566 I ns/openshift-image-registry pod/node-ca-5w4xw Created container node-ca Oct 13 23:56:36.596 I ns/openshift-image-registry pod/node-ca-5w4xw Started container node-ca Oct 14 00:00:27.730 W ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 14 00:00:27.733 W ns/openshift-image-registry pod/node-ca-5w4xw node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 14 00:00:27.738 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf Stopping container machine-config-daemon Oct 14 00:00:27.742 I ns/openshift-image-registry pod/node-ca-5w4xw Stopping container node-ca Oct 14 00:00:27.745 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-5w4xw Oct 14 00:00:27.750 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-rxxtf Oct 14 00:00:27.751 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-rxxtf Oct 14 00:00:27.755 I ns/openshift-image-registry pod/node-ca-5w4xw Marking for deletion Pod openshift-image-registry/node-ca-5w4xw Oct 14 00:00:38.126 W ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 14 00:00:38.156 W ns/openshift-image-registry pod/node-ca-5w4xw node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 14 00:01:09.797 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (12 times) Oct 14 00:01:10.722 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (12 times) Oct 14 00:01:11.588 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (12 times) Oct 14 00:01:11.803 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (15 times) Oct 14 00:01:11.991 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (15 times) Oct 14 00:01:12.177 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (15 times) Oct 14 00:01:32.826 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq node/ created Oct 14 00:01:32.837 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-2g2jq Oct 14 00:01:32.842 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Successfully assigned openshift-machine-config-operator/machine-config-daemon-2g2jq to ip-10-0-131-112.us-east-2.compute.internal Oct 14 00:01:32.845 I ns/openshift-image-registry pod/node-ca-gr4h4 node/ created Oct 14 00:01:32.851 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-gr4h4 Oct 14 00:01:32.855 I ns/openshift-image-registry pod/node-ca-gr4h4 Successfully assigned openshift-image-registry/node-ca-gr4h4 to ip-10-0-131-112.us-east-2.compute.internal Oct 14 00:01:33.502 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 14 00:01:33.622 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Created container machine-config-daemon Oct 14 00:01:33.645 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Started container machine-config-daemon Oct 14 00:01:40.522 I ns/openshift-image-registry pod/node-ca-gr4h4 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 14 00:01:40.641 I ns/openshift-image-registry pod/node-ca-gr4h4 Created container node-ca Oct 14 00:01:40.663 I ns/openshift-image-registry pod/node-ca-gr4h4 Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201014-000845.xml error: 1 fail, 39 pass, 39 skip (1h23m8s) 2020/10/14 00:08:46 Container test in pod e2e-aws-serial failed, exit code 1, reason Error 2020/10/14 00:14:25 Copied 120.93MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/14 00:14:25 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/14 00:14:25 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/14 00:14:25 Ran for 2h4m17s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-xh6icmvm/e2e-aws-serial failed after 1h59m29s (failed containers: test): ContainerFailed one or more containers exited Container test exited with code 1, reason Error --- 5 times) Oct 14 00:01:32.826 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq node/ created Oct 14 00:01:32.837 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-2g2jq Oct 14 00:01:32.842 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Successfully assigned openshift-machine-config-operator/machine-config-daemon-2g2jq to ip-10-0-131-112.us-east-2.compute.internal Oct 14 00:01:32.845 I ns/openshift-image-registry pod/node-ca-gr4h4 node/ created Oct 14 00:01:32.851 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-gr4h4 Oct 14 00:01:32.855 I ns/openshift-image-registry pod/node-ca-gr4h4 Successfully assigned openshift-image-registry/node-ca-gr4h4 to ip-10-0-131-112.us-east-2.compute.internal Oct 14 00:01:33.502 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 14 00:01:33.622 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Created container machine-config-daemon Oct 14 00:01:33.645 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Started container machine-config-daemon Oct 14 00:01:40.522 I ns/openshift-image-registry pod/node-ca-gr4h4 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 14 00:01:40.641 I ns/openshift-image-registry pod/node-ca-gr4h4 Created container node-ca Oct 14 00:01:40.663 I ns/openshift-image-registry pod/node-ca-gr4h4 Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201014-000845.xml error: 1 fail, 39 pass, 39 skip (1h23m8s) --- '\", \"cluster_count\": 13}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 14, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/14 22:10:47 ci-operator version v20201014-c4a0d92 2020/10/14 22:10:47 No source defined 2020/10/14 22:10:47 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/14 22:10:47 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-3qj939qp 2020/10/14 22:10:47 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/14 22:10:47 Creating namespace ci-op-3qj939qp 2020/10/14 22:10:47 Setting up pipeline imagestream for the test 2020/10/14 22:10:47 Created secret e2e-aws-serial-cluster-profile 2020/10/14 22:10:47 Created secret pull-secret 2020/10/14 22:10:47 Created PDB for pods with openshift.io/build.name label 2020/10/14 22:10:47 Created PDB for pods with created-by-ci label 2020/10/14 22:10:47 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-3qj939qp/stable:${component} 2020/10/14 22:10:50 Importing release image latest 2020/10/14 22:10:50 Executing pod \\\"release-images-latest-cli\\\" 2020/10/14 22:11:15 Executing pod \\\"release-images-latest\\\" 2020/10/14 22:12:05 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/14 22:12:05 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/14 22:12:06 Acquired lease \\\"6ff5edc7-d5b5-44ad-9414-60ff9e2dbb67\\\" for \\\"aws-quota-slice\\\" 2020/10/14 22:12:06 Executing template e2e-aws-serial 2020/10/14 22:12:06 Creating or restarting template instance 2020/10/14 22:12:06 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/14 22:12:06 Waiting for template instance to be ready 2020/10/14 22:12:08 Running pod e2e-aws-serial 2020/10/14 22:50:56 Container setup in pod e2e-aws-serial completed successfully 2020/10/15 00:14:14 Copied 114.33MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/15 00:14:14 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/15 00:14:14 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/15 00:14:15 Ran for 2h3m27s '\", \"cluster_count\": 14}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 14, \"cluster_size\": 2, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> 22:12:05 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> 22:12:05 Acquiring lease for \\\"aws-quota-slice\\\" <*> 22:12:06 Acquired lease <*> for \\\"aws-quota-slice\\\" <*> 22:12:06 Executing template e2e-aws-serial <*> 22:12:06 Creating or restarting template instance <*> 22:12:06 Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> 22:12:06 Waiting for template instance to be ready <*> 22:12:08 Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 14}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 14, \"cluster_size\": 3, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 14}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 14, \"cluster_size\": 4, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 14}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 14, \"cluster_size\": 5, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 14}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 15, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/21 22:15:26 ci-operator version v20201021-d45a03c 2020/10/21 22:15:26 No source defined 2020/10/21 22:15:26 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/21 22:15:26 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-vv1cddgl 2020/10/21 22:15:26 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/21 22:15:26 Creating namespace ci-op-vv1cddgl 2020/10/21 22:15:26 Setting up pipeline imagestream for the test 2020/10/21 22:15:26 Created secret e2e-aws-serial-cluster-profile 2020/10/21 22:15:26 Created secret pull-secret 2020/10/21 22:15:26 Created PDB for pods with openshift.io/build.name label 2020/10/21 22:15:26 Created PDB for pods with created-by-ci label 2020/10/21 22:15:26 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-vv1cddgl/stable:${component} 2020/10/21 22:15:29 Importing release image latest 2020/10/21 22:15:30 Executing pod \\\"release-images-latest-cli\\\" 2020/10/21 22:15:36 Executing pod \\\"release-images-latest\\\" 2020/10/21 22:16:23 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/21 22:16:23 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/21 22:16:23 Acquired lease \\\"6abd33c9-1f1d-4d0f-9faa-e6bbfeb06460\\\" for \\\"aws-quota-slice\\\" 2020/10/21 22:16:23 Executing template e2e-aws-serial 2020/10/21 22:16:23 Creating or restarting template instance 2020/10/21 22:16:23 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/21 22:16:23 Waiting for template instance to be ready 2020/10/21 22:16:25 Running pod e2e-aws-serial 2020/10/21 22:45:42 Container setup in pod e2e-aws-serial completed successfully secret/support created started: (0/1/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m47s) 2020-10-21T22:48:37 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/2/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.8s) 2020-10-21T22:49:18 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/3/79) \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (39s) 2020-10-21T22:49:57 \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/4/79) \\\"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\\\" skip [github.com/openshift/origin/test/extended/images/signatures.go:25]: disable because containers/image: https://github.com/containers/image/pull/570 skipped: (14.4s) 2020-10-21T22:50:12 \\\"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\\\" started: (0/5/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T22:50:52 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/6/79) \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (36.3s) 2020-10-21T22:51:28 \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/7/79) \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (2m2s) 2020-10-21T22:53:30 \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/8/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" Oct 21 22:53:31.532: INFO: >>> kubeConfig: /tmp/admin.kubeconfig Oct 21 22:53:31.534: INFO: Waiting up to 30m0s for all (but 100) nodes to be schedulable Oct 21 22:53:32.048: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace \\\\'kube-system\\\\' to be running and ready Oct 21 22:53:32.316: INFO: 0 / 0 pods in namespace \\\\'kube-system\\\\' are running and ready (0 seconds elapsed) Oct 21 22:53:32.316: INFO: expected 0 pod replicas in namespace \\\\'kube-system\\\\', 0 are Running and Ready. Oct 21 22:53:32.316: INFO: Waiting up to 5m0s for all daemonsets in namespace \\\\'kube-system\\\\' to start Oct 21 22:53:32.404: INFO: e2e test version: v1.13.4-138-g41dc99c Oct 21 22:53:32.486: INFO: kube-apiserver version: v1.13.4-138-g41dc99c [BeforeEach] [Top Level] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/test/extended/util/test.go:69 [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:153 STEP: Creating a kubernetes client Oct 21 22:53:32.488: INFO: >>> kubeConfig: /tmp/admin.kubeconfig STEP: Building a namespace api object, basename sched-priority Oct 21 22:53:37.542: INFO: About to run a Kube e2e test, ensuring namespace is privileged Oct 21 22:53:38.591: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled. STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:71 Oct 21 22:53:38.672: INFO: Waiting up to 1m0s for all nodes to be ready Oct 21 22:54:39.916: INFO: Waiting for terminating namespaces to be deleted... Oct 21 22:54:40.003: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace \\\\'kube-system\\\\' to be running and ready Oct 21 22:54:40.256: INFO: 0 / 0 pods in namespace \\\\'kube-system\\\\' are running and ready (0 seconds elapsed) Oct 21 22:54:40.256: INFO: expected 0 pod replicas in namespace \\\\'kube-system\\\\', 0 are Running and Ready. [It] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:146 Oct 21 22:54:40.256: INFO: ComputeCpuMemFraction for node: ip-10-0-142-54.us-west-1.compute.internal Oct 21 22:54:40.366: INFO: Pod for on the node: tuned-74x4c, Cpu: 10, Mem: 20971520 Oct 21 22:54:40.366: INFO: Pod for on the node: dns-default-7fm2d, Cpu: 110, Mem: 283115520 Oct 21 22:54:40.366: INFO: Pod for on the node: image-registry-7554c494f8-vn5kq, Cpu: 100, Mem: 268435456 Oct 21 22:54:40.366: INFO: Pod for on the node: node-ca-sqzxh, Cpu: 10, Mem: 10485760 Oct 21 22:54:40.366: INFO: Pod for on the node: router-default-76df456ddd-wnlxt, Cpu: 100, Mem: 268435456 Oct 21 22:54:40.366: INFO: Pod for on the node: machine-config-daemon-bhwc4, Cpu: 20, Mem: 52428800 Oct 21 22:54:40.366: INFO: Pod for on the node: certified-operators-758bf864b9-pmtnc, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.366: INFO: Pod for on the node: community-operators-6985ff7ccc-9jxc9, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.366: INFO: Pod for on the node: redhat-operators-6489d8696-7jqlz, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.366: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Oct 21 22:54:40.366: INFO: Pod for on the node: grafana-649f787944-ln2tc, Cpu: 200, Mem: 314572800 Oct 21 22:54:40.366: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-p69w5, Cpu: 300, Mem: 629145600 Oct 21 22:54:40.366: INFO: Pod for on the node: node-exporter-z499s, Cpu: 110, Mem: 230686720 Oct 21 22:54:40.366: INFO: Pod for on the node: prometheus-adapter-7b86577c4-mwcrw, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.366: INFO: Pod for on the node: telemeter-client-869d867dd8-wrt8b, Cpu: 210, Mem: 440401920 Oct 21 22:54:40.366: INFO: Pod for on the node: multus-jqjk6, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.366: INFO: Pod for on the node: ovs-wlc9p, Cpu: 200, Mem: 419430400 Oct 21 22:54:40.366: INFO: Pod for on the node: sdn-n2nlx, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.366: INFO: Node: ip-10-0-142-54.us-west-1.compute.internal, totalRequestedCpuResource: 1570, cpuAllocatableMil: 3500, cpuFraction: 0.44857142857142857 Oct 21 22:54:40.366: INFO: Node: ip-10-0-142-54.us-west-1.compute.internal, totalRequestedMemResource: 3252682752, memAllocatableVal: 16181800960, memFraction: 0.20100869860161721 Oct 21 22:54:40.366: INFO: ComputeCpuMemFraction for node: ip-10-0-143-110.us-west-1.compute.internal Oct 21 22:54:40.472: INFO: Pod for on the node: tuned-cnzrp, Cpu: 10, Mem: 20971520 Oct 21 22:54:40.472: INFO: Pod for on the node: dns-default-5mk9j, Cpu: 110, Mem: 283115520 Oct 21 22:54:40.472: INFO: Pod for on the node: node-ca-cxxsk, Cpu: 10, Mem: 10485760 Oct 21 22:54:40.472: INFO: Pod for on the node: machine-config-daemon-z9qzz, Cpu: 20, Mem: 52428800 Oct 21 22:54:40.472: INFO: Pod for on the node: node-exporter-fhpkb, Cpu: 110, Mem: 230686720 Oct 21 22:54:40.472: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Oct 21 22:54:40.472: INFO: Pod for on the node: multus-59h2z, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.472: INFO: Pod for on the node: ovs-58xn4, Cpu: 200, Mem: 419430400 Oct 21 22:54:40.472: INFO: Pod for on the node: sdn-h4mht, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.472: INFO: Node: ip-10-0-143-110.us-west-1.compute.internal, totalRequestedCpuResource: 660, cpuAllocatableMil: 3500, cpuFraction: 0.18857142857142858 Oct 21 22:54:40.472: INFO: Node: ip-10-0-143-110.us-west-1.compute.internal, totalRequestedMemResource: 1331691520, memAllocatableVal: 16181800960, memFraction: 0.08229563095552993 Oct 21 22:54:40.472: INFO: ComputeCpuMemFraction for node: ip-10-0-147-199.us-west-1.compute.internal Oct 21 22:54:40.575: INFO: Pod for on the node: tuned-dxcks, Cpu: 10, Mem: 20971520 Oct 21 22:54:40.575: INFO: Pod for on the node: dns-default-hlb8d, Cpu: 110, Mem: 283115520 Oct 21 22:54:40.575: INFO: Pod for on the node: node-ca-xzq8t, Cpu: 10, Mem: 10485760 Oct 21 22:54:40.575: INFO: Pod for on the node: router-default-76df456ddd-9f8l9, Cpu: 100, Mem: 268435456 Oct 21 22:54:40.575: INFO: Pod for on the node: machine-config-daemon-p7wps, Cpu: 20, Mem: 52428800 Oct 21 22:54:40.575: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Oct 21 22:54:40.575: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Oct 21 22:54:40.575: INFO: Pod for on the node: node-exporter-79lp8, Cpu: 110, Mem: 230686720 Oct 21 22:54:40.575: INFO: Pod for on the node: prometheus-adapter-7b86577c4-s57f8, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.575: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Oct 21 22:54:40.575: INFO: Pod for on the node: prometheus-operator-5d4588dd6-tzzgw, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.575: INFO: Pod for on the node: multus-dnd4k, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.575: INFO: Pod for on the node: olm-operators-66p8d, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.575: INFO: Pod for on the node: ovs-9dp2w, Cpu: 200, Mem: 419430400 Oct 21 22:54:40.575: INFO: Pod for on the node: sdn-tbbsf, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.575: INFO: Node: ip-10-0-147-199.us-west-1.compute.internal, totalRequestedCpuResource: 1360, cpuAllocatableMil: 3500, cpuFraction: 0.38857142857142857 Oct 21 22:54:40.575: INFO: Node: ip-10-0-147-199.us-west-1.compute.internal, totalRequestedMemResource: 2858418176, memAllocatableVal: 16181792768, memFraction: 0.17664409728770047 Oct 21 22:54:40.680: INFO: Waiting for running... Oct 21 22:54:50.877: INFO: Waiting for running... Oct 21 22:55:01.075: INFO: Waiting for running... STEP: Compute Cpu, Mem Fraction after create balanced pods. Oct 21 22:55:11.176: INFO: ComputeCpuMemFraction for node: ip-10-0-142-54.us-west-1.compute.internal Oct 21 22:55:11.680: INFO: Pod for on the node: 66fb5002-13f0-11eb-ab84-0a58ac10517a-0, Cpu: 180, Mem: 4838217728 Oct 21 22:55:11.680: INFO: Pod for on the node: tuned-74x4c, Cpu: 10, Mem: 20971520 Oct 21 22:55:11.680: INFO: Pod for on the node: dns-default-7fm2d, Cpu: 110, Mem: 283115520 Oct 21 22:55:11.680: INFO: Pod for on the node: image-registry-7554c494f8-vn5kq, Cpu: 100, Mem: 268435456 Oct 21 22:55:11.680: INFO: Pod for on the node: node-ca-sqzxh, Cpu: 10, Mem: 10485760 Oct 21 22:55:11.680: INFO: Pod for on the node: router-default-76df456ddd-wnlxt, Cpu: 100, Mem: 268435456 Oct 21 22:55:11.680: INFO: Pod for on the node: machine-config-daemon-bhwc4, Cpu: 20, Mem: 52428800 Oct 21 22:55:11.680: INFO: Pod for on the node: certified-operators-758bf864b9-pmtnc, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.680: INFO: Pod for on the node: community-operators-6985ff7ccc-9jxc9, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.680: INFO: Pod for on the node: redhat-operators-6489d8696-7jqlz, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.680: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Oct 21 22:55:11.680: INFO: Pod for on the node: grafana-649f787944-ln2tc, Cpu: 200, Mem: 314572800 Oct 21 22:55:11.680: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-p69w5, Cpu: 300, Mem: 629145600 Oct 21 22:55:11.680: INFO: Pod for on the node: node-exporter-z499s, Cpu: 110, Mem: 230686720 Oct 21 22:55:11.680: INFO: Pod for on the node: prometheus-adapter-7b86577c4-mwcrw, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.680: INFO: Pod for on the node: telemeter-client-869d867dd8-wrt8b, Cpu: 210, Mem: 440401920 Oct 21 22:55:11.680: INFO: Pod for on the node: multus-jqjk6, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.680: INFO: Pod for on the node: ovs-wlc9p, Cpu: 200, Mem: 419430400 Oct 21 22:55:11.680: INFO: Pod for on the node: sdn-n2nlx, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.680: INFO: Node: ip-10-0-142-54.us-west-1.compute.internal, totalRequestedCpuResource: 1750, cpuAllocatableMil: 3500, cpuFraction: 0.5 Oct 21 22:55:11.680: INFO: Node: ip-10-0-142-54.us-west-1.compute.internal, totalRequestedMemResource: 8090900480, memAllocatableVal: 16181800960, memFraction: 0.5 STEP: Compute Cpu, Mem Fraction after create balanced pods. Oct 21 22:55:11.680: INFO: ComputeCpuMemFraction for node: ip-10-0-143-110.us-west-1.compute.internal Oct 21 22:55:11.863: INFO: Pod for on the node: 6d10a1ae-13f0-11eb-ab84-0a58ac10517a-0, Cpu: 1089, Mem: 6759208960 Oct 21 22:55:11.863: INFO: Pod for on the node: tuned-cnzrp, Cpu: 10, Mem: 20971520 Oct 21 22:55:11.863: INFO: Pod for on the node: dns-default-5mk9j, Cpu: 110, Mem: 283115520 Oct 21 22:55:11.863: INFO: Pod for on the node: node-ca-cxxsk, Cpu: 10, Mem: 10485760 Oct 21 22:55:11.863: INFO: Pod for on the node: machine-config-daemon-z9qzz, Cpu: 20, Mem: 52428800 Oct 21 22:55:11.863: INFO: Pod for on the node: node-exporter-fhpkb, Cpu: 110, Mem: 230686720 Oct 21 22:55:11.863: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Oct 21 22:55:11.863: INFO: Pod for on the node: multus-59h2z, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.863: INFO: Pod for on the node: ovs-58xn4, Cpu: 200, Mem: 419430400 Oct 21 22:55:11.863: INFO: Pod for on the node: sdn-h4mht, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.863: INFO: Node: ip-10-0-143-110.us-west-1.compute.internal, totalRequestedCpuResource: 1749, cpuAllocatableMil: 3500, cpuFraction: 0.4997142857142857 Oct 21 22:55:11.863: INFO: Node: ip-10-0-143-110.us-west-1.compute.internal, totalRequestedMemResource: 8090900480, memAllocatableVal: 16181800960, memFraction: 0.5 STEP: Compute Cpu, Mem Fraction after create balanced pods. Oct 21 22:55:11.863: INFO: ComputeCpuMemFraction for node: ip-10-0-147-199.us-west-1.compute.internal Oct 21 22:55:11.975: INFO: Pod for on the node: 73248907-13f0-11eb-ab84-0a58ac10517a-0, Cpu: 390, Mem: 5232478208 Oct 21 22:55:11.975: INFO: Pod for on the node: tuned-dxcks, Cpu: 10, Mem: 20971520 Oct 21 22:55:11.975: INFO: Pod for on the node: dns-default-hlb8d, Cpu: 110, Mem: 283115520 Oct 21 22:55:11.975: INFO: Pod for on the node: node-ca-xzq8t, Cpu: 10, Mem: 10485760 Oct 21 22:55:11.975: INFO: Pod for on the node: router-default-76df456ddd-9f8l9, Cpu: 100, Mem: 268435456 Oct 21 22:55:11.975: INFO: Pod for on the node: machine-config-daemon-p7wps, Cpu: 20, Mem: 52428800 Oct 21 22:55:11.975: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Oct 21 22:55:11.975: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Oct 21 22:55:11.975: INFO: Pod for on the node: node-exporter-79lp8, Cpu: 110, Mem: 230686720 Oct 21 22:55:11.975: INFO: Pod for on the node: prometheus-adapter-7b86577c4-s57f8, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.975: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Oct 21 22:55:11.975: INFO: Pod for on the node: prometheus-operator-5d4588dd6-tzzgw, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.975: INFO: Pod for on the node: multus-dnd4k, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.975: INFO: Pod for on the node: olm-operators-66p8d, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.975: INFO: Pod for on the node: ovs-9dp2w, Cpu: 200, Mem: 419430400 Oct 21 22:55:11.975: INFO: Pod for on the node: sdn-tbbsf, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.975: INFO: Node: ip-10-0-147-199.us-west-1.compute.internal, totalRequestedCpuResource: 1750, cpuAllocatableMil: 3500, cpuFraction: 0.5 Oct 21 22:55:11.975: INFO: Node: ip-10-0-147-199.us-west-1.compute.internal, totalRequestedMemResource: 8090896384, memAllocatableVal: 16181792768, memFraction: 0.5 STEP: Create a RC, with 0 replicas STEP: Trying to apply avoidPod annotations on the first node. Oct 21 22:55:12.399: INFO: Conflict when trying to add/update avoidPonds {[{{&OwnerReference{Kind:ReplicationController,Name:scheduler-priority-avoid-pod,UID:79b978a3-13f0-11eb-9b44-06dc8e64439d,APIVersion:v1,Controller:*true,BlockOwnerDeletion:nil,}} 0001-01-01 00:00:00 +0000 UTC some reson some message}]} to ip-10-0-142-54.us-west-1.compute.internal STEP: deleting ReplicationController scheduler-priority-avoid-pod in namespace e2e-tests-sched-priority-757tb, will wait for the garbage collector to delete the pods Oct 21 22:57:12.678: INFO: Deleting ReplicationController scheduler-priority-avoid-pod took: 92.052032ms Oct 21 22:57:12.678: INFO: Terminating ReplicationController scheduler-priority-avoid-pod pods took: 93.442\\\\xc2\\\\xb5s [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:154 STEP: Collecting events from namespace \\\"e2e-tests-sched-priority-757tb\\\". STEP: Found 9 events. Oct 21 22:57:12.764: INFO: At 2020-10-21 22:54:49 +0000 UTC - event for 66fb5002-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-142-54.us-west-1.compute.internal} Pulled: Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Oct 21 22:57:12.764: INFO: At 2020-10-21 22:54:49 +0000 UTC - event for 66fb5002-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-142-54.us-west-1.compute.internal} Created: Created container 66fb5002-13f0-11eb-ab84-0a58ac10517a-0 Oct 21 22:57:12.764: INFO: At 2020-10-21 22:54:49 +0000 UTC - event for 66fb5002-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-142-54.us-west-1.compute.internal} Started: Started container 66fb5002-13f0-11eb-ab84-0a58ac10517a-0 Oct 21 22:57:12.764: INFO: At 2020-10-21 22:54:58 +0000 UTC - event for 6d10a1ae-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-143-110.us-west-1.compute.internal} Pulled: Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Oct 21 22:57:12.764: INFO: At 2020-10-21 22:54:58 +0000 UTC - event for 6d10a1ae-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-143-110.us-west-1.compute.internal} Created: Created container 6d10a1ae-13f0-11eb-ab84-0a58ac10517a-0 Oct 21 22:57:12.764: INFO: At 2020-10-21 22:54:58 +0000 UTC - event for 6d10a1ae-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-143-110.us-west-1.compute.internal} Started: Started container 6d10a1ae-13f0-11eb-ab84-0a58ac10517a-0 Oct 21 22:57:12.764: INFO: At 2020-10-21 22:55:08 +0000 UTC - event for 73248907-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-147-199.us-west-1.compute.internal} Pulled: Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Oct 21 22:57:12.764: INFO: At 2020-10-21 22:55:08 +0000 UTC - event for 73248907-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-147-199.us-west-1.compute.internal} Created: Created container 73248907-13f0-11eb-ab84-0a58ac10517a-0 Oct 21 22:57:12.764: INFO: At 2020-10-21 22:55:08 +0000 UTC - event for 73248907-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-147-199.us-west-1.compute.internal} Started: Started container 73248907-13f0-11eb-ab84-0a58ac10517a-0 Oct 21 22:57:12.929: INFO: skipping dumping cluster info - cluster too large Oct 21 22:57:12.929: INFO: Waiting up to 3m0s for all (but 100) nodes to be ready STEP: Destroying namespace \\\"e2e-tests-sched-priority-757tb\\\" for this suite. Oct 21 22:57:37.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Oct 21 22:57:43.272: INFO: namespace e2e-tests-sched-priority-757tb deletion completed in 30.256522442s [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:68 Oct 21 22:57:43.275: INFO: Running AfterSuite actions on all nodes Oct 21 22:57:43.275: INFO: Running AfterSuite actions on node 1 fail [k8s.io/kubernetes/test/e2e/scheduling/priorities.go:191]: Expected error: <*errors.errorString | 0xc0002b0400>: { s: \\\"timed out waiting for the condition\\\", } timed out waiting for the condition not to have occurred Oct 21 22:55:31.482 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (9 times) Oct 21 22:55:32.491 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (5 times) Oct 21 22:55:33.359 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (5 times) Oct 21 22:55:34.374 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (6 times) Oct 21 22:55:34.527 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (8 times) Oct 21 22:55:34.641 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (9 times) failed: (4m13s) 2020-10-21T22:57:43 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/9/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-21T22:58:23 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/10/79) \\\"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:247]: Could not get controller-manager metrics - skipping skipped: (29.5s) 2020-10-21T22:58:53 \\\"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/11/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m38s) 2020-10-21T23:00:31 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/12/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.5s) 2020-10-21T23:00:54 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/13/79) \\\"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m15s) 2020-10-21T23:02:09 \\\"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/14/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:02:50 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/15/79) \\\"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\\\" passed: (58.8s) 2020-10-21T23:03:49 \\\"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\\\" started: (1/16/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:04:29 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/17/79) \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m4s) 2020-10-21T23:05:34 \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/18/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:06:14 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/19/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:06:55 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/20/79) \\\"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:193]: Could not get controller-manager metrics - skipping skipped: (1m2s) 2020-10-21T23:07:57 \\\"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/21/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-21T23:08:37 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/22/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m54s) 2020-10-21T23:11:31 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/23/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m57s) 2020-10-21T23:13:29 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/24/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (19.1s) 2020-10-21T23:13:48 \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/25/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:14:29 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/26/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m34s) 2020-10-21T23:17:03 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/27/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-21T23:17:43 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/28/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:18:24 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/29/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:19:05 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/30/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.4s) 2020-10-21T23:19:27 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/31/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (50.7s) 2020-10-21T23:20:18 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/32/79) \\\"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (3m15s) 2020-10-21T23:23:33 \\\"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/33/79) \\\"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m20s) 2020-10-21T23:24:53 \\\"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/34/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.6s) 2020-10-21T23:25:16 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/35/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-10-21T23:25:56 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/36/79) \\\"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/scheduling/taint_based_evictions.go:63]: Requires at least 2 nodes (not -1) skipped: (19s) 2020-10-21T23:26:15 \\\"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/37/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m21s) 2020-10-21T23:27:36 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/38/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m52s) 2020-10-21T23:29:28 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/39/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m32s) 2020-10-21T23:31:59 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/40/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m24s) 2020-10-21T23:33:23 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/41/79) \\\"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m54s) 2020-10-21T23:35:17 \\\"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/42/79) \\\"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m50s) 2020-10-21T23:37:07 \\\"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/43/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.7s) 2020-10-21T23:37:30 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/44/79) \\\"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:81]: Environment does not support getting controller-manager metrics - skipping skipped: (19.2s) 2020-10-21T23:37:49 \\\"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/45/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.7s) 2020-10-21T23:38:11 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/46/79) \\\"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (19.5s) 2020-10-21T23:38:31 \\\"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/47/79) \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (30.9s) 2020-10-21T23:39:02 \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/48/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-21T23:39:43 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/49/79) \\\"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m11s) 2020-10-21T23:40:53 \\\"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/50/79) \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m3s) 2020-10-21T23:41:56 \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/51/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (19.3s) 2020-10-21T23:42:15 \\\"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/52/79) \\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m18s) 2020-10-21T23:43:33 \\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/53/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (41.7s) 2020-10-21T23:44:15 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/54/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m51s) 2020-10-21T23:46:06 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/55/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-10-21T23:46:46 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/56/79) \\\"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\\\" passed: (40.7s) 2020-10-21T23:47:27 \\\"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\\\" started: (1/57/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-21T23:48:07 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/58/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.6s) 2020-10-21T23:48:30 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/59/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (34.7s) 2020-10-21T23:49:05 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/60/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-21T23:49:45 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/61/79) \\\"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m39s) 2020-10-21T23:51:24 \\\"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/62/79) \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m40s) 2020-10-21T23:54:04 \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/63/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (19.3s) 2020-10-21T23:54:23 \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/64/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (44.7s) 2020-10-21T23:55:08 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/65/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (21s) 2020-10-21T23:55:29 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/66/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (51.3s) 2020-10-21T23:56:20 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/67/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m10s) 2020-10-21T23:58:31 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/68/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (32.6s) 2020-10-21T23:59:03 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/69/79) \\\"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m58s) 2020-10-22T00:01:01 \\\"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/70/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.8s) 2020-10-22T00:01:24 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/71/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (19.2s) 2020-10-22T00:01:43 \\\"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/72/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m44s) 2020-10-22T00:04:27 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/73/79) \\\"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/apps/daemon_set.go:378]: Requires at least 2 nodes (not -1) skipped: (20.2s) 2020-10-22T00:04:48 \\\"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/74/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (33s) 2020-10-22T00:05:21 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/75/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (21.2s) 2020-10-22T00:05:42 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/76/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.6s) 2020-10-22T00:06:04 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/77/79) \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m59s) 2020-10-22T00:08:03 \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/78/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-22T00:08:44 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/79/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-10-22T00:09:24 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" Timeline: Oct 21 22:45:52.682 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-129-97.us-west-1.compute.internal node/ip-10-0-129-97.us-west-1.compute.internal created Oct 21 22:45:52.684 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Created Pod/revision-pruner-6-ip-10-0-129-97.us-west-1.compute.internal -n openshift-kube-apiserver because it was missing Oct 21 22:46:01.067 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-129-97.us-west-1.compute.internal Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c18ce2736a4fed8091de16f868bc46e6a9a03b7956c2418bad42fbb041ffbafb\\\" already present on machine Oct 21 22:46:01.255 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-129-97.us-west-1.compute.internal Created container pruner Oct 21 22:46:01.294 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-129-97.us-west-1.compute.internal Started container pruner Oct 21 22:51:13.982 W ns/openshift-machine-config-operator pod/machine-config-daemon-nkwv7 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 22:51:14.037 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-kmjst Oct 21 22:51:14.037 I ns/openshift-machine-config-operator pod/machine-config-daemon-nkwv7 Stopping container machine-config-daemon Oct 21 22:51:14.038 I ns/openshift-image-registry pod/node-ca-kmjst Marking for deletion Pod openshift-image-registry/node-ca-kmjst Oct 21 22:51:14.038 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-nkwv7 Oct 21 22:51:14.038 W ns/openshift-image-registry pod/node-ca-kmjst node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 22:51:14.053 I ns/openshift-monitoring pod/alertmanager-main-1 Marking for deletion Pod openshift-monitoring/alertmanager-main-1 Oct 21 22:51:14.053 I ns/openshift-image-registry pod/node-ca-kmjst Stopping container node-ca Oct 21 22:51:14.053 I ns/openshift-monitoring pod/grafana-649f787944-bd9j2 Marking for deletion Pod openshift-monitoring/grafana-649f787944-bd9j2 Oct 21 22:51:14.054 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 22:51:14.109 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus Oct 21 22:51:14.109 W ns/openshift-monitoring pod/grafana-649f787944-bd9j2 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 22:51:14.110 W ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 22:51:14.110 W ns/openshift-monitoring pod/alertmanager-main-1 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 0s Oct 21 22:51:14.111 W ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-bsgkm node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 22:51:14.126 I ns/openshift-monitoring pod/prometheus-k8s-1 Marking for deletion Pod openshift-monitoring/prometheus-k8s-1 Oct 21 22:51:14.126 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container rules-configmap-reloader Oct 21 22:51:14.126 I ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 Marking for deletion Pod openshift-ingress/router-default-76df456ddd-9gdw7 Oct 21 22:51:14.126 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prom-label-proxy Oct 21 22:51:14.126 I ns/openshift-monitoring pod/grafana-649f787944-bd9j2 Stopping container grafana Oct 21 22:51:14.126 I ns/openshift-machine-config-operator pod/machine-config-daemon-nkwv7 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-nkwv7 Oct 21 22:51:14.126 W ns/openshift-monitoring pod/alertmanager-main-1 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:14.126 I ns/openshift-ingress pod/router-default-76df456ddd-9f8l9 node/ created Oct 21 22:51:14.126 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-mwcrw node/ created Oct 21 22:51:14.127 I ns/openshift-monitoring pod/grafana-649f787944-bd9j2 Stopping container grafana-proxy Oct 21 22:51:14.127 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-bsgkm Marking for deletion Pod openshift-monitoring/prometheus-adapter-7b86577c4-bsgkm Oct 21 22:51:14.127 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager Oct 21 22:51:14.127 I ns/openshift-monitoring pod/alertmanager-main-1 Cancelling deletion of Pod openshift-monitoring/alertmanager-main-1 Oct 21 22:51:14.127 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-bsgkm Stopping container prometheus-adapter Oct 21 22:51:14.180 I ns/openshift-ingress replicaset/router-default-76df456ddd Created pod: router-default-76df456ddd-9f8l9 Oct 21 22:51:14.180 I ns/openshift-ingress pod/router-default-76df456ddd-9f8l9 Successfully assigned openshift-ingress/router-default-76df456ddd-9f8l9 to ip-10-0-147-199.us-west-1.compute.internal Oct 21 22:51:14.201 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager-proxy Oct 21 22:51:14.202 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-mwcrw Successfully assigned openshift-monitoring/prometheus-adapter-7b86577c4-mwcrw to ip-10-0-142-54.us-west-1.compute.internal Oct 21 22:51:14.202 I ns/openshift-monitoring replicaset/prometheus-adapter-7b86577c4 Created pod: prometheus-adapter-7b86577c4-mwcrw Oct 21 22:51:14.219 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container config-reloader Oct 21 22:51:14.223 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc node/ created Oct 21 22:51:14.251 I ns/openshift-monitoring replicaset/grafana-649f787944 Created pod: grafana-649f787944-ln2tc Oct 21 22:51:14.265 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Successfully assigned openshift-monitoring/grafana-649f787944-ln2tc to ip-10-0-142-54.us-west-1.compute.internal Oct 21 22:51:14.313 I ns/openshift-monitoring pod/alertmanager-main-1 node/ created Oct 21 22:51:14.318 I ns/openshift-monitoring statefulset/alertmanager-main create Pod alertmanager-main-1 in StatefulSet alertmanager-main successful Oct 21 22:51:14.390 I ns/openshift-monitoring pod/alertmanager-main-1 Successfully assigned openshift-monitoring/alertmanager-main-1 to ip-10-0-147-199.us-west-1.compute.internal Oct 21 22:51:14.404 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager-proxy (2 times) Oct 21 22:51:14.597 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container config-reloader (2 times) Oct 21 22:51:14.798 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager (2 times) Oct 21 22:51:14.993 I ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 Stopping container router Oct 21 22:51:15.195 W ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 Liveness probe failed: Get http://10.129.2.3:1936/healthz: dial tcp 10.129.2.3:1936: connect: connection refused Oct 21 22:51:16.032 W ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-bsgkm node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 22:51:16.032 W ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-bsgkm node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-adapter container stopped being ready Oct 21 22:51:16.048 W ns/openshift-image-registry pod/node-ca-kmjst node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 22:51:16.048 W ns/openshift-image-registry pod/node-ca-kmjst node/ip-10-0-143-110.us-west-1.compute.internal container=node-ca container stopped being ready Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=rules-configmap-reloader container stopped being ready Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-config-reloader container stopped being ready Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-proxy container stopped being ready Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=kube-rbac-proxy container stopped being ready Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prom-label-proxy container stopped being ready Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container stopped being ready Oct 21 22:51:16.195 W ns/openshift-machine-config-operator pod/machine-config-daemon-nkwv7 node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 22:51:16.195 W ns/openshift-machine-config-operator pod/machine-config-daemon-nkwv7 node/ip-10-0-143-110.us-west-1.compute.internal container=machine-config-daemon container stopped being ready Oct 21 22:51:16.596 W ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 22:51:16.596 W ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 node/ip-10-0-143-110.us-west-1.compute.internal container=router container stopped being ready Oct 21 22:51:16.996 E ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 node/ip-10-0-143-110.us-west-1.compute.internal container=router container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 22:51:17.396 W ns/openshift-monitoring pod/grafana-649f787944-bd9j2 node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 22:51:17.396 W ns/openshift-monitoring pod/grafana-649f787944-bd9j2 node/ip-10-0-143-110.us-west-1.compute.internal container=grafana container stopped being ready Oct 21 22:51:17.396 W ns/openshift-monitoring pod/grafana-649f787944-bd9j2 node/ip-10-0-143-110.us-west-1.compute.internal container=grafana-proxy container stopped being ready Oct 21 22:51:17.606 W ns/openshift-monitoring pod/grafana-649f787944-bd9j2 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:18.407 W ns/openshift-machine-config-operator pod/machine-config-daemon-nkwv7 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:18.418 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz node/ created Oct 21 22:51:18.422 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-z9qzz Oct 21 22:51:18.469 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz Successfully assigned openshift-machine-config-operator/machine-config-daemon-z9qzz to ip-10-0-143-110.us-west-1.compute.internal Oct 21 22:51:19.205 W ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-bsgkm node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:19.687 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 21 22:51:19.816 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz Created container machine-config-daemon Oct 21 22:51:19.848 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz Started container machine-config-daemon Oct 21 22:51:20.007 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:20.099 I ns/openshift-monitoring pod/prometheus-k8s-1 node/ created Oct 21 22:51:20.104 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-1 in StatefulSet prometheus-k8s successful Oct 21 22:51:20.122 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully assigned openshift-monitoring/prometheus-k8s-1 to ip-10-0-143-110.us-west-1.compute.internal Oct 21 22:51:21.145 W ns/openshift-image-registry pod/node-ca-kmjst node/ip-10-0-143-110.us-west-1.compute.internal pod has been pending longer than a minute Oct 21 22:51:21.145 W ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 node/ip-10-0-143-110.us-west-1.compute.internal pod has been pending longer than a minute Oct 21 22:51:22.334 I ns/openshift-ingress pod/router-default-76df456ddd-9f8l9 Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" Oct 21 22:51:22.606 W ns/openshift-image-registry pod/node-ca-kmjst node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:22.646 I ns/openshift-image-registry pod/node-ca-cxxsk node/ created Oct 21 22:51:22.653 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-cxxsk Oct 21 22:51:22.659 I ns/openshift-image-registry pod/node-ca-cxxsk Successfully assigned openshift-image-registry/node-ca-cxxsk to ip-10-0-143-110.us-west-1.compute.internal Oct 21 22:51:23.637 I ns/openshift-monitoring pod/alertmanager-main-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b9d07e8eed67a34b50a3afaa36cd4b9f052697894e64b5c203d0675d12ab5ec\\\" already present on machine Oct 21 22:51:23.797 I ns/openshift-monitoring pod/alertmanager-main-1 Created container alertmanager Oct 21 22:51:23.822 I ns/openshift-monitoring pod/alertmanager-main-1 Started container alertmanager Oct 21 22:51:23.829 I ns/openshift-monitoring pod/alertmanager-main-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Oct 21 22:51:24.001 I ns/openshift-monitoring pod/alertmanager-main-1 Created container config-reloader Oct 21 22:51:24.023 I ns/openshift-monitoring pod/alertmanager-main-1 Started container config-reloader Oct 21 22:51:24.030 I ns/openshift-monitoring pod/alertmanager-main-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Oct 21 22:51:24.177 I ns/openshift-monitoring pod/alertmanager-main-1 Created container alertmanager-proxy Oct 21 22:51:24.202 I ns/openshift-monitoring pod/alertmanager-main-1 Started container alertmanager-proxy Oct 21 22:51:24.648 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-mwcrw Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\\\" Oct 21 22:51:24.809 W ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:25.454 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c229ac6674c217359c243e4621aae565c2af46caaf231a077b7f7e3d6f07ef5a\\\" Oct 21 22:51:26.938 I ns/openshift-ingress pod/router-default-76df456ddd-9f8l9 Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" Oct 21 22:51:27.092 I ns/openshift-ingress pod/router-default-76df456ddd-9f8l9 Created container router Oct 21 22:51:27.114 I ns/openshift-ingress pod/router-default-76df456ddd-9f8l9 Started container router Oct 21 22:51:29.407 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-mwcrw Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\\\" Oct 21 22:51:29.608 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-mwcrw Created container prometheus-adapter Oct 21 22:51:29.642 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-mwcrw Started container prometheus-adapter Oct 21 22:51:29.927 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine Oct 21 22:51:30.097 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus Oct 21 22:51:30.125 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus Oct 21 22:51:30.132 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\\\" already present on machine Oct 21 22:51:30.284 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-config-reloader Oct 21 22:51:30.307 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-config-reloader Oct 21 22:51:30.315 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Oct 21 22:51:30.461 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-proxy Oct 21 22:51:30.488 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-proxy Oct 21 22:51:30.495 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Oct 21 22:51:30.661 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container kube-rbac-proxy Oct 21 22:51:30.686 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container kube-rbac-proxy Oct 21 22:51:30.693 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\\\" already present on machine Oct 21 22:51:30.840 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prom-label-proxy Oct 21 22:51:30.926 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prom-label-proxy Oct 21 22:51:31.107 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container exited with code 1 (Error): Oct 21 22:51:31.127 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Oct 21 22:51:31.326 I ns/openshift-image-registry pod/node-ca-cxxsk Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 21 22:51:31.527 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container rules-configmap-reloader Oct 21 22:51:31.726 I ns/openshift-image-registry pod/node-ca-cxxsk Created container node-ca Oct 21 22:51:31.927 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container rules-configmap-reloader Oct 21 22:51:32.126 I ns/openshift-image-registry pod/node-ca-cxxsk Started container node-ca Oct 21 22:51:32.330 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine (2 times) Oct 21 22:51:32.530 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus (2 times) Oct 21 22:51:32.730 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus (2 times) Oct 21 22:51:33.211 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c229ac6674c217359c243e4621aae565c2af46caaf231a077b7f7e3d6f07ef5a\\\" Oct 21 22:51:33.389 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Created container grafana Oct 21 22:51:33.413 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Started container grafana Oct 21 22:51:33.419 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Oct 21 22:51:33.563 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Created container grafana-proxy Oct 21 22:51:33.588 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Started container grafana-proxy Oct 21 22:51:33.946 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container restarted Oct 21 22:53:30.690 - 252s I test=\\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" running Oct 21 22:55:31.482 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (9 times) Oct 21 22:55:32.491 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (5 times) Oct 21 22:55:33.359 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (5 times) Oct 21 22:55:34.374 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (6 times) Oct 21 22:55:34.527 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (8 times) Oct 21 22:55:34.641 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (9 times) Oct 21 22:57:43.292 I test=\\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" failed Oct 21 23:05:31.509 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (10 times) Oct 21 23:05:32.527 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (6 times) Oct 21 23:05:33.484 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (6 times) Oct 21 23:05:34.311 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (7 times) Oct 21 23:05:34.457 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (9 times) Oct 21 23:05:34.580 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (10 times) Oct 21 23:07:41.413 W persistentvolume/pvc-2166e4e6-13f2-11eb-9b44-06dc8e64439d Error deleting EBS volume \\\"vol-069a135c4da9eb540\\\" since volume is currently attached to \\\"i-0681d725f9f4ff5a1\\\" Oct 21 23:09:46.745 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz Stopping container machine-config-daemon Oct 21 23:09:46.746 W ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 23:09:46.746 W ns/openshift-image-registry pod/node-ca-cxxsk node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 23:09:46.750 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-z9qzz Oct 21 23:09:46.750 I ns/openshift-monitoring pod/prometheus-k8s-1 Marking for deletion Pod openshift-monitoring/prometheus-k8s-1 (2 times) Oct 21 23:09:46.755 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 23:09:46.817 I ns/openshift-image-registry pod/node-ca-cxxsk Stopping container node-ca Oct 21 23:09:46.817 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-z9qzz Oct 21 23:09:46.817 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-cxxsk Oct 21 23:09:46.817 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus Oct 21 23:09:46.817 I ns/openshift-image-registry pod/node-ca-cxxsk Marking for deletion Pod openshift-image-registry/node-ca-cxxsk Oct 21 23:09:46.817 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus-config-reloader Oct 21 23:09:46.817 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus-proxy Oct 21 23:09:46.818 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prom-label-proxy Oct 21 23:09:46.818 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container rules-configmap-reloader Oct 21 23:09:46.824 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container kube-rbac-proxy Oct 21 23:09:47.797 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-config-reloader container exited with code 2 (Error): Oct 21 23:09:47.797 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-proxy container exited with code 2 (Error): Oct 21 23:09:47.797 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=rules-configmap-reloader container exited with code 2 (Error): Oct 21 23:09:49.568 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:09:49.638 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-1 in StatefulSet prometheus-k8s successful (2 times) Oct 21 23:09:49.638 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully assigned openshift-monitoring/prometheus-k8s-1 to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:09:49.639 I ns/openshift-monitoring pod/prometheus-k8s-1 node/ created Oct 21 23:09:52.108 W ns/openshift-image-registry pod/node-ca-cxxsk node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:09:52.168 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-wbk6b Oct 21 23:09:52.168 I ns/openshift-image-registry pod/node-ca-wbk6b Successfully assigned openshift-image-registry/node-ca-wbk6b to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:09:52.169 I ns/openshift-image-registry pod/node-ca-wbk6b node/ created Oct 21 23:09:59.932 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine Oct 21 23:10:00.024 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus Oct 21 23:10:00.050 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus Oct 21 23:10:00.058 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\\\" already present on machine Oct 21 23:10:00.205 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-config-reloader Oct 21 23:10:00.227 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-config-reloader Oct 21 23:10:00.235 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Oct 21 23:10:00.376 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-proxy Oct 21 23:10:00.406 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-proxy Oct 21 23:10:00.414 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Oct 21 23:10:00.591 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container kube-rbac-proxy Oct 21 23:10:00.614 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container kube-rbac-proxy Oct 21 23:10:00.622 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\\\" already present on machine Oct 21 23:10:00.755 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prom-label-proxy Oct 21 23:10:00.876 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prom-label-proxy Oct 21 23:10:01.076 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Oct 21 23:10:01.275 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container rules-configmap-reloader Oct 21 23:10:01.477 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container rules-configmap-reloader Oct 21 23:10:01.675 I ns/openshift-image-registry pod/node-ca-wbk6b Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 21 23:10:01.831 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container exited with code 1 (Error): Oct 21 23:10:01.875 I ns/openshift-image-registry pod/node-ca-wbk6b Created container node-ca Oct 21 23:10:02.093 I ns/openshift-image-registry pod/node-ca-wbk6b Started container node-ca Oct 21 23:10:02.278 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine (2 times) Oct 21 23:10:02.478 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus (2 times) Oct 21 23:10:02.678 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus (2 times) Oct 21 23:10:04.494 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container restarted Oct 21 23:10:41.293 W ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:10:41.293 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 node/ created Oct 21 23:10:41.293 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-f9w59 Oct 21 23:10:41.295 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 Successfully assigned openshift-machine-config-operator/machine-config-daemon-f9w59 to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:10:41.922 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 21 23:10:42.038 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 Created container machine-config-daemon Oct 21 23:10:42.063 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 Started container machine-config-daemon Oct 21 23:15:32.427 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (7 times) Oct 21 23:15:33.353 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (8 times) Oct 21 23:15:33.508 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (10 times) Oct 21 23:15:33.633 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (11 times) Oct 21 23:15:33.772 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (11 times) Oct 21 23:15:34.721 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (7 times) Oct 21 23:15:37.573 W ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 23:15:37.575 W ns/openshift-image-registry pod/node-ca-wbk6b node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 23:15:37.588 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 Stopping container machine-config-daemon Oct 21 23:15:37.596 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-wbk6b Oct 21 23:15:37.596 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 23:15:37.598 I ns/openshift-image-registry pod/node-ca-wbk6b Stopping container node-ca Oct 21 23:15:37.626 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-f9w59 Oct 21 23:15:37.626 I ns/openshift-monitoring pod/prometheus-k8s-1 Marking for deletion Pod openshift-monitoring/prometheus-k8s-1 (3 times) Oct 21 23:15:37.626 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus Oct 21 23:15:37.626 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-f9w59 Oct 21 23:15:37.634 I ns/openshift-image-registry pod/node-ca-wbk6b Marking for deletion Pod openshift-image-registry/node-ca-wbk6b Oct 21 23:15:37.634 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container rules-configmap-reloader Oct 21 23:15:37.642 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prom-label-proxy Oct 21 23:15:37.651 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container kube-rbac-proxy Oct 21 23:15:37.658 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus-proxy Oct 21 23:15:39.561 W ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prom-label-proxy container stopped being ready Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container stopped being ready Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-config-reloader container stopped being ready Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-proxy container stopped being ready Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=rules-configmap-reloader container stopped being ready Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=kube-rbac-proxy container stopped being ready Oct 21 23:15:40.827 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=kube-rbac-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 23:15:40.827 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prom-label-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 23:15:40.827 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 23:15:40.827 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 23:15:40.827 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=rules-configmap-reloader container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 23:15:40.827 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-config-reloader container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 23:15:51.145 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal pod has been pending longer than a minute Oct 21 23:15:51.331 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:15:51.364 W ns/openshift-image-registry pod/node-ca-wbk6b node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:15:51.378 I ns/openshift-monitoring pod/prometheus-k8s-1 node/ created Oct 21 23:15:51.386 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-1 in StatefulSet prometheus-k8s successful (3 times) Oct 21 23:15:51.401 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully assigned openshift-monitoring/prometheus-k8s-1 to ip-10-0-142-54.us-west-1.compute.internal Oct 21 23:15:59.368 I ns/openshift-monitoring pod/prometheus-k8s-1 Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" Oct 21 23:16:04.787 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" Oct 21 23:16:04.961 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus Oct 21 23:16:04.988 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus Oct 21 23:16:04.993 I ns/openshift-monitoring pod/prometheus-k8s-1 Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\\\" Oct 21 23:16:08.618 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\\\" Oct 21 23:16:08.790 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-config-reloader Oct 21 23:16:08.813 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-config-reloader Oct 21 23:16:08.818 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Oct 21 23:16:08.990 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-proxy Oct 21 23:16:09.012 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-proxy Oct 21 23:16:09.017 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Oct 21 23:16:09.163 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container kube-rbac-proxy Oct 21 23:16:09.185 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container kube-rbac-proxy Oct 21 23:16:09.191 I ns/openshift-monitoring pod/prometheus-k8s-1 Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\\\" Oct 21 23:16:12.833 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\\\" Oct 21 23:16:12.988 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prom-label-proxy Oct 21 23:16:13.011 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prom-label-proxy Oct 21 23:16:13.016 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Oct 21 23:16:13.157 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container rules-configmap-reloader Oct 21 23:16:13.179 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container rules-configmap-reloader Oct 21 23:16:13.989 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine Oct 21 23:16:14.067 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-142-54.us-west-1.compute.internal container=prometheus container exited with code 1 (Error): Oct 21 23:16:14.171 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus (2 times) Oct 21 23:16:14.281 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus (2 times) Oct 21 23:16:15.257 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-142-54.us-west-1.compute.internal container=prometheus container restarted Oct 21 23:16:51.441 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm node/ created Oct 21 23:16:51.447 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-crnnm Oct 21 23:16:51.456 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm Successfully assigned openshift-machine-config-operator/machine-config-daemon-crnnm to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:16:51.467 I ns/openshift-image-registry pod/node-ca-qg5rv node/ created Oct 21 23:16:51.473 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-qg5rv Oct 21 23:16:51.487 I ns/openshift-image-registry pod/node-ca-qg5rv Successfully assigned openshift-image-registry/node-ca-qg5rv to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:16:52.129 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 21 23:16:52.252 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm Created container machine-config-daemon Oct 21 23:16:52.278 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm Started container machine-config-daemon Oct 21 23:16:59.478 I ns/openshift-image-registry pod/node-ca-qg5rv Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 21 23:16:59.611 I ns/openshift-image-registry pod/node-ca-qg5rv Created container node-ca Oct 21 23:16:59.636 I ns/openshift-image-registry pod/node-ca-qg5rv Started container node-ca Oct 21 23:25:32.363 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (8 times) Oct 21 23:25:33.237 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (8 times) Oct 21 23:25:34.103 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (9 times) Oct 21 23:25:34.257 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (11 times) Oct 21 23:25:34.392 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (12 times) Oct 21 23:25:34.520 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (12 times) Oct 21 23:33:08.448 W ns/openshift-image-registry pod/node-ca-qg5rv node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 23:33:08.454 W ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 23:33:08.460 I ns/openshift-image-registry pod/node-ca-qg5rv Stopping container node-ca Oct 21 23:33:08.462 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-qg5rv Oct 21 23:33:08.467 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-crnnm Oct 21 23:33:08.468 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm Stopping container machine-config-daemon Oct 21 23:33:08.504 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-crnnm Oct 21 23:33:08.504 I ns/openshift-image-registry pod/node-ca-qg5rv Marking for deletion Pod openshift-image-registry/node-ca-qg5rv Oct 21 23:33:21.293 W ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:33:21.293 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd node/ created Oct 21 23:33:21.294 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-4tfsd Oct 21 23:33:21.294 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd Successfully assigned openshift-machine-config-operator/machine-config-daemon-4tfsd to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:33:21.329 W ns/openshift-image-registry pod/node-ca-qg5rv node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:33:21.365 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4brks Oct 21 23:33:21.365 I ns/openshift-image-registry pod/node-ca-4brks Successfully assigned openshift-image-registry/node-ca-4brks to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:33:21.365 I ns/openshift-image-registry pod/node-ca-4brks node/ created Oct 21 23:33:22.757 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 21 23:33:22.886 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd Created container machine-config-daemon Oct 21 23:33:22.915 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd Started container machine-config-daemon Oct 21 23:33:30.322 I ns/openshift-image-registry pod/node-ca-4brks Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 21 23:33:30.451 I ns/openshift-image-registry pod/node-ca-4brks Created container node-ca Oct 21 23:33:30.479 I ns/openshift-image-registry pod/node-ca-4brks Started container node-ca Oct 21 23:35:31.506 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (13 times) Oct 21 23:35:31.644 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (13 times) Oct 21 23:35:32.647 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (9 times) Oct 21 23:35:33.468 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (9 times) Oct 21 23:35:34.344 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (10 times) Oct 21 23:35:34.486 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (12 times) Oct 21 23:38:19.359 I ns/kube-system pod/pod0-system-node-critical node/ created Oct 21 23:38:19.376 I ns/kube-system pod/pod0-system-node-critical Successfully assigned kube-system/pod0-system-node-critical to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:38:19.435 I ns/kube-system pod/pod1-system-cluster-critical node/ created Oct 21 23:38:19.448 I ns/kube-system pod/pod1-system-cluster-critical Successfully assigned kube-system/pod1-system-cluster-critical to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:38:19.515 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 0s Oct 21 23:38:19.522 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:38:19.600 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 0s Oct 21 23:38:19.605 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:38:29.442 W ns/kube-system pod/pod0-system-node-critical Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_pod0-system-node-critical_kube-system_7fdfd1e7-13f6-11eb-be53-026094549e09_0(d55c5fa2b9920e466c16dfc42455b2d6954e838274f66236d03b7a461a5fa191): Multus: Err adding pod to network \\\"openshift-sdn\\\": cannot set \\\"openshift-sdn\\\" ifname to \\\"eth0\\\": no netns: failed to Statfs \\\"/proc/76949/ns/net\\\": no such file or directory Oct 21 23:40:22.469 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \\\"pod1-system-cluster-critical_kube-system(7feb8674-13f6-11eb-be53-026094549e09)\\\": timeout expired waiting for volumes to attach or mount for pod \\\"kube-system\\\"/\\\"pod1-system-cluster-critical\\\". list of unmounted volumes=[default-token-z8dnz]. list of unattached volumes=[default-token-z8dnz] Oct 21 23:45:31.493 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (14 times) Oct 21 23:45:32.666 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (10 times) Oct 21 23:45:33.542 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (10 times) Oct 21 23:45:34.428 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (11 times) Oct 21 23:45:34.565 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (13 times) Oct 21 23:45:34.690 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (14 times) Oct 21 23:52:33.361 W ns/openshift-image-registry pod/node-ca-4brks node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 23:52:33.417 I ns/openshift-image-registry pod/node-ca-4brks Stopping container node-ca Oct 21 23:52:33.417 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-4tfsd Oct 21 23:52:33.417 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-4tfsd Oct 21 23:52:33.417 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd Stopping container machine-config-daemon Oct 21 23:52:33.417 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4brks Oct 21 23:52:33.417 I ns/openshift-image-registry pod/node-ca-4brks Marking for deletion Pod openshift-image-registry/node-ca-4brks Oct 21 23:52:33.417 W ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 23:52:41.267 W ns/openshift-image-registry pod/node-ca-4brks node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:52:41.341 W ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:53:38.596 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 node/ created Oct 21 23:53:38.654 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-5vxz9 Oct 21 23:53:38.655 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 Successfully assigned openshift-machine-config-operator/machine-config-daemon-5vxz9 to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:53:38.655 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-srv2w Oct 21 23:53:38.655 I ns/openshift-image-registry pod/node-ca-srv2w Successfully assigned openshift-image-registry/node-ca-srv2w to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:53:38.655 I ns/openshift-image-registry pod/node-ca-srv2w node/ created Oct 21 23:53:39.538 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 21 23:53:39.699 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 Created container machine-config-daemon Oct 21 23:53:39.745 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 Started container machine-config-daemon Oct 21 23:53:46.745 I ns/openshift-image-registry pod/node-ca-srv2w Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 21 23:53:46.874 I ns/openshift-image-registry pod/node-ca-srv2w Created container node-ca Oct 21 23:53:46.898 I ns/openshift-image-registry pod/node-ca-srv2w Started container node-ca Oct 21 23:55:32.455 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (11 times) Oct 21 23:55:33.436 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (11 times) Oct 21 23:55:34.378 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (12 times) Oct 21 23:55:34.524 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (14 times) Oct 21 23:55:34.703 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (15 times) Oct 21 23:55:34.817 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (15 times) Oct 21 23:57:40.624 I ns/kube-system pod/critical-pod node/ created Oct 21 23:57:40.630 W ns/kube-system pod/critical-pod 0/6 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 3 node(s) had taints that the pod didn\\\\'t tolerate. Oct 21 23:57:40.696 W ns/kube-system pod/critical-pod 0/6 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 3 node(s) had taints that the pod didn\\\\'t tolerate. (2 times) Oct 21 23:57:51.293 I ns/kube-system pod/critical-pod Successfully assigned kube-system/critical-pod to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:57:59.097 I ns/kube-system pod/critical-pod Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Oct 21 23:57:59.236 I ns/kube-system pod/critical-pod Created container critical-pod Oct 21 23:57:59.293 I ns/kube-system pod/critical-pod Started container critical-pod Oct 21 23:58:01.177 W ns/kube-system pod/critical-pod node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 0s Oct 21 23:58:01.182 W ns/kube-system pod/critical-pod node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 22 00:02:52.678 W ns/openshift-image-registry pod/node-ca-srv2w node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 22 00:02:52.680 W ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 22 00:02:52.692 I ns/openshift-image-registry pod/node-ca-srv2w Stopping container node-ca Oct 22 00:02:52.692 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-srv2w Oct 22 00:02:52.705 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 Stopping container machine-config-daemon Oct 22 00:02:52.705 I ns/openshift-image-registry pod/node-ca-srv2w Marking for deletion Pod openshift-image-registry/node-ca-srv2w Oct 22 00:02:52.705 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-5vxz9 Oct 22 00:02:52.735 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-5vxz9 Oct 22 00:02:54.231 W ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 22 00:02:54.231 W ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 node/ip-10-0-143-110.us-west-1.compute.internal container=machine-config-daemon container stopped being ready Oct 22 00:02:54.294 W ns/openshift-image-registry pod/node-ca-srv2w node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 22 00:02:54.294 W ns/openshift-image-registry pod/node-ca-srv2w node/ip-10-0-143-110.us-west-1.compute.internal container=node-ca container stopped being ready Oct 22 00:02:55.293 W ns/openshift-image-registry pod/node-ca-srv2w node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 22 00:03:01.294 W ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 22 00:03:57.916 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b node/ created Oct 22 00:03:57.926 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-2th9b Oct 22 00:03:57.937 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b Successfully assigned openshift-machine-config-operator/machine-config-daemon-2th9b to ip-10-0-143-110.us-west-1.compute.internal Oct 22 00:03:57.970 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-56gsn Oct 22 00:03:57.970 I ns/openshift-image-registry pod/node-ca-56gsn Successfully assigned openshift-image-registry/node-ca-56gsn to ip-10-0-143-110.us-west-1.compute.internal Oct 22 00:03:57.971 I ns/openshift-image-registry pod/node-ca-56gsn node/ created Oct 22 00:03:58.593 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 22 00:03:58.719 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b Created container machine-config-daemon Oct 22 00:03:58.745 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b Started container machine-config-daemon Oct 22 00:04:05.572 I ns/openshift-image-registry pod/node-ca-56gsn Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 22 00:04:05.696 I ns/openshift-image-registry pod/node-ca-56gsn Created container node-ca Oct 22 00:04:05.723 I ns/openshift-image-registry pod/node-ca-56gsn Started container node-ca Oct 22 00:05:32.333 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (12 times) Oct 22 00:05:33.197 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (12 times) Oct 22 00:05:34.049 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (13 times) Oct 22 00:05:34.207 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (15 times) Oct 22 00:05:34.348 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (16 times) Oct 22 00:05:34.471 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (16 times) Oct 22 00:07:13.781 W ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 22 00:07:13.840 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b Stopping container machine-config-daemon Oct 22 00:07:13.840 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-2th9b Oct 22 00:07:13.840 I ns/openshift-image-registry pod/node-ca-56gsn Stopping container node-ca Oct 22 00:07:13.841 I ns/openshift-image-registry pod/node-ca-56gsn Marking for deletion Pod openshift-image-registry/node-ca-56gsn Oct 22 00:07:13.841 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-56gsn Oct 22 00:07:13.841 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-2th9b Oct 22 00:07:13.841 W ns/openshift-image-registry pod/node-ca-56gsn node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 22 00:07:15.648 W ns/openshift-image-registry pod/node-ca-56gsn node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 22 00:07:15.709 W ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 22 00:07:51.434 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf node/ created Oct 22 00:07:51.439 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-qwgwf Oct 22 00:07:51.453 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Successfully assigned openshift-machine-config-operator/machine-config-daemon-qwgwf to ip-10-0-143-110.us-west-1.compute.internal Oct 22 00:07:51.487 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-7qmdn Oct 22 00:07:51.487 I ns/openshift-image-registry pod/node-ca-7qmdn node/ created Oct 22 00:07:51.492 I ns/openshift-image-registry pod/node-ca-7qmdn Successfully assigned openshift-image-registry/node-ca-7qmdn to ip-10-0-143-110.us-west-1.compute.internal Oct 22 00:07:52.365 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 22 00:07:52.506 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Created container machine-config-daemon Oct 22 00:07:52.533 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Started container machine-config-daemon Oct 22 00:07:59.615 I ns/openshift-image-registry pod/node-ca-7qmdn Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 22 00:07:59.741 I ns/openshift-image-registry pod/node-ca-7qmdn Created container node-ca Oct 22 00:07:59.767 I ns/openshift-image-registry pod/node-ca-7qmdn Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201022-000924.xml error: 1 fail, 39 pass, 39 skip (1h23m33s) 2020/10/22 00:09:24 Container test in pod e2e-aws-serial failed, exit code 1, reason Error 2020/10/22 00:15:50 Copied 125.68MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/22 00:15:50 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/22 00:15:50 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/22 00:15:51 Ran for 2h0m24s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-vv1cddgl/e2e-aws-serial failed after 1h59m16s (failed containers: test): ContainerFailed one or more containers exited Container test exited with code 1, reason Error --- nternal deleted Oct 22 00:07:51.434 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf node/ created Oct 22 00:07:51.439 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-qwgwf Oct 22 00:07:51.453 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Successfully assigned openshift-machine-config-operator/machine-config-daemon-qwgwf to ip-10-0-143-110.us-west-1.compute.internal Oct 22 00:07:51.487 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-7qmdn Oct 22 00:07:51.487 I ns/openshift-image-registry pod/node-ca-7qmdn node/ created Oct 22 00:07:51.492 I ns/openshift-image-registry pod/node-ca-7qmdn Successfully assigned openshift-image-registry/node-ca-7qmdn to ip-10-0-143-110.us-west-1.compute.internal Oct 22 00:07:52.365 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Oct 22 00:07:52.506 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Created container machine-config-daemon Oct 22 00:07:52.533 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Started container machine-config-daemon Oct 22 00:07:59.615 I ns/openshift-image-registry pod/node-ca-7qmdn Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Oct 22 00:07:59.741 I ns/openshift-image-registry pod/node-ca-7qmdn Created container node-ca Oct 22 00:07:59.767 I ns/openshift-image-registry pod/node-ca-7qmdn Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201022-000924.xml error: 1 fail, 39 pass, 39 skip (1h23m33s) --- '\", \"cluster_count\": 15}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 16, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/22 22:16:14 ci-operator version v20201022-5a97895 2020/10/22 22:16:14 No source defined 2020/10/22 22:16:14 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/22 22:16:14 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-h6z8vdkq 2020/10/22 22:16:14 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/22 22:16:14 Creating namespace ci-op-h6z8vdkq 2020/10/22 22:16:14 Setting up pipeline imagestream for the test 2020/10/22 22:16:14 Created secret e2e-aws-serial-cluster-profile 2020/10/22 22:16:14 Created secret pull-secret 2020/10/22 22:16:14 Created PDB for pods with openshift.io/build.name label 2020/10/22 22:16:14 Created PDB for pods with created-by-ci label 2020/10/22 22:16:14 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-h6z8vdkq/stable:${component} 2020/10/22 22:16:16 Importing release image latest 2020/10/22 22:16:17 Executing pod \\\"release-images-latest-cli\\\" 2020/10/22 22:16:29 Executing pod \\\"release-images-latest\\\" 2020/10/22 22:17:22 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/22 22:17:22 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/22 22:17:22 Acquired lease \\\"ea3120df-7633-4f58-9a56-701fcac10196\\\" for \\\"aws-quota-slice\\\" 2020/10/22 22:17:22 Executing template e2e-aws-serial 2020/10/22 22:17:22 Creating or restarting template instance 2020/10/22 22:17:22 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/22 22:17:22 Waiting for template instance to be ready 2020/10/22 22:17:24 Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-1 (zones: us-east-1b us-east-1c) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=warning msg=\\\"Found override for ReleaseImage. Please be warned, this is not advised\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Master Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Worker Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Openshift Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Common Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Creating infrastructure resources...\\\" level=error level=error msg=\\\"Error: Error applying plan:\\\" level=error level=error msg=\\\"1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* module.masters.aws_instance.master[2]: 1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* aws_instance.master.2: Error waiting for instance (i-08d1bca630d4287a5) to become ready: Failed to reach target state. Reason: Server.InternalError: Internal error on launch\\\" level=error level=error level=error level=error level=error level=error msg=\\\"Terraform does not automatically rollback in the face of errors.\\\" level=error msg=\\\"Instead, your Terraform state file has been partially updated with\\\" level=error msg=\\\"any resources that successfully completed. Please address the error\\\" level=error msg=\\\"above and apply again to incrementally change your infrastructure.\\\" level=error level=error level=fatal msg=\\\"failed to fetch Cluster: failed to generate asset \\\\\\\\\\\"Cluster\\\\\\\\\\\": failed to create cluster: failed to apply using Terraform\\\" 2020/10/22 22:24:37 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2020/10/22 22:29:33 Copied 6.97MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/22 22:29:33 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/22 22:29:33 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/22 22:29:33 Ran for 13m19s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-h6z8vdkq/e2e-aws-serial failed after 12m8s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-1 (zones: us-east-1b us-east-1c) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=warning msg=\\\"Found override for ReleaseImage. Please be warned, this is not advised\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Master Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Worker Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Openshift Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Common Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Creating infrastructure resources...\\\" level=error level=error msg=\\\"Error: Error applying plan:\\\" level=error level=error msg=\\\"1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* module.masters.aws_instance.master[2]: 1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* aws_instance.master.2: Error waiting for instance (i-08d1bca630d4287a5) to become ready: Failed to reach target state. Reason: Server.InternalError: Internal error on launch\\\" level=error level=error level=error level=error level=error level=error msg=\\\"Terraform does not automatically rollback in the face of errors.\\\" level=error msg=\\\"Instead, your Terraform state file has been partially updated with\\\" level=error msg=\\\"any resources that successfully completed. Please address the error\\\" level=error msg=\\\"above and apply again to incrementally change your infrastructure.\\\" level=error level=error level=fatal msg=\\\"failed to fetch Cluster: failed to generate asset \\\\\\\\\\\"Cluster\\\\\\\\\\\": failed to create cluster: failed to apply using Terraform\\\" --- '\", \"cluster_count\": 16}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 14, \"cluster_size\": 6, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 16}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 17, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/24 22:17:26 ci-operator version v20201023-52a7284 2020/10/24 22:17:26 No source defined 2020/10/24 22:17:26 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/24 22:17:26 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-y3vkcpw1 2020/10/24 22:17:26 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/24 22:17:26 Creating namespace ci-op-y3vkcpw1 2020/10/24 22:17:26 Setting up pipeline imagestream for the test 2020/10/24 22:17:26 Created secret e2e-aws-serial-cluster-profile 2020/10/24 22:17:26 Created secret pull-secret 2020/10/24 22:17:26 Created PDB for pods with openshift.io/build.name label 2020/10/24 22:17:26 Created PDB for pods with created-by-ci label 2020/10/24 22:17:26 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-y3vkcpw1/stable:${component} 2020/10/24 22:17:28 Importing release image latest 2020/10/24 22:17:29 Executing pod \\\"release-images-latest-cli\\\" 2020/10/24 22:17:33 Executing pod \\\"release-images-latest\\\" 2020/10/24 22:18:28 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/24 22:18:28 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/24 22:18:28 Acquired lease \\\"0ef89c10-0bc5-4e3d-93ee-66f9b8ce1365\\\" for \\\"aws-quota-slice\\\" 2020/10/24 22:18:28 Executing template e2e-aws-serial 2020/10/24 22:18:28 Creating or restarting template instance 2020/10/24 22:18:28 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/24 22:18:28 Waiting for template instance to be ready 2020/10/24 22:18:30 Running pod e2e-aws-serial 2020/10/24 22:46:36 Container setup in pod e2e-aws-serial completed successfully 2020/10/25 00:07:23 Container test in pod e2e-aws-serial completed successfully 2020/10/25 00:13:23 Copied 116.33MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/25 00:13:23 Releasing lease for \\\"aws-quota-slice\\\" 2020/10/25 00:13:23 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/25 00:13:23 Ran for 1h55m57s '\", \"cluster_count\": 17}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 14, \"cluster_size\": 7, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 17}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 17, \"cluster_size\": 2, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 17}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 14, \"cluster_size\": 8, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 17}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 14, \"cluster_size\": 9, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 17}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 14, \"cluster_size\": 10, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 17}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 14, \"cluster_size\": 11, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 17}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 18, \"cluster_size\": 1, \"template_mined\": \"b'2020/10/31 22:22:59 ci-operator version v20201031-2d599eb 2020/10/31 22:22:59 No source defined 2020/10/31 22:22:59 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/31 22:22:59 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-w4p1zz2v 2020/10/31 22:22:59 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/31 22:22:59 Creating namespace ci-op-w4p1zz2v 2020/10/31 22:22:59 Setting up pipeline imagestream for the test 2020/10/31 22:22:59 Created secret e2e-aws-serial-cluster-profile 2020/10/31 22:22:59 Created secret pull-secret 2020/10/31 22:22:59 Created PDB for pods with openshift.io/build.name label 2020/10/31 22:22:59 Created PDB for pods with created-by-ci label 2020/10/31 22:22:59 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-w4p1zz2v/stable:${component} 2020/10/31 22:23:02 Importing release image latest 2020/10/31 22:23:03 Executing pod \\\"release-images-latest-cli\\\" 2020/10/31 22:23:06 Executing pod \\\"release-images-latest\\\" 2020/10/31 22:24:33 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/31 22:24:33 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/10/31 22:24:33 Acquiring lease for \\\"aws-quota-slice\\\" 2020/10/31 22:24:33 Acquired lease \\\"6170012f-1198-462f-aafd-ab1c54a58851\\\" for \\\"aws-quota-slice\\\" 2020/10/31 22:24:33 Executing template e2e-aws-serial 2020/10/31 22:24:33 Creating or restarting template instance 2020/10/31 22:24:33 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/31 22:24:33 Waiting for template instance to be ready 2020/10/31 22:24:35 Running pod e2e-aws-serial 2020/10/31 22:50:56 Container setup in pod e2e-aws-serial completed successfully 2020/11/01 00:05:22 Copied 104.57MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/11/01 00:05:23 Releasing leases for \\\"e2e-aws-serial\\\" 2020/11/01 00:05:23 Releasing lease \\\"6170012f-1198-462f-aafd-ab1c54a58851\\\" for \\\"aws-quota-slice\\\" 2020/11/01 00:05:23 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/11/01 00:05:23 Ran for 1h42m23s '\", \"cluster_count\": 18}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 19, \"cluster_size\": 1, \"template_mined\": \"b'2020/11/01 22:23:50 ci-operator version v20201031-2d599eb 2020/11/01 22:23:50 No source defined 2020/11/01 22:23:50 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/11/01 22:23:50 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-r533i76t 2020/11/01 22:23:50 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/11/01 22:23:50 Creating namespace ci-op-r533i76t 2020/11/01 22:23:50 Setting up pipeline imagestream for the test 2020/11/01 22:23:50 Created secret e2e-aws-serial-cluster-profile 2020/11/01 22:23:50 Created secret pull-secret 2020/11/01 22:23:50 Created PDB for pods with openshift.io/build.name label 2020/11/01 22:23:50 Created PDB for pods with created-by-ci label 2020/11/01 22:23:50 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-r533i76t/stable:${component} 2020/11/01 22:23:53 Importing release image latest 2020/11/01 22:23:53 Executing pod \\\"release-images-latest-cli\\\" 2020/11/01 22:23:59 Executing pod \\\"release-images-latest\\\" 2020/11/01 22:24:47 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/11/01 22:24:47 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/11/01 22:24:47 Acquiring lease for \\\"aws-quota-slice\\\" 2020/11/01 22:24:47 Acquired lease \\\"2ac7db5a-35b5-4a53-b989-346f5c1cd386\\\" for \\\"aws-quota-slice\\\" 2020/11/01 22:24:47 Executing template e2e-aws-serial 2020/11/01 22:24:47 Creating or restarting template instance 2020/11/01 22:24:47 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/11/01 22:24:47 Waiting for template instance to be ready 2020/11/01 22:24:49 Running pod e2e-aws-serial 2020/11/01 22:57:48 Container setup in pod e2e-aws-serial completed successfully 2020/11/02 00:07:48 Container test in pod e2e-aws-serial completed successfully 2020/11/02 00:13:32 Container teardown in pod e2e-aws-serial completed successfully 2020/11/02 00:13:32 Pod e2e-aws-serial succeeded after 1h48m43s 2020/11/02 00:13:39 Copied 110.96MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/11/02 00:13:39 Releasing leases for \\\"e2e-aws-serial\\\" 2020/11/02 00:13:39 Releasing lease \\\"2ac7db5a-35b5-4a53-b989-346f5c1cd386\\\" for \\\"aws-quota-slice\\\" 2020/11/02 00:13:39 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/11/02 00:13:39 Ran for 1h49m49s '\", \"cluster_count\": 19}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 18, \"cluster_size\": 2, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 19}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 19, \"cluster_size\": 2, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 19}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 18, \"cluster_size\": 3, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 19}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 18, \"cluster_size\": 4, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 19}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 18, \"cluster_size\": 5, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 19}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 19, \"cluster_size\": 3, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 19}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 19, \"cluster_size\": 4, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 19}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 19, \"cluster_size\": 5, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 19}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 20, \"cluster_size\": 1, \"template_mined\": \"b'2020/11/10 22:30:03 ci-operator version v20201110-c1e9e9c 2020/11/10 22:30:03 No source defined 2020/11/10 22:30:03 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/11/10 22:30:03 warning: overriding parameter \\\"LEASED_RESOURCE\\\" 2020/11/10 22:30:03 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-grvg2yy3 2020/11/10 22:30:03 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/11/10 22:30:03 Creating namespace ci-op-grvg2yy3 2020/11/10 22:30:03 Setting up pipeline imagestream for the test 2020/11/10 22:30:03 Created secret e2e-aws-serial-cluster-profile 2020/11/10 22:30:03 Created secret pull-secret 2020/11/10 22:30:03 Created PDB for pods with openshift.io/build.name label 2020/11/10 22:30:03 Created PDB for pods with created-by-ci label 2020/11/10 22:30:03 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-grvg2yy3/stable:${component} 2020/11/10 22:30:05 Importing release image latest 2020/11/10 22:30:06 Executing pod \\\"release-images-latest-cli\\\" 2020/11/10 22:30:21 Executing pod \\\"release-images-latest\\\" 2020/11/10 22:31:20 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/11/10 22:31:20 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/11/10 22:31:20 Acquiring lease for \\\"aws-quota-slice\\\" 2020/11/10 22:31:20 Acquired lease \\\"ab46917a-2030-440d-aed8-da1479047658\\\" for \\\"aws-quota-slice\\\" 2020/11/10 22:31:20 Executing template e2e-aws-serial 2020/11/10 22:31:20 Creating or restarting template instance 2020/11/10 22:31:20 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/11/10 22:31:20 Waiting for template instance to be ready 2020/11/10 22:31:23 Running pod e2e-aws-serial 2020/11/10 23:05:56 Container setup in pod e2e-aws-serial completed successfully 2020/11/11 00:28:06 Copied 106.70MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/11/11 00:28:06 Releasing leases for \\\"e2e-aws-serial\\\" 2020/11/11 00:28:06 Releasing lease \\\"ab46917a-2030-440d-aed8-da1479047658\\\" for \\\"aws-quota-slice\\\" 2020/11/11 00:28:06 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/11/11 00:28:07 Ran for 1h58m3s '\", \"cluster_count\": 20}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 20, \"cluster_size\": 2, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> warning: overriding parameter \\\"LEASED_RESOURCE\\\" <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 20}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 20, \"cluster_size\": 3, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> warning: overriding parameter \\\"LEASED_RESOURCE\\\" <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 20}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 20, \"cluster_size\": 4, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> warning: overriding parameter \\\"LEASED_RESOURCE\\\" <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 20}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 21, \"cluster_size\": 1, \"template_mined\": \"b'2020/11/14 22:34:12 ci-operator version v20201113-e46b6a4 2020/11/14 22:34:12 No source defined 2020/11/14 22:34:12 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/11/14 22:34:12 warning: overriding parameter \\\"LEASED_RESOURCE\\\" 2020/11/14 22:34:12 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-i12kcdn7 2020/11/14 22:34:12 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/11/14 22:34:12 Creating namespace ci-op-i12kcdn7 2020/11/14 22:34:12 Setting up pipeline imagestream for the test 2020/11/14 22:34:12 Created secret e2e-aws-serial-cluster-profile 2020/11/14 22:34:12 Created secret pull-secret 2020/11/14 22:34:12 Created PDB for pods with openshift.io/build.name label 2020/11/14 22:34:12 Created PDB for pods with created-by-ci label 2020/11/14 22:34:12 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-i12kcdn7/stable:${component} 2020/11/14 22:34:16 Importing release image latest 2020/11/14 22:34:17 Executing pod \\\"release-images-latest-cli\\\" 2020/11/14 22:34:30 Executing pod \\\"release-images-latest\\\" 2020/11/14 22:35:24 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/11/14 22:35:24 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/11/14 22:35:24 Acquiring lease for \\\"aws-quota-slice\\\" 2020/11/14 22:35:24 Acquired lease \\\"fd5aff4f-18ec-4542-a726-2510deb7a53d\\\" for \\\"aws-quota-slice\\\" 2020/11/14 22:35:24 Executing template e2e-aws-serial 2020/11/14 22:35:24 Creating or restarting template instance 2020/11/14 22:35:24 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/11/14 22:35:24 Waiting for template instance to be ready 2020/11/14 22:35:26 Running pod e2e-aws-serial 2020/11/14 23:03:09 Container setup in pod e2e-aws-serial completed successfully 2020/11/15 00:12:58 Container test in pod e2e-aws-serial completed successfully 2020/11/15 00:19:23 Copied 108.07MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/11/15 00:19:24 Releasing leases for \\\"e2e-aws-serial\\\" 2020/11/15 00:19:24 Releasing lease \\\"fd5aff4f-18ec-4542-a726-2510deb7a53d\\\" for \\\"aws-quota-slice\\\" 2020/11/15 00:19:24 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/11/15 00:19:24 Ran for 1h45m12s '\", \"cluster_count\": 21}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 20, \"cluster_size\": 5, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> warning: overriding parameter \\\"LEASED_RESOURCE\\\" <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 21}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 22, \"cluster_size\": 1, \"template_mined\": \"b'2020/11/16 22:36:12 ci-operator version v20201116-df0a006 2020/11/16 22:36:12 No source defined 2020/11/16 22:36:12 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/11/16 22:36:13 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-iq3f65xv 2020/11/16 22:36:13 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/11/16 22:36:13 Creating namespace ci-op-iq3f65xv 2020/11/16 22:36:13 Setting up pipeline imagestream for the test 2020/11/16 22:36:13 Created secret e2e-aws-serial-cluster-profile 2020/11/16 22:36:13 Created secret pull-secret 2020/11/16 22:36:13 Created PDB for pods with openshift.io/build.name label 2020/11/16 22:36:13 Created PDB for pods with created-by-ci label 2020/11/16 22:36:13 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-iq3f65xv/stable:${component} 2020/11/16 22:36:15 Importing release image latest 2020/11/16 22:36:15 Executing pod \\\"release-images-latest-cli\\\" 2020/11/16 22:36:21 Executing pod \\\"release-images-latest\\\" 2020/11/16 22:37:08 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/11/16 22:37:08 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/11/16 22:37:08 Acquiring lease for \\\"aws-quota-slice\\\" 2020/11/16 22:37:08 Acquired lease \\\"38b2813b-1b56-40c2-8c7a-02ce1a2b941b\\\" for \\\"aws-quota-slice\\\" 2020/11/16 22:37:08 Executing template e2e-aws-serial 2020/11/16 22:37:08 Creating or restarting template instance 2020/11/16 22:37:08 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/11/16 22:37:08 Waiting for template instance to be ready 2020/11/16 22:37:10 Running pod e2e-aws-serial 2020/11/16 23:06:50 Container setup in pod e2e-aws-serial completed successfully 2020/11/17 00:16:26 Container test in pod e2e-aws-serial completed successfully 2020/11/17 00:24:38 Copied 110.00MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/11/17 00:24:38 Releasing leases for \\\"e2e-aws-serial\\\" 2020/11/17 00:24:38 Releasing lease \\\"38b2813b-1b56-40c2-8c7a-02ce1a2b941b\\\" for \\\"aws-quota-slice\\\" 2020/11/17 00:24:38 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/11/17 00:24:38 Ran for 1h48m25s '\", \"cluster_count\": 22}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 18, \"cluster_size\": 6, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 22}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 22, \"cluster_size\": 2, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 22}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 23, \"cluster_size\": 1, \"template_mined\": \"b'2020/11/19 22:39:26 ci-operator version v20201119-ff03225 2020/11/19 22:39:26 No source defined 2020/11/19 22:39:26 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/11/19 22:39:26 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-bxig87c1 2020/11/19 22:39:26 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/11/19 22:39:26 Creating namespace ci-op-bxig87c1 2020/11/19 22:39:26 Setting up pipeline imagestream for the test 2020/11/19 22:39:26 Created secret e2e-aws-serial-cluster-profile 2020/11/19 22:39:26 Created secret pull-secret 2020/11/19 22:39:26 Created PDB for pods with openshift.io/build.name label 2020/11/19 22:39:26 Created PDB for pods with created-by-ci label 2020/11/19 22:39:26 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-bxig87c1/stable:${component} 2020/11/19 22:39:28 Importing release image latest 2020/11/19 22:39:29 Executing pod \\\"release-images-latest-cli\\\" 2020/11/19 22:39:34 Executing pod \\\"release-images-latest\\\" 2020/11/19 22:41:00 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/11/19 22:41:00 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/11/19 22:41:00 Acquiring lease for \\\"aws-quota-slice\\\" 2020/11/19 22:41:00 Acquired lease \\\"d65cd353-d217-4fbd-ba5b-aa4a522fb068\\\" for \\\"aws-quota-slice\\\" 2020/11/19 22:41:00 Executing template e2e-aws-serial 2020/11/19 22:41:00 Creating or restarting template instance 2020/11/19 22:41:00 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/11/19 22:41:00 Waiting for template instance to be ready 2020/11/19 22:41:02 Running pod e2e-aws-serial 2020/11/19 23:16:21 Container setup in pod e2e-aws-serial completed successfully secret/support created started: (0/1/79) \\\"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\\\" passed: (1m2s) 2020-11-19T23:17:30 \\\"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\\\" started: (0/2/79) \\\"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m4s) 2020-11-19T23:19:34 \\\"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/3/79) \\\"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/scheduling/taint_based_evictions.go:63]: Requires at least 2 nodes (not -1) skipped: (20.4s) 2020-11-19T23:19:54 \\\"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/4/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (26.4s) 2020-11-19T23:20:21 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/5/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (23s) 2020-11-19T23:20:44 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/6/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-19T23:21:24 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/7/79) \\\"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m2s) 2020-11-19T23:23:26 \\\"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/8/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m54s) 2020-11-19T23:26:20 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/9/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (22.6s) 2020-11-19T23:26:42 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/10/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40s) 2020-11-19T23:27:22 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/11/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (50.6s) 2020-11-19T23:28:13 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/12/79) \\\"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m28s) 2020-11-19T23:29:41 \\\"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/13/79) \\\"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/apps/daemon_set.go:378]: Requires at least 2 nodes (not -1) skipped: (21.5s) 2020-11-19T23:30:02 \\\"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/14/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-19T23:30:42 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/15/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-19T23:31:23 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/16/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24s) 2020-11-19T23:31:47 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/17/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m49s) 2020-11-19T23:34:36 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/18/79) \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (37.9s) 2020-11-19T23:35:14 \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/19/79) \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m5s) 2020-11-19T23:36:18 \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/20/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.2s) 2020-11-19T23:36:59 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/21/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m30s) 2020-11-19T23:38:28 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/22/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.8s) 2020-11-19T23:38:49 \\\"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/23/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-19T23:39:30 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/24/79) \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (41.7s) 2020-11-19T23:40:11 \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/25/79) \\\"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:193]: Could not get controller-manager metrics - skipping skipped: (1m9s) 2020-11-19T23:41:21 \\\"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/26/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m37s) 2020-11-19T23:43:58 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/27/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m5s) 2020-11-19T23:46:03 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/28/79) \\\"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\\\" passed: (37.1s) 2020-11-19T23:46:40 \\\"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\\\" started: (0/29/79) \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m48s) 2020-11-19T23:48:28 \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/30/79) \\\"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\\\" skip [github.com/openshift/origin/test/extended/images/signatures.go:25]: disable because containers/image: https://github.com/containers/image/pull/570 skipped: (15.2s) 2020-11-19T23:48:44 \\\"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\\\" started: (0/31/79) \\\"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:247]: Could not get controller-manager metrics - skipping skipped: (28.6s) 2020-11-19T23:49:12 \\\"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/32/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.1s) 2020-11-19T23:49:36 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/33/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.3s) 2020-11-19T23:50:01 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/34/79) \\\"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m36s) 2020-11-19T23:51:37 \\\"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/35/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24s) 2020-11-19T23:52:01 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/36/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m44s) 2020-11-19T23:54:44 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/37/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m0s) 2020-11-19T23:56:45 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/38/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.5s) 2020-11-19T23:57:09 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/39/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-11-19T23:57:49 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/40/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.2s) 2020-11-19T23:58:30 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/41/79) \\\"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:81]: Environment does not support getting controller-manager metrics - skipping skipped: (20.7s) 2020-11-19T23:58:50 \\\"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/42/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m56s) 2020-11-20T00:00:46 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/43/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-20T00:01:27 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/44/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (43.6s) 2020-11-20T00:02:10 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/45/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-20T00:02:51 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/46/79) \\\"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (2m10s) 2020-11-20T00:05:01 \\\"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/47/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-11-20T00:05:41 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/48/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m33s) 2020-11-20T00:08:14 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/49/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-20T00:08:54 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/50/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.8s) 2020-11-20T00:09:15 \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/51/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" Nov 20 00:09:16.498: INFO: >>> kubeConfig: /tmp/admin.kubeconfig Nov 20 00:09:16.500: INFO: Waiting up to 30m0s for all (but 100) nodes to be schedulable Nov 20 00:09:17.021: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace \\\\'kube-system\\\\' to be running and ready Nov 20 00:09:17.286: INFO: 0 / 0 pods in namespace \\\\'kube-system\\\\' are running and ready (0 seconds elapsed) Nov 20 00:09:17.286: INFO: expected 0 pod replicas in namespace \\\\'kube-system\\\\', 0 are Running and Ready. Nov 20 00:09:17.286: INFO: Waiting up to 5m0s for all daemonsets in namespace \\\\'kube-system\\\\' to start Nov 20 00:09:17.379: INFO: e2e test version: v1.13.4-138-g41dc99c Nov 20 00:09:17.461: INFO: kube-apiserver version: v1.13.4-138-g41dc99c [BeforeEach] [Top Level] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/test/extended/util/test.go:69 [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:153 STEP: Creating a kubernetes client Nov 20 00:09:17.463: INFO: >>> kubeConfig: /tmp/admin.kubeconfig STEP: Building a namespace api object, basename sched-priority Nov 20 00:09:22.594: INFO: About to run a Kube e2e test, ensuring namespace is privileged Nov 20 00:09:23.627: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled. STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:71 Nov 20 00:09:23.710: INFO: Waiting up to 1m0s for all nodes to be ready Nov 20 00:10:24.824: INFO: Waiting for terminating namespaces to be deleted... Nov 20 00:10:24.909: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace \\\\'kube-system\\\\' to be running and ready Nov 20 00:10:25.163: INFO: 0 / 0 pods in namespace \\\\'kube-system\\\\' are running and ready (0 seconds elapsed) Nov 20 00:10:25.163: INFO: expected 0 pod replicas in namespace \\\\'kube-system\\\\', 0 are Running and Ready. [It] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:146 Nov 20 00:10:25.163: INFO: ComputeCpuMemFraction for node: ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:10:25.264: INFO: Pod for on the node: tuned-jfbcn, Cpu: 10, Mem: 20971520 Nov 20 00:10:25.264: INFO: Pod for on the node: dns-default-9bb6j, Cpu: 110, Mem: 283115520 Nov 20 00:10:25.264: INFO: Pod for on the node: node-ca-rdx58, Cpu: 10, Mem: 10485760 Nov 20 00:10:25.264: INFO: Pod for on the node: machine-config-daemon-2nwhn, Cpu: 20, Mem: 52428800 Nov 20 00:10:25.264: INFO: Pod for on the node: node-exporter-dxfx8, Cpu: 110, Mem: 230686720 Nov 20 00:10:25.264: INFO: Pod for on the node: multus-z4jlm, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.264: INFO: Pod for on the node: ovs-9dthp, Cpu: 200, Mem: 419430400 Nov 20 00:10:25.264: INFO: Pod for on the node: sdn-dfk44, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.264: INFO: Node: ip-10-0-132-239.us-west-2.compute.internal, totalRequestedCpuResource: 660, cpuAllocatableMil: 3500, cpuFraction: 0.18857142857142858 Nov 20 00:10:25.264: INFO: Node: ip-10-0-132-239.us-west-2.compute.internal, totalRequestedMemResource: 1331691520, memAllocatableVal: 16181792768, memFraction: 0.08229567261752736 Nov 20 00:10:25.264: INFO: ComputeCpuMemFraction for node: ip-10-0-134-44.us-west-2.compute.internal Nov 20 00:10:25.371: INFO: Pod for on the node: tuned-wgwsl, Cpu: 10, Mem: 20971520 Nov 20 00:10:25.371: INFO: Pod for on the node: dns-default-p6jkj, Cpu: 110, Mem: 283115520 Nov 20 00:10:25.371: INFO: Pod for on the node: image-registry-54fd967995-sjxmw, Cpu: 100, Mem: 268435456 Nov 20 00:10:25.371: INFO: Pod for on the node: node-ca-xkjnr, Cpu: 10, Mem: 10485760 Nov 20 00:10:25.371: INFO: Pod for on the node: router-default-756b45d66d-dtcjj, Cpu: 100, Mem: 268435456 Nov 20 00:10:25.371: INFO: Pod for on the node: machine-config-daemon-9cdtt, Cpu: 20, Mem: 52428800 Nov 20 00:10:25.371: INFO: Pod for on the node: certified-operators-74db69bc55-vjs88, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.371: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Nov 20 00:10:25.371: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Nov 20 00:10:25.371: INFO: Pod for on the node: grafana-649f787944-7r225, Cpu: 200, Mem: 314572800 Nov 20 00:10:25.371: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-kt9sp, Cpu: 300, Mem: 629145600 Nov 20 00:10:25.371: INFO: Pod for on the node: node-exporter-9xlm7, Cpu: 110, Mem: 230686720 Nov 20 00:10:25.371: INFO: Pod for on the node: prometheus-adapter-6b8bf7fcb6-c9ccp, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.371: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Nov 20 00:10:25.372: INFO: Pod for on the node: multus-8sxsv, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.372: INFO: Pod for on the node: ovs-cxp2j, Cpu: 200, Mem: 419430400 Nov 20 00:10:25.372: INFO: Pod for on the node: sdn-b6gvc, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.372: INFO: Node: ip-10-0-134-44.us-west-2.compute.internal, totalRequestedCpuResource: 1660, cpuAllocatableMil: 3500, cpuFraction: 0.4742857142857143 Nov 20 00:10:25.372: INFO: Node: ip-10-0-134-44.us-west-2.compute.internal, totalRequestedMemResource: 3441426432, memAllocatableVal: 16181800960, memFraction: 0.21267264629610177 Nov 20 00:10:25.372: INFO: ComputeCpuMemFraction for node: ip-10-0-146-237.us-west-2.compute.internal Nov 20 00:10:25.473: INFO: Pod for on the node: tuned-69g6d, Cpu: 10, Mem: 20971520 Nov 20 00:10:25.473: INFO: Pod for on the node: dns-default-9tv4w, Cpu: 110, Mem: 283115520 Nov 20 00:10:25.473: INFO: Pod for on the node: node-ca-fl47z, Cpu: 10, Mem: 10485760 Nov 20 00:10:25.473: INFO: Pod for on the node: router-default-756b45d66d-dc2ds, Cpu: 100, Mem: 268435456 Nov 20 00:10:25.473: INFO: Pod for on the node: machine-config-daemon-rtlm7, Cpu: 20, Mem: 52428800 Nov 20 00:10:25.473: INFO: Pod for on the node: community-operators-65d8c88b5c-lrgl8, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.473: INFO: Pod for on the node: redhat-operators-598f74f9b-h42zk, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.473: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Nov 20 00:10:25.473: INFO: Pod for on the node: node-exporter-tjp24, Cpu: 110, Mem: 230686720 Nov 20 00:10:25.473: INFO: Pod for on the node: prometheus-adapter-6b8bf7fcb6-mwh87, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.473: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Nov 20 00:10:25.473: INFO: Pod for on the node: prometheus-operator-5d4588dd6-d6zf5, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.473: INFO: Pod for on the node: telemeter-client-5d87684b5-lzp2x, Cpu: 210, Mem: 440401920 Nov 20 00:10:25.473: INFO: Pod for on the node: multus-l7gw6, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.473: INFO: Pod for on the node: ovs-rkp57, Cpu: 200, Mem: 419430400 Nov 20 00:10:25.473: INFO: Pod for on the node: sdn-jvhnl, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.473: INFO: Node: ip-10-0-146-237.us-west-2.compute.internal, totalRequestedCpuResource: 1270, cpuAllocatableMil: 3500, cpuFraction: 0.3628571428571429 Nov 20 00:10:25.473: INFO: Node: ip-10-0-146-237.us-west-2.compute.internal, totalRequestedMemResource: 2669674496, memAllocatableVal: 16181800960, memFraction: 0.16498006016754269 Nov 20 00:10:25.570: INFO: Waiting for running... Nov 20 00:10:35.761: INFO: Waiting for running... Nov 20 00:10:50.953: INFO: Waiting for running... STEP: Compute Cpu, Mem Fraction after create balanced pods. Nov 20 00:11:01.055: INFO: ComputeCpuMemFraction for node: ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:11:01.571: INFO: Pod for on the node: c9ee7b37-2ac4-11eb-9365-0a58ac104a46-0, Cpu: 1089, Mem: 6759204864 Nov 20 00:11:01.571: INFO: Pod for on the node: tuned-jfbcn, Cpu: 10, Mem: 20971520 Nov 20 00:11:01.571: INFO: Pod for on the node: dns-default-9bb6j, Cpu: 110, Mem: 283115520 Nov 20 00:11:01.571: INFO: Pod for on the node: node-ca-rdx58, Cpu: 10, Mem: 10485760 Nov 20 00:11:01.571: INFO: Pod for on the node: machine-config-daemon-2nwhn, Cpu: 20, Mem: 52428800 Nov 20 00:11:01.571: INFO: Pod for on the node: node-exporter-dxfx8, Cpu: 110, Mem: 230686720 Nov 20 00:11:01.571: INFO: Pod for on the node: multus-z4jlm, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.571: INFO: Pod for on the node: ovs-9dthp, Cpu: 200, Mem: 419430400 Nov 20 00:11:01.571: INFO: Pod for on the node: sdn-dfk44, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.571: INFO: Node: ip-10-0-132-239.us-west-2.compute.internal, totalRequestedCpuResource: 1749, cpuAllocatableMil: 3500, cpuFraction: 0.4997142857142857 Nov 20 00:11:01.571: INFO: Node: ip-10-0-132-239.us-west-2.compute.internal, totalRequestedMemResource: 8090896384, memAllocatableVal: 16181792768, memFraction: 0.5 STEP: Compute Cpu, Mem Fraction after create balanced pods. Nov 20 00:11:01.571: INFO: ComputeCpuMemFraction for node: ip-10-0-134-44.us-west-2.compute.internal Nov 20 00:11:01.752: INFO: Pod for on the node: d00269c8-2ac4-11eb-9365-0a58ac104a46-0, Cpu: 89, Mem: 4649474048 Nov 20 00:11:01.752: INFO: Pod for on the node: tuned-wgwsl, Cpu: 10, Mem: 20971520 Nov 20 00:11:01.752: INFO: Pod for on the node: dns-default-p6jkj, Cpu: 110, Mem: 283115520 Nov 20 00:11:01.752: INFO: Pod for on the node: image-registry-54fd967995-sjxmw, Cpu: 100, Mem: 268435456 Nov 20 00:11:01.752: INFO: Pod for on the node: node-ca-xkjnr, Cpu: 10, Mem: 10485760 Nov 20 00:11:01.752: INFO: Pod for on the node: router-default-756b45d66d-dtcjj, Cpu: 100, Mem: 268435456 Nov 20 00:11:01.752: INFO: Pod for on the node: machine-config-daemon-9cdtt, Cpu: 20, Mem: 52428800 Nov 20 00:11:01.752: INFO: Pod for on the node: certified-operators-74db69bc55-vjs88, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.752: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Nov 20 00:11:01.752: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Nov 20 00:11:01.752: INFO: Pod for on the node: grafana-649f787944-7r225, Cpu: 200, Mem: 314572800 Nov 20 00:11:01.752: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-kt9sp, Cpu: 300, Mem: 629145600 Nov 20 00:11:01.752: INFO: Pod for on the node: node-exporter-9xlm7, Cpu: 110, Mem: 230686720 Nov 20 00:11:01.752: INFO: Pod for on the node: prometheus-adapter-6b8bf7fcb6-c9ccp, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.752: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Nov 20 00:11:01.752: INFO: Pod for on the node: multus-8sxsv, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.752: INFO: Pod for on the node: ovs-cxp2j, Cpu: 200, Mem: 419430400 Nov 20 00:11:01.752: INFO: Pod for on the node: sdn-b6gvc, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.752: INFO: Node: ip-10-0-134-44.us-west-2.compute.internal, totalRequestedCpuResource: 1749, cpuAllocatableMil: 3500, cpuFraction: 0.4997142857142857 Nov 20 00:11:01.752: INFO: Node: ip-10-0-134-44.us-west-2.compute.internal, totalRequestedMemResource: 8090900480, memAllocatableVal: 16181800960, memFraction: 0.5 STEP: Compute Cpu, Mem Fraction after create balanced pods. Nov 20 00:11:01.752: INFO: ComputeCpuMemFraction for node: ip-10-0-146-237.us-west-2.compute.internal Nov 20 00:11:01.853: INFO: Pod for on the node: d910877e-2ac4-11eb-9365-0a58ac104a46-0, Cpu: 479, Mem: 5421225984 Nov 20 00:11:01.854: INFO: Pod for on the node: tuned-69g6d, Cpu: 10, Mem: 20971520 Nov 20 00:11:01.854: INFO: Pod for on the node: dns-default-9tv4w, Cpu: 110, Mem: 283115520 Nov 20 00:11:01.854: INFO: Pod for on the node: node-ca-fl47z, Cpu: 10, Mem: 10485760 Nov 20 00:11:01.854: INFO: Pod for on the node: router-default-756b45d66d-dc2ds, Cpu: 100, Mem: 268435456 Nov 20 00:11:01.854: INFO: Pod for on the node: machine-config-daemon-rtlm7, Cpu: 20, Mem: 52428800 Nov 20 00:11:01.854: INFO: Pod for on the node: community-operators-65d8c88b5c-lrgl8, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.854: INFO: Pod for on the node: redhat-operators-598f74f9b-h42zk, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.854: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Nov 20 00:11:01.854: INFO: Pod for on the node: node-exporter-tjp24, Cpu: 110, Mem: 230686720 Nov 20 00:11:01.854: INFO: Pod for on the node: prometheus-adapter-6b8bf7fcb6-mwh87, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.854: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Nov 20 00:11:01.854: INFO: Pod for on the node: prometheus-operator-5d4588dd6-d6zf5, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.854: INFO: Pod for on the node: telemeter-client-5d87684b5-lzp2x, Cpu: 210, Mem: 440401920 Nov 20 00:11:01.854: INFO: Pod for on the node: multus-l7gw6, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.854: INFO: Pod for on the node: ovs-rkp57, Cpu: 200, Mem: 419430400 Nov 20 00:11:01.854: INFO: Pod for on the node: sdn-jvhnl, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.854: INFO: Node: ip-10-0-146-237.us-west-2.compute.internal, totalRequestedCpuResource: 1749, cpuAllocatableMil: 3500, cpuFraction: 0.4997142857142857 Nov 20 00:11:01.854: INFO: Node: ip-10-0-146-237.us-west-2.compute.internal, totalRequestedMemResource: 8090900480, memAllocatableVal: 16181800960, memFraction: 0.5 STEP: Create a RC, with 0 replicas STEP: Trying to apply avoidPod annotations on the first node. Nov 20 00:11:02.278: INFO: Conflict when trying to add/update avoidPonds {[{{&OwnerReference{Kind:ReplicationController,Name:scheduler-priority-avoid-pod,UID:dfa4592a-2ac4-11eb-aac2-069d8b705ae3,APIVersion:v1,Controller:*true,BlockOwnerDeletion:nil,}} 0001-01-01 00:00:00 +0000 UTC some reson some message}]} to ip-10-0-132-239.us-west-2.compute.internal STEP: deleting ReplicationController scheduler-priority-avoid-pod in namespace e2e-tests-sched-priority-9txg9, will wait for the garbage collector to delete the pods Nov 20 00:13:02.555: INFO: Deleting ReplicationController scheduler-priority-avoid-pod took: 89.171437ms Nov 20 00:13:02.555: INFO: Terminating ReplicationController scheduler-priority-avoid-pod pods took: 68.656\\\\xc2\\\\xb5s [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:154 STEP: Collecting events from namespace \\\"e2e-tests-sched-priority-9txg9\\\". STEP: Found 9 events. Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:33 +0000 UTC - event for c9ee7b37-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-132-239.us-west-2.compute.internal} Pulled: Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:33 +0000 UTC - event for c9ee7b37-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-132-239.us-west-2.compute.internal} Created: Created container c9ee7b37-2ac4-11eb-9365-0a58ac104a46-0 Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:33 +0000 UTC - event for c9ee7b37-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-132-239.us-west-2.compute.internal} Started: Started container c9ee7b37-2ac4-11eb-9365-0a58ac104a46-0 Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:43 +0000 UTC - event for d00269c8-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-134-44.us-west-2.compute.internal} Pulled: Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:44 +0000 UTC - event for d00269c8-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-134-44.us-west-2.compute.internal} Created: Created container d00269c8-2ac4-11eb-9365-0a58ac104a46-0 Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:45 +0000 UTC - event for d00269c8-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-134-44.us-west-2.compute.internal} Started: Started container d00269c8-2ac4-11eb-9365-0a58ac104a46-0 Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:58 +0000 UTC - event for d910877e-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-146-237.us-west-2.compute.internal} Pulled: Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:58 +0000 UTC - event for d910877e-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-146-237.us-west-2.compute.internal} Created: Created container d910877e-2ac4-11eb-9365-0a58ac104a46-0 Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:58 +0000 UTC - event for d910877e-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-146-237.us-west-2.compute.internal} Started: Started container d910877e-2ac4-11eb-9365-0a58ac104a46-0 Nov 20 00:13:02.806: INFO: skipping dumping cluster info - cluster too large Nov 20 00:13:02.807: INFO: Waiting up to 3m0s for all (but 100) nodes to be ready STEP: Destroying namespace \\\"e2e-tests-sched-priority-9txg9\\\" for this suite. Nov 20 00:13:17.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Nov 20 00:13:23.195: INFO: namespace e2e-tests-sched-priority-9txg9 deletion completed in 20.301575405s [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:68 Nov 20 00:13:23.198: INFO: Running AfterSuite actions on all nodes Nov 20 00:13:23.198: INFO: Running AfterSuite actions on node 1 fail [k8s.io/kubernetes/test/e2e/scheduling/priorities.go:191]: Expected error: <*errors.errorString | 0xc0002d83f0>: { s: \\\"timed out waiting for the condition\\\", } timed out waiting for the condition not to have occurred failed: (4m8s) 2020-11-20T00:13:23 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/52/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.2s) 2020-11-20T00:14:03 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/53/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.5s) 2020-11-20T00:14:27 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/54/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (42.3s) 2020-11-20T00:15:10 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/55/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.9s) 2020-11-20T00:15:31 \\\"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/56/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-11-20T00:16:11 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/57/79) \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m8s) 2020-11-20T00:17:19 \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/58/79) \\\"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m15s) 2020-11-20T00:18:34 \\\"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/59/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m55s) 2020-11-20T00:20:28 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/60/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (35.2s) 2020-11-20T00:21:04 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/61/79) \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m50s) 2020-11-20T00:22:54 \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/62/79) \\\"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (2m55s) 2020-11-20T00:25:48 \\\"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/63/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-20T00:26:29 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/64/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.9s) 2020-11-20T00:27:10 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/65/79) \\\"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m9s) 2020-11-20T00:28:19 \\\"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/66/79) \\\"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (20.9s) 2020-11-20T00:28:39 \\\"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/67/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-20T00:29:20 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/68/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (34.3s) 2020-11-20T00:29:54 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/69/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.7s) 2020-11-20T00:30:15 \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/70/79) \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (35s) 2020-11-20T00:30:50 \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/71/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-20T00:31:31 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/72/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-11-20T00:32:11 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/73/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m33s) 2020-11-20T00:33:44 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/74/79) \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m44s) 2020-11-20T00:36:28 \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/75/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (57.1s) 2020-11-20T00:37:25 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/76/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-20T00:38:06 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/77/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m22s) 2020-11-20T00:39:28 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/78/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.1s) 2020-11-20T00:39:52 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/79/79) \\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m25s) 2020-11-20T00:41:17 \\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" Timeline: Nov 19 23:24:36.397 W ns/openshift-machine-config-operator pod/machine-config-daemon-lsnww node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:24:36.398 W ns/openshift-image-registry pod/node-ca-w7mtc node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:24:36.465 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:24:36.466 W ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:24:36.479 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-lsnww Nov 19 23:24:36.479 I ns/openshift-image-registry pod/node-ca-w7mtc Marking for deletion Pod openshift-image-registry/node-ca-w7mtc Nov 19 23:24:36.479 I ns/openshift-machine-config-operator pod/machine-config-daemon-lsnww Stopping container machine-config-daemon Nov 19 23:24:36.479 W ns/openshift-ingress pod/router-default-756b45d66d-nphxt node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:24:36.480 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 0s Nov 19 23:24:36.480 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:24:36.487 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-w7mtc Nov 19 23:24:36.487 I ns/openshift-image-registry pod/node-ca-w7mtc Stopping container node-ca Nov 19 23:24:36.487 I ns/openshift-monitoring pod/prometheus-k8s-1 Marking for deletion Pod openshift-monitoring/prometheus-k8s-1 Nov 19 23:24:36.487 I ns/openshift-machine-config-operator pod/machine-config-daemon-lsnww Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-lsnww Nov 19 23:24:36.487 I ns/openshift-monitoring pod/alertmanager-main-0 Marking for deletion Pod openshift-monitoring/alertmanager-main-0 Nov 19 23:24:36.487 I ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 node/ created Nov 19 23:24:36.488 I ns/openshift-ingress pod/router-default-756b45d66d-nphxt Marking for deletion Pod openshift-ingress/router-default-756b45d66d-nphxt Nov 19 23:24:36.488 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus Nov 19 23:24:36.488 I ns/openshift-ingress pod/router-default-756b45d66d-dtcjj node/ created Nov 19 23:24:36.488 I ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh Marking for deletion Pod openshift-marketplace/certified-operators-74db69bc55-9wggh Nov 19 23:24:36.488 I ns/openshift-monitoring pod/alertmanager-main-0 Cancelling deletion of Pod openshift-monitoring/alertmanager-main-0 Nov 19 23:24:36.488 I ns/openshift-marketplace replicaset/certified-operators-74db69bc55 Created pod: certified-operators-74db69bc55-vjs88 Nov 19 23:24:36.547 I ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Successfully assigned openshift-marketplace/certified-operators-74db69bc55-vjs88 to ip-10-0-134-44.us-west-2.compute.internal Nov 19 23:24:36.547 I ns/openshift-ingress replicaset/router-default-756b45d66d Created pod: router-default-756b45d66d-dtcjj Nov 19 23:24:36.547 I ns/openshift-ingress pod/router-default-756b45d66d-dtcjj Successfully assigned openshift-ingress/router-default-756b45d66d-dtcjj to ip-10-0-134-44.us-west-2.compute.internal Nov 19 23:24:36.547 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container rules-configmap-reloader Nov 19 23:24:36.547 I ns/openshift-monitoring statefulset/alertmanager-main create Pod alertmanager-main-0 in StatefulSet alertmanager-main successful Nov 19 23:24:36.548 I ns/openshift-monitoring pod/alertmanager-main-0 node/ created Nov 19 23:24:36.561 I ns/openshift-monitoring pod/alertmanager-main-0 Successfully assigned openshift-monitoring/alertmanager-main-0 to ip-10-0-134-44.us-west-2.compute.internal Nov 19 23:24:36.561 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container kube-rbac-proxy Nov 19 23:24:36.561 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus-proxy Nov 19 23:24:36.561 I ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh Stopping container certified-operators Nov 19 23:24:36.566 I ns/openshift-ingress pod/router-default-756b45d66d-nphxt Stopping container router Nov 19 23:24:36.571 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager Nov 19 23:24:36.575 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy Nov 19 23:24:36.610 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader Nov 19 23:24:36.808 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager (2 times) Nov 19 23:24:37.007 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy (2 times) Nov 19 23:24:37.207 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader (2 times) Nov 19 23:24:38.381 W ns/openshift-ingress pod/router-default-756b45d66d-nphxt node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:24:38.381 W ns/openshift-ingress pod/router-default-756b45d66d-nphxt node/ip-10-0-132-239.us-west-2.compute.internal container=router container stopped being ready Nov 19 23:24:38.732 W ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:24:38.732 W ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh node/ip-10-0-132-239.us-west-2.compute.internal container=certified-operators container stopped being ready Nov 19 23:24:39.131 E ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh node/ip-10-0-132-239.us-west-2.compute.internal container=certified-operators container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prom-label-proxy container stopped being ready Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-config-reloader container stopped being ready Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=kube-rbac-proxy container stopped being ready Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=rules-configmap-reloader container stopped being ready Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container stopped being ready Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-proxy container stopped being ready Nov 19 23:24:39.931 W ns/openshift-image-registry pod/node-ca-w7mtc node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:24:39.931 W ns/openshift-image-registry pod/node-ca-w7mtc node/ip-10-0-132-239.us-west-2.compute.internal container=node-ca container stopped being ready Nov 19 23:24:40.134 W ns/openshift-image-registry pod/node-ca-w7mtc node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:24:40.155 I ns/openshift-image-registry pod/node-ca-q8n5q node/ created Nov 19 23:24:40.158 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-q8n5q Nov 19 23:24:40.165 I ns/openshift-image-registry pod/node-ca-q8n5q Successfully assigned openshift-image-registry/node-ca-q8n5q to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:24:40.345 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (8 times) Nov 19 23:24:40.493 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (8 times) Nov 19 23:24:40.622 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (8 times) Nov 19 23:24:41.133 W ns/openshift-ingress pod/router-default-756b45d66d-nphxt node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:24:41.655 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (5 times) Nov 19 23:24:42.109 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-1 in StatefulSet prometheus-k8s successful Nov 19 23:24:42.109 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:24:42.110 I ns/openshift-monitoring pod/prometheus-k8s-1 node/ created Nov 19 23:24:42.119 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully assigned openshift-monitoring/prometheus-k8s-1 to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:24:42.587 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (7 times) Nov 19 23:24:43.084 W ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh node/ip-10-0-132-239.us-west-2.compute.internal pod has been pending longer than a minute Nov 19 23:24:43.604 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (5 times) Nov 19 23:24:45.138 I ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:69e43ce2394b34a984aca4c30461087dcb4cef9eb6c9a61467ac192d11782ca6\\\" Nov 19 23:24:45.245 I ns/openshift-ingress pod/router-default-756b45d66d-dtcjj Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" Nov 19 23:24:45.425 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b9d07e8eed67a34b50a3afaa36cd4b9f052697894e64b5c203d0675d12ab5ec\\\" already present on machine Nov 19 23:24:45.576 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager Nov 19 23:24:45.601 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager Nov 19 23:24:45.603 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Nov 19 23:24:45.800 I ns/openshift-monitoring pod/alertmanager-main-0 Created container config-reloader Nov 19 23:24:45.830 I ns/openshift-monitoring pod/alertmanager-main-0 Started container config-reloader Nov 19 23:24:45.832 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Nov 19 23:24:45.975 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager-proxy Nov 19 23:24:46.001 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager-proxy Nov 19 23:24:47.136 W ns/openshift-machine-config-operator pod/machine-config-daemon-lsnww node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:24:47.215 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-vjpg6 Nov 19 23:24:47.215 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 Successfully assigned openshift-machine-config-operator/machine-config-daemon-vjpg6 to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:24:47.215 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 node/ created Nov 19 23:24:47.933 W ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:24:48.425 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 19 23:24:48.545 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 Created container machine-config-daemon Nov 19 23:24:48.567 I ns/openshift-image-registry pod/node-ca-q8n5q Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 19 23:24:48.578 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 Started container machine-config-daemon Nov 19 23:24:48.732 I ns/openshift-image-registry pod/node-ca-q8n5q Created container node-ca Nov 19 23:24:48.763 I ns/openshift-image-registry pod/node-ca-q8n5q Started container node-ca Nov 19 23:24:50.428 I ns/openshift-ingress pod/router-default-756b45d66d-dtcjj Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" Nov 19 23:24:50.596 I ns/openshift-ingress pod/router-default-756b45d66d-dtcjj Created container router Nov 19 23:24:50.626 I ns/openshift-ingress pod/router-default-756b45d66d-dtcjj Started container router Nov 19 23:24:52.082 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine Nov 19 23:24:52.238 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus Nov 19 23:24:52.263 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus Nov 19 23:24:52.268 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\\\" already present on machine Nov 19 23:24:52.450 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-config-reloader Nov 19 23:24:52.473 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-config-reloader Nov 19 23:24:52.475 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Nov 19 23:24:52.622 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-proxy Nov 19 23:24:52.648 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-proxy Nov 19 23:24:52.651 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Nov 19 23:24:52.804 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container kube-rbac-proxy Nov 19 23:24:52.832 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container kube-rbac-proxy Nov 19 23:24:52.835 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\\\" already present on machine Nov 19 23:24:52.966 I ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:69e43ce2394b34a984aca4c30461087dcb4cef9eb6c9a61467ac192d11782ca6\\\" Nov 19 23:24:52.982 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prom-label-proxy Nov 19 23:24:53.083 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prom-label-proxy Nov 19 23:24:53.102 I ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Created container certified-operators Nov 19 23:24:53.127 I ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Started container certified-operators Nov 19 23:24:53.284 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Nov 19 23:24:53.433 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container exited with code 1 (Error): Nov 19 23:24:53.484 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container rules-configmap-reloader Nov 19 23:24:53.693 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container rules-configmap-reloader Nov 19 23:24:53.884 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine (2 times) Nov 19 23:24:54.085 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus (2 times) Nov 19 23:24:54.284 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus (2 times) Nov 19 23:24:54.733 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container restarted Nov 19 23:25:01.657 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Readiness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ Nov 19 23:25:03.735 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Liveness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ Nov 19 23:25:11.656 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Readiness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ (2 times) Nov 19 23:25:13.732 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Liveness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ (2 times) Nov 19 23:25:21.655 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Readiness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ (3 times) Nov 19 23:25:23.733 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Liveness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ (3 times) Nov 19 23:25:31.662 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Readiness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ (4 times) Nov 19 23:25:33.744 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Liveness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ (4 times) Nov 19 23:34:40.339 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (9 times) Nov 19 23:34:41.327 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (6 times) Nov 19 23:34:42.405 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (8 times) Nov 19 23:34:43.546 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (6 times) Nov 19 23:34:43.704 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (9 times) Nov 19 23:34:43.845 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (9 times) Nov 19 23:34:58.648 W ns/openshift-image-registry pod/node-ca-q8n5q node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:34:58.723 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-vjpg6 Nov 19 23:34:58.723 I ns/openshift-image-registry pod/node-ca-q8n5q Stopping container node-ca Nov 19 23:34:58.723 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-q8n5q Nov 19 23:34:58.723 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 Stopping container machine-config-daemon Nov 19 23:34:58.724 W ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:34:58.731 I ns/openshift-monitoring pod/prometheus-k8s-1 Marking for deletion Pod openshift-monitoring/prometheus-k8s-1 (2 times) Nov 19 23:34:58.731 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-vjpg6 Nov 19 23:34:58.731 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus Nov 19 23:34:58.731 I ns/openshift-image-registry pod/node-ca-q8n5q Marking for deletion Pod openshift-image-registry/node-ca-q8n5q Nov 19 23:34:58.731 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container rules-configmap-reloader Nov 19 23:34:58.732 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:34:58.806 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prom-label-proxy Nov 19 23:34:58.806 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container kube-rbac-proxy Nov 19 23:34:58.806 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus-proxy Nov 19 23:34:58.806 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus-config-reloader Nov 19 23:34:59.702 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-proxy container exited with code 2 (Error): Nov 19 23:34:59.702 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=rules-configmap-reloader container exited with code 2 (Error): Nov 19 23:34:59.702 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-config-reloader container exited with code 2 (Error): Nov 19 23:35:02.344 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:35:02.425 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully assigned openshift-monitoring/prometheus-k8s-1 to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:35:02.426 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-1 in StatefulSet prometheus-k8s successful (2 times) Nov 19 23:35:02.426 I ns/openshift-monitoring pod/prometheus-k8s-1 node/ created Nov 19 23:35:05.261 W ns/openshift-image-registry pod/node-ca-q8n5q node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:35:05.340 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-ncvkl Nov 19 23:35:05.340 I ns/openshift-image-registry pod/node-ca-ncvkl Successfully assigned openshift-image-registry/node-ca-ncvkl to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:35:05.341 I ns/openshift-image-registry pod/node-ca-ncvkl node/ created Nov 19 23:35:06.261 W ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:35:06.340 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-dwwsn Nov 19 23:35:06.340 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn Successfully assigned openshift-machine-config-operator/machine-config-daemon-dwwsn to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:35:06.340 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn node/ created Nov 19 23:35:07.624 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 19 23:35:07.686 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn Created container machine-config-daemon Nov 19 23:35:07.711 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn Started container machine-config-daemon Nov 19 23:35:12.443 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine Nov 19 23:35:12.550 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus Nov 19 23:35:12.583 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus Nov 19 23:35:12.586 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\\\" already present on machine Nov 19 23:35:12.745 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-config-reloader Nov 19 23:35:12.770 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-config-reloader Nov 19 23:35:12.773 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Nov 19 23:35:12.919 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-proxy Nov 19 23:35:12.954 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-proxy Nov 19 23:35:12.957 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Nov 19 23:35:13.114 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container kube-rbac-proxy Nov 19 23:35:13.139 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container kube-rbac-proxy Nov 19 23:35:13.142 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\\\" already present on machine Nov 19 23:35:13.290 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prom-label-proxy Nov 19 23:35:13.371 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prom-label-proxy Nov 19 23:35:13.571 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Nov 19 23:35:13.731 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container exited with code 1 (Error): Nov 19 23:35:13.771 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container rules-configmap-reloader Nov 19 23:35:13.977 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container rules-configmap-reloader Nov 19 23:35:14.172 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine (2 times) Nov 19 23:35:14.372 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus (2 times) Nov 19 23:35:14.572 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus (2 times) Nov 19 23:35:14.771 I ns/openshift-image-registry pod/node-ca-ncvkl Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 19 23:35:14.971 I ns/openshift-image-registry pod/node-ca-ncvkl Created container node-ca Nov 19 23:35:15.128 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container restarted Nov 19 23:35:15.171 I ns/openshift-image-registry pod/node-ca-ncvkl Started container node-ca Nov 19 23:38:08.760 W ns/openshift-image-registry pod/node-ca-ncvkl node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:38:08.760 W ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:38:08.768 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-dwwsn Nov 19 23:38:08.769 I ns/openshift-image-registry pod/node-ca-ncvkl Stopping container node-ca Nov 19 23:38:08.773 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn Stopping container machine-config-daemon Nov 19 23:38:08.776 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-ncvkl Nov 19 23:38:08.779 I ns/openshift-image-registry pod/node-ca-ncvkl Marking for deletion Pod openshift-image-registry/node-ca-ncvkl Nov 19 23:38:08.779 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:38:08.783 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-dwwsn Nov 19 23:38:08.833 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus Nov 19 23:38:08.833 I ns/openshift-monitoring pod/prometheus-k8s-1 Marking for deletion Pod openshift-monitoring/prometheus-k8s-1 (3 times) Nov 19 23:38:08.833 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container rules-configmap-reloader Nov 19 23:38:08.833 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prom-label-proxy Nov 19 23:38:08.833 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container kube-rbac-proxy Nov 19 23:38:10.194 W ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:38:10.194 W ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn node/ip-10-0-132-239.us-west-2.compute.internal container=machine-config-daemon container stopped being ready Nov 19 23:38:10.252 W ns/openshift-image-registry pod/node-ca-ncvkl node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:38:10.252 W ns/openshift-image-registry pod/node-ca-ncvkl node/ip-10-0-132-239.us-west-2.compute.internal container=node-ca container stopped being ready Nov 19 23:38:10.254 E ns/openshift-image-registry pod/node-ca-ncvkl node/ip-10-0-132-239.us-west-2.compute.internal container=node-ca container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-config-reloader container stopped being ready Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container stopped being ready Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=rules-configmap-reloader container stopped being ready Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=kube-rbac-proxy container stopped being ready Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prom-label-proxy container stopped being ready Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-proxy container stopped being ready Nov 19 23:38:10.922 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=rules-configmap-reloader container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:10.922 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:10.922 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=kube-rbac-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:10.922 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prom-label-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:10.922 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:10.922 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-config-reloader container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:13.084 W ns/openshift-image-registry pod/node-ca-ncvkl node/ip-10-0-132-239.us-west-2.compute.internal pod has been pending longer than a minute Nov 19 23:38:13.084 W ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn node/ip-10-0-132-239.us-west-2.compute.internal pod has been pending longer than a minute Nov 19 23:38:13.084 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal pod has been pending longer than a minute Nov 19 23:38:15.715 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:38:15.715 W ns/openshift-image-registry pod/node-ca-ncvkl node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:38:15.797 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-1 in StatefulSet prometheus-k8s successful (3 times) Nov 19 23:38:15.797 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully assigned openshift-monitoring/prometheus-k8s-1 to ip-10-0-134-44.us-west-2.compute.internal Nov 19 23:38:15.798 W ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:38:15.798 I ns/openshift-monitoring pod/prometheus-k8s-1 node/ created Nov 19 23:38:16.206 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq node/ created Nov 19 23:38:16.211 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-skgfq Nov 19 23:38:16.215 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq Successfully assigned openshift-machine-config-operator/machine-config-daemon-skgfq to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:38:16.222 I ns/openshift-image-registry pod/node-ca-jmwlw node/ created Nov 19 23:38:16.225 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-jmwlw Nov 19 23:38:16.231 I ns/openshift-image-registry pod/node-ca-jmwlw Successfully assigned openshift-image-registry/node-ca-jmwlw to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:38:17.916 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 19 23:38:18.046 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq Created container machine-config-daemon Nov 19 23:38:18.084 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq Started container machine-config-daemon Nov 19 23:38:24.012 I ns/openshift-monitoring pod/prometheus-k8s-1 Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" Nov 19 23:38:25.295 I ns/openshift-image-registry pod/node-ca-jmwlw Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 19 23:38:25.423 I ns/openshift-image-registry pod/node-ca-jmwlw Created container node-ca Nov 19 23:38:25.448 I ns/openshift-image-registry pod/node-ca-jmwlw Started container node-ca Nov 19 23:38:29.620 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" Nov 19 23:38:29.783 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus Nov 19 23:38:29.820 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus Nov 19 23:38:29.822 I ns/openshift-monitoring pod/prometheus-k8s-1 Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\\\" Nov 19 23:38:34.292 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\\\" Nov 19 23:38:34.440 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-config-reloader Nov 19 23:38:34.464 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-config-reloader Nov 19 23:38:34.467 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Nov 19 23:38:34.646 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-proxy Nov 19 23:38:34.672 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-proxy Nov 19 23:38:34.676 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Nov 19 23:38:34.830 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container kube-rbac-proxy Nov 19 23:38:34.853 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container kube-rbac-proxy Nov 19 23:38:34.856 I ns/openshift-monitoring pod/prometheus-k8s-1 Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\\\" Nov 19 23:38:38.790 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\\\" Nov 19 23:38:38.958 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prom-label-proxy Nov 19 23:38:38.981 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prom-label-proxy Nov 19 23:38:38.984 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Nov 19 23:38:39.141 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container rules-configmap-reloader Nov 19 23:38:39.169 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container rules-configmap-reloader Nov 19 23:38:40.088 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine Nov 19 23:38:40.088 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-134-44.us-west-2.compute.internal container=prometheus container exited with code 1 (Error): Nov 19 23:38:40.178 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus (2 times) Nov 19 23:38:40.204 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus (2 times) Nov 19 23:38:41.626 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-134-44.us-west-2.compute.internal container=prometheus container restarted Nov 19 23:41:05.840 W persistentvolume/pvc-95db1176-2ac0-11eb-b0cc-02f1b89323e7 Error deleting EBS volume \\\"vol-0133054b3faf7a686\\\" since volume is currently attached to \\\"i-03abce89e1e637147\\\" Nov 19 23:42:30.700 W ns/openshift-image-registry pod/node-ca-jmwlw node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:42:30.700 W ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:42:30.710 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq Stopping container machine-config-daemon Nov 19 23:42:30.712 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-jmwlw Nov 19 23:42:30.714 I ns/openshift-image-registry pod/node-ca-jmwlw Marking for deletion Pod openshift-image-registry/node-ca-jmwlw Nov 19 23:42:30.715 I ns/openshift-image-registry pod/node-ca-jmwlw Stopping container node-ca Nov 19 23:42:30.717 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-skgfq Nov 19 23:42:30.717 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-skgfq Nov 19 23:42:35.631 W ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:42:35.712 W ns/openshift-image-registry pod/node-ca-jmwlw node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:43:45.817 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 node/ created Nov 19 23:43:45.821 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-n8nd4 Nov 19 23:43:45.890 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 Successfully assigned openshift-machine-config-operator/machine-config-daemon-n8nd4 to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:43:45.890 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-6tkxb Nov 19 23:43:45.890 I ns/openshift-image-registry pod/node-ca-6tkxb Successfully assigned openshift-image-registry/node-ca-6tkxb to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:43:45.890 I ns/openshift-image-registry pod/node-ca-6tkxb node/ created Nov 19 23:43:46.499 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 19 23:43:46.637 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 Created container machine-config-daemon Nov 19 23:43:46.665 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 Started container machine-config-daemon Nov 19 23:43:54.262 I ns/openshift-image-registry pod/node-ca-6tkxb Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 19 23:43:54.382 I ns/openshift-image-registry pod/node-ca-6tkxb Created container node-ca Nov 19 23:43:54.408 I ns/openshift-image-registry pod/node-ca-6tkxb Started container node-ca Nov 19 23:44:41.153 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (9 times) Nov 19 23:44:42.162 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (7 times) Nov 19 23:44:42.314 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (10 times) Nov 19 23:44:42.454 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (10 times) Nov 19 23:44:42.598 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (10 times) Nov 19 23:44:43.484 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (7 times) Nov 19 23:53:10.940 W ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:53:10.940 W ns/openshift-image-registry pod/node-ca-6tkxb node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:53:11.013 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 Stopping container machine-config-daemon Nov 19 23:53:11.013 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-n8nd4 Nov 19 23:53:11.013 I ns/openshift-image-registry pod/node-ca-6tkxb Marking for deletion Pod openshift-image-registry/node-ca-6tkxb Nov 19 23:53:11.013 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-6tkxb Nov 19 23:53:11.013 I ns/openshift-image-registry pod/node-ca-6tkxb Stopping container node-ca Nov 19 23:53:11.013 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-n8nd4 Nov 19 23:53:15.632 W ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:53:15.711 W ns/openshift-image-registry pod/node-ca-6tkxb node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:54:16.249 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn node/ created Nov 19 23:54:16.251 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-2nwhn Nov 19 23:54:16.251 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn Successfully assigned openshift-machine-config-operator/machine-config-daemon-2nwhn to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:54:16.272 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-rdx58 Nov 19 23:54:16.272 I ns/openshift-image-registry pod/node-ca-rdx58 Successfully assigned openshift-image-registry/node-ca-rdx58 to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:54:16.272 I ns/openshift-image-registry pod/node-ca-rdx58 node/ created Nov 19 23:54:16.883 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 19 23:54:17.003 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn Created container machine-config-daemon Nov 19 23:54:17.029 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn Started container machine-config-daemon Nov 19 23:54:23.970 I ns/openshift-image-registry pod/node-ca-rdx58 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 19 23:54:24.097 I ns/openshift-image-registry pod/node-ca-rdx58 Created container node-ca Nov 19 23:54:24.122 I ns/openshift-image-registry pod/node-ca-rdx58 Started container node-ca Nov 19 23:54:40.345 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (11 times) Nov 19 23:54:41.387 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (8 times) Nov 19 23:54:42.188 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (10 times) Nov 19 23:54:43.143 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (8 times) Nov 19 23:54:43.309 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (11 times) Nov 19 23:54:43.469 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (11 times) Nov 20 00:00:11.459 I ns/kube-system pod/critical-pod node/ created Nov 20 00:00:11.464 W ns/kube-system pod/critical-pod 0/6 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 3 node(s) had taints that the pod didn\\\\'t tolerate. Nov 20 00:00:11.541 W ns/kube-system pod/critical-pod 0/6 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 3 node(s) had taints that the pod didn\\\\'t tolerate. (2 times) Nov 20 00:00:15.638 I ns/kube-system pod/critical-pod Successfully assigned kube-system/critical-pod to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:00:23.496 I ns/kube-system pod/critical-pod Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Nov 20 00:00:23.631 I ns/kube-system pod/critical-pod Created container critical-pod Nov 20 00:00:23.653 I ns/kube-system pod/critical-pod Started container critical-pod Nov 20 00:00:26.057 W ns/kube-system pod/critical-pod node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 0s Nov 20 00:00:26.061 W ns/kube-system pod/critical-pod node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:00:26.065 I ns/kube-system pod/critical-pod Stopping container critical-pod Nov 20 00:04:41.182 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (9 times) Nov 20 00:04:42.012 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (11 times) Nov 20 00:04:42.926 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (9 times) Nov 20 00:04:43.090 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (12 times) Nov 20 00:04:43.227 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (12 times) Nov 20 00:04:43.355 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (12 times) Nov 20 00:09:15.607 - 247s I test=\\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" running Nov 20 00:13:23.214 I test=\\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" failed Nov 20 00:14:40.295 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (13 times) Nov 20 00:14:41.352 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (10 times) Nov 20 00:14:42.589 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (12 times) Nov 20 00:14:43.487 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (10 times) Nov 20 00:14:43.660 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (13 times) Nov 20 00:14:43.787 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (13 times) Nov 20 00:22:14.224 W ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 20 00:22:14.224 W ns/openshift-image-registry pod/node-ca-rdx58 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 20 00:22:14.233 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn Stopping container machine-config-daemon Nov 20 00:22:14.235 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-2nwhn Nov 20 00:22:14.236 I ns/openshift-image-registry pod/node-ca-rdx58 Marking for deletion Pod openshift-image-registry/node-ca-rdx58 Nov 20 00:22:14.236 I ns/openshift-image-registry pod/node-ca-rdx58 Stopping container node-ca Nov 20 00:22:14.252 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-rdx58 Nov 20 00:22:14.252 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-2nwhn Nov 20 00:22:15.687 W ns/openshift-image-registry pod/node-ca-rdx58 node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 20 00:22:15.687 W ns/openshift-image-registry pod/node-ca-rdx58 node/ip-10-0-132-239.us-west-2.compute.internal container=node-ca container stopped being ready Nov 20 00:22:15.695 W ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 20 00:22:15.695 W ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn node/ip-10-0-132-239.us-west-2.compute.internal container=machine-config-daemon container stopped being ready Nov 20 00:22:25.632 W ns/openshift-image-registry pod/node-ca-rdx58 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:22:25.712 W ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:22:41.883 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh node/ created Nov 20 00:22:41.887 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-fl6xh Nov 20 00:22:41.956 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh Successfully assigned openshift-machine-config-operator/machine-config-daemon-fl6xh to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:22:41.956 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-cbcsc Nov 20 00:22:41.956 I ns/openshift-image-registry pod/node-ca-cbcsc Successfully assigned openshift-image-registry/node-ca-cbcsc to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:22:41.956 I ns/openshift-image-registry pod/node-ca-cbcsc node/ created Nov 20 00:22:42.553 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 20 00:22:42.685 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh Created container machine-config-daemon Nov 20 00:22:42.710 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh Started container machine-config-daemon Nov 20 00:22:49.460 I ns/openshift-image-registry pod/node-ca-cbcsc Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 20 00:22:49.597 I ns/openshift-image-registry pod/node-ca-cbcsc Created container node-ca Nov 20 00:22:49.629 I ns/openshift-image-registry pod/node-ca-cbcsc Started container node-ca Nov 20 00:24:40.289 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (14 times) Nov 20 00:24:40.427 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (14 times) Nov 20 00:24:41.377 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (11 times) Nov 20 00:24:42.345 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (13 times) Nov 20 00:24:43.256 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (11 times) Nov 20 00:24:43.414 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (14 times) Nov 20 00:28:27.423 I ns/kube-system pod/pod0-system-node-critical node/ created Nov 20 00:28:27.435 I ns/kube-system pod/pod0-system-node-critical Successfully assigned kube-system/pod0-system-node-critical to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:28:27.507 I ns/kube-system pod/pod1-system-cluster-critical node/ created Nov 20 00:28:27.516 I ns/kube-system pod/pod1-system-cluster-critical Successfully assigned kube-system/pod1-system-cluster-critical to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:28:27.594 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 0s Nov 20 00:28:27.598 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:28:27.686 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 0s Nov 20 00:28:27.689 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:28:37.371 W ns/kube-system pod/pod0-system-node-critical Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_pod0-system-node-critical_kube-system_4ecc5780-2ac7-11eb-b0cc-02f1b89323e7_0(202a5d3a2e97a97c0b20a31661acbc8902e197de448ab8b381953044a6b18748): Multus: Err adding pod to network \\\"openshift-sdn\\\": cannot set \\\"openshift-sdn\\\" ifname to \\\"eth0\\\": no netns: failed to Statfs \\\"/proc/110393/ns/net\\\": no such file or directory Nov 20 00:30:30.532 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \\\"pod1-system-cluster-critical_kube-system(4ed93b88-2ac7-11eb-b0cc-02f1b89323e7)\\\": timeout expired waiting for volumes to attach or mount for pod \\\"kube-system\\\"/\\\"pod1-system-cluster-critical\\\". list of unmounted volumes=[default-token-pvg6z]. list of unattached volumes=[default-token-pvg6z] Nov 20 00:34:41.162 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (12 times) Nov 20 00:34:41.996 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (14 times) Nov 20 00:34:42.953 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (12 times) Nov 20 00:34:43.118 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (15 times) Nov 20 00:34:43.276 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (15 times) Nov 20 00:34:43.437 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (15 times) Nov 20 00:34:54.805 W ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 20 00:34:54.805 W ns/openshift-image-registry pod/node-ca-cbcsc node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 20 00:34:54.813 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh Stopping container machine-config-daemon Nov 20 00:34:54.880 I ns/openshift-image-registry pod/node-ca-cbcsc Stopping container node-ca Nov 20 00:34:54.880 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-fl6xh Nov 20 00:34:54.880 I ns/openshift-image-registry pod/node-ca-cbcsc Marking for deletion Pod openshift-image-registry/node-ca-cbcsc Nov 20 00:34:54.880 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-cbcsc Nov 20 00:34:54.880 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-fl6xh Nov 20 00:35:05.631 W ns/openshift-image-registry pod/node-ca-cbcsc node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:35:05.709 W ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:36:00.068 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g node/ created Nov 20 00:36:00.138 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-dgg4g Nov 20 00:36:00.139 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Successfully assigned openshift-machine-config-operator/machine-config-daemon-dgg4g to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:36:00.139 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-b5q5l Nov 20 00:36:00.139 I ns/openshift-image-registry pod/node-ca-b5q5l node/ created Nov 20 00:36:00.149 I ns/openshift-image-registry pod/node-ca-b5q5l Successfully assigned openshift-image-registry/node-ca-b5q5l to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:36:00.766 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 20 00:36:00.890 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Created container machine-config-daemon Nov 20 00:36:00.917 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Started container machine-config-daemon Nov 20 00:36:08.327 I ns/openshift-image-registry pod/node-ca-b5q5l Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 20 00:36:08.464 I ns/openshift-image-registry pod/node-ca-b5q5l Created container node-ca Nov 20 00:36:08.491 I ns/openshift-image-registry pod/node-ca-b5q5l Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201120-004117.xml error: 1 fail, 39 pass, 39 skip (1h24m49s) 2020/11/20 00:41:18 Container test in pod e2e-aws-serial failed, exit code 1, reason Error 2020/11/20 00:48:50 Copied 127.96MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/11/20 00:48:50 Releasing leases for \\\"e2e-aws-serial\\\" 2020/11/20 00:48:50 Releasing lease \\\"d65cd353-d217-4fbd-ba5b-aa4a522fb068\\\" for \\\"aws-quota-slice\\\" 2020/11/20 00:48:50 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/11/20 00:48:50 Ran for 2h9m24s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-bxig87c1/e2e-aws-serial failed after 2h7m36s (failed containers: test): ContainerFailed one or more containers exited Container test exited with code 1, reason Error --- nternal deleted Nov 20 00:36:00.068 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g node/ created Nov 20 00:36:00.138 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-dgg4g Nov 20 00:36:00.139 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Successfully assigned openshift-machine-config-operator/machine-config-daemon-dgg4g to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:36:00.139 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-b5q5l Nov 20 00:36:00.139 I ns/openshift-image-registry pod/node-ca-b5q5l node/ created Nov 20 00:36:00.149 I ns/openshift-image-registry pod/node-ca-b5q5l Successfully assigned openshift-image-registry/node-ca-b5q5l to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:36:00.766 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 20 00:36:00.890 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Created container machine-config-daemon Nov 20 00:36:00.917 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Started container machine-config-daemon Nov 20 00:36:08.327 I ns/openshift-image-registry pod/node-ca-b5q5l Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 20 00:36:08.464 I ns/openshift-image-registry pod/node-ca-b5q5l Created container node-ca Nov 20 00:36:08.491 I ns/openshift-image-registry pod/node-ca-b5q5l Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201120-004117.xml error: 1 fail, 39 pass, 39 skip (1h24m49s) --- '\", \"cluster_count\": 23}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 18, \"cluster_size\": 7, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 23}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 18, \"cluster_size\": 8, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 23}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 19, \"cluster_size\": 6, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 23}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 24, \"cluster_size\": 1, \"template_mined\": \"b'2020/11/23 22:42:37 ci-operator version v20201123-4f43d72 2020/11/23 22:42:37 No source defined 2020/11/23 22:42:37 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/11/23 22:42:37 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-6yz8hxjr 2020/11/23 22:42:37 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/11/23 22:42:37 Creating namespace ci-op-6yz8hxjr 2020/11/23 22:42:37 Setting up pipeline imagestream for the test 2020/11/23 22:42:37 Created secret e2e-aws-serial-cluster-profile 2020/11/23 22:42:37 Created secret pull-secret 2020/11/23 22:42:37 Created PDB for pods with openshift.io/build.name label 2020/11/23 22:42:37 Created PDB for pods with created-by-ci label 2020/11/23 22:42:37 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:${component} 2020/11/23 22:42:39 Importing release image latest 2020/11/23 22:42:40 Executing pod \\\"release-images-latest-cli\\\" 2020/11/23 22:42:45 Executing pod \\\"release-images-latest\\\" 2020/11/23 22:43:32 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/11/23 22:43:32 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/11/23 22:43:32 Acquiring lease for \\\"aws-quota-slice\\\" 2020/11/23 22:43:32 Acquired lease \\\"b315453f-673a-4c36-a4d3-64dd02315ee8\\\" for \\\"aws-quota-slice\\\" 2020/11/23 22:43:32 Executing template e2e-aws-serial 2020/11/23 22:43:32 Creating or restarting template instance 2020/11/23 22:43:32 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/11/23 22:43:32 Waiting for template instance to be ready 2020/11/23 22:43:34 Running pod e2e-aws-serial 2020/11/23 23:13:34 Container setup in pod e2e-aws-serial completed successfully 2020/11/24 00:36:24 Container test in pod e2e-aws-serial completed successfully 2020/11/24 00:42:54 Container teardown in pod e2e-aws-serial completed successfully 2020/11/24 00:43:02 Copied 109.18MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/11/24 00:43:34 pod didn\\\\'t start running within 15m0s: * Container artifacts is not ready with reason ContainerCreating * Container setup is not ready with reason ContainerCreating * Container teardown is not ready with reason ContainerCreating * Container test is not ready with reason ContainerCreating Found 16 events for Pod e2e-aws-serial: * 1x default-scheduler: Successfully assigned ci-op-6yz8hxjr/e2e-aws-serial to origin-ci-ig-n-1p7q * 1x kubelet: pulling image \\\"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:tests\\\" * 1x kubelet: Successfully pulled image \\\"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:tests\\\" * 1x kubelet: Created container * 1x kubelet: Started container * 1x kubelet: pulling image \\\"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:installer\\\" * 1x kubelet: Successfully pulled image \\\"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:installer\\\" * 1x kubelet: Created container * 1x kubelet: Started container * 1x kubelet: Container image \\\"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:tests\\\" already present on machine * 1x kubelet: Created container * 1x kubelet: Started container * 1x kubelet: pulling image \\\"quay.io/prometheus/busybox:latest\\\" * 1x kubelet: Successfully pulled image \\\"quay.io/prometheus/busybox:latest\\\" * 1x kubelet: Created container * 1x kubelet: Started container 2020/11/24 00:43:34 Releasing leases for \\\"e2e-aws-serial\\\" 2020/11/24 00:43:34 Releasing lease \\\"b315453f-673a-4c36-a4d3-64dd02315ee8\\\" for \\\"aws-quota-slice\\\" 2020/11/24 00:43:34 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/11/24 00:43:34 Ran for 2h0m57s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: pod didn\\\\'t start running within 15m0s: * Container artifacts is not ready with reason ContainerCreating * Container setup is not ready with reason ContainerCreating * Container teardown is not ready with reason ContainerCreating * Container test is not ready with reason ContainerCreating Found 16 events for Pod e2e-aws-serial: * 1x default-scheduler: Successfully assigned ci-op-6yz8hxjr/e2e-aws-serial to origin-ci-ig-n-1p7q * 1x kubelet: pulling image \\\"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:tests\\\" * 1x kubelet: Successfully pulled image \\\"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:tests\\\" * 1x kubelet: Created container * 1x kubelet: Started container * 1x kubelet: pulling image \\\"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:installer\\\" * 1x kubelet: Successfully pulled image \\\"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:installer\\\" * 1x kubelet: Created container * 1x kubelet: Started container * 1x kubelet: Container image \\\"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:tests\\\" already present on machine * 1x kubelet: Created container * 1x kubelet: Started container * 1x kubelet: pulling image \\\"quay.io/prometheus/busybox:latest\\\" * 1x kubelet: Successfully pulled image \\\"quay.io/prometheus/busybox:latest\\\" * 1x kubelet: Created container * 1x kubelet: Started container '\", \"cluster_count\": 24}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 25, \"cluster_size\": 1, \"template_mined\": \"b'2020/11/24 22:42:51 ci-operator version v20201124-1769199 2020/11/24 22:42:51 No source defined 2020/11/24 22:42:51 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/11/24 22:42:51 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-h9bfimnx 2020/11/24 22:42:51 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/11/24 22:42:51 Creating namespace ci-op-h9bfimnx 2020/11/24 22:42:51 Setting up pipeline imagestream for the test 2020/11/24 22:42:51 Created secret e2e-aws-serial-cluster-profile 2020/11/24 22:42:51 Created secret pull-secret 2020/11/24 22:42:51 Created PDB for pods with openshift.io/build.name label 2020/11/24 22:42:51 Created PDB for pods with created-by-ci label 2020/11/24 22:42:51 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-h9bfimnx/stable:${component} 2020/11/24 22:42:54 Importing release image latest 2020/11/24 22:42:54 Executing pod \\\"release-images-latest-cli\\\" 2020/11/24 22:43:04 Executing pod \\\"release-images-latest\\\" 2020/11/24 22:44:31 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/11/24 22:44:31 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/11/24 22:44:31 Acquiring lease for \\\"aws-quota-slice\\\" 2020/11/24 22:44:31 Acquired lease \\\"ac73dfe1-fc96-4796-9058-a5a214204280\\\" for \\\"aws-quota-slice\\\" 2020/11/24 22:44:31 Executing template e2e-aws-serial 2020/11/24 22:44:31 Creating or restarting template instance 2020/11/24 22:44:31 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/11/24 22:44:31 Waiting for template instance to be ready 2020/11/24 22:44:34 Running pod e2e-aws-serial 2020/11/24 23:10:44 Container setup in pod e2e-aws-serial completed successfully 2020/11/25 00:33:54 Container test in pod e2e-aws-serial completed successfully 2020/11/25 00:41:29 Container teardown in pod e2e-aws-serial completed successfully 2020/11/25 00:41:37 Copied 121.66MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/11/25 00:42:14 Container artifacts in pod e2e-aws-serial completed successfully 2020/11/25 00:42:14 Pod e2e-aws-serial succeeded after 1h57m40s 2020/11/25 00:42:14 Releasing leases for \\\"e2e-aws-serial\\\" 2020/11/25 00:42:14 Releasing lease \\\"ac73dfe1-fc96-4796-9058-a5a214204280\\\" for \\\"aws-quota-slice\\\" 2020/11/25 00:42:14 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/11/25 00:42:14 Ran for 1h59m22s '\", \"cluster_count\": 25}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 26, \"cluster_size\": 1, \"template_mined\": \"b'2020/11/25 22:43:55 ci-operator version v20201125-2caa480 2020/11/25 22:43:55 No source defined 2020/11/25 22:43:55 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/11/25 22:43:55 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-yrkswzmk 2020/11/25 22:43:55 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/11/25 22:43:55 Creating namespace ci-op-yrkswzmk 2020/11/25 22:43:55 Setting up pipeline imagestream for the test 2020/11/25 22:43:55 Created secret e2e-aws-serial-cluster-profile 2020/11/25 22:43:55 Created secret pull-secret 2020/11/25 22:43:55 Created PDB for pods with openshift.io/build.name label 2020/11/25 22:43:55 Created PDB for pods with created-by-ci label 2020/11/25 22:43:55 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-yrkswzmk/stable:${component} 2020/11/25 22:43:57 Importing release image latest 2020/11/25 22:43:58 Executing pod \\\"release-images-latest-cli\\\" 2020/11/25 22:44:08 Executing pod \\\"release-images-latest\\\" 2020/11/25 22:45:40 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/11/25 22:45:40 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/11/25 22:45:40 Acquiring lease for \\\"aws-quota-slice\\\" 2020/11/25 22:45:40 Acquired lease \\\"37f911f5-b8d2-4f47-99a0-9a2fde2bc334\\\" for \\\"aws-quota-slice\\\" 2020/11/25 22:45:40 Executing template e2e-aws-serial 2020/11/25 22:45:40 Creating or restarting template instance 2020/11/25 22:45:40 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/11/25 22:45:40 Waiting for template instance to be ready 2020/11/25 22:45:42 Running pod e2e-aws-serial 2020/11/25 23:17:07 Container setup in pod e2e-aws-serial completed successfully secret/support created started: (0/1/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (42.1s) 2020-11-25T23:18:04 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/2/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m39s) 2020-11-25T23:20:43 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/3/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (41.1s) 2020-11-25T23:21:24 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/4/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (26.2s) 2020-11-25T23:21:50 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/5/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-11-25T23:22:31 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/6/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m54s) 2020-11-25T23:25:25 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/7/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (37.4s) 2020-11-25T23:26:02 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/8/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (37.1s) 2020-11-25T23:26:39 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/9/79) \\\"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\\\" skip [github.com/openshift/origin/test/extended/images/signatures.go:25]: disable because containers/image: https://github.com/containers/image/pull/570 skipped: (15.4s) 2020-11-25T23:26:55 \\\"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\\\" started: (0/10/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.1s) 2020-11-25T23:27:19 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/11/79) \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m8s) 2020-11-25T23:28:27 \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/12/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (50.5s) 2020-11-25T23:29:17 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/13/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m26s) 2020-11-25T23:30:43 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/14/79) \\\"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m36s) 2020-11-25T23:32:19 \\\"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/15/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-11-25T23:33:00 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/16/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (23s) 2020-11-25T23:33:23 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/17/79) \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m11s) 2020-11-25T23:34:33 \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/18/79) \\\"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m10s) 2020-11-25T23:35:43 \\\"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/19/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-11-25T23:36:23 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/20/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m44s) 2020-11-25T23:39:07 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/21/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m9s) 2020-11-25T23:41:17 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/22/79) \\\"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m11s) 2020-11-25T23:43:28 \\\"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/23/79) \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m57s) 2020-11-25T23:45:25 \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/24/79) \\\"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (20.8s) 2020-11-25T23:45:46 \\\"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/25/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-25T23:46:27 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/26/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-11-25T23:47:07 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/27/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" Nov 25 23:47:08.277: INFO: >>> kubeConfig: /tmp/admin.kubeconfig Nov 25 23:47:08.280: INFO: Waiting up to 30m0s for all (but 100) nodes to be schedulable Nov 25 23:47:08.795: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace \\\\'kube-system\\\\' to be running and ready Nov 25 23:47:09.068: INFO: 0 / 0 pods in namespace \\\\'kube-system\\\\' are running and ready (0 seconds elapsed) Nov 25 23:47:09.068: INFO: expected 0 pod replicas in namespace \\\\'kube-system\\\\', 0 are Running and Ready. Nov 25 23:47:09.068: INFO: Waiting up to 5m0s for all daemonsets in namespace \\\\'kube-system\\\\' to start Nov 25 23:47:09.158: INFO: e2e test version: v1.13.4-138-g41dc99c Nov 25 23:47:09.242: INFO: kube-apiserver version: v1.13.4-138-g41dc99c [BeforeEach] [Top Level] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/test/extended/util/test.go:69 [BeforeEach] [sig-storage] PersistentVolumes-local /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:153 STEP: Creating a kubernetes client Nov 25 23:47:09.246: INFO: >>> kubeConfig: /tmp/admin.kubeconfig STEP: Building a namespace api object, basename persistent-local-volumes-test Nov 25 23:47:14.482: INFO: About to run a Kube e2e test, ensuring namespace is privileged Nov 25 23:47:15.544: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled. STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-storage] PersistentVolumes-local /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:185 [BeforeEach] Local volume provisioner [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:436 STEP: Bootstrapping local volume provisioner STEP: Binding priviledged Pod Security Policy to the service account local-storage-admin STEP: Initializing local volume discovery base path on node ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:47:20.856: INFO: Running \\\\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c mkdir -p /tmp/e2e-tests-persistent-local-volumes-test-5lh44 -m 777\\\\' Nov 25 23:47:22.075: INFO: stderr: \\\"\\\" Nov 25 23:47:22.075: INFO: stdout: \\\"\\\" STEP: Initializing local volume discovery base path on node ip-10-0-142-135.us-west-2.compute.internal Nov 25 23:47:26.342: INFO: Running \\\\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-142-135.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c mkdir -p /tmp/e2e-tests-persistent-local-volumes-test-5lh44 -m 777\\\\' Nov 25 23:47:27.542: INFO: stderr: \\\"\\\" Nov 25 23:47:27.542: INFO: stdout: \\\"\\\" STEP: Initializing local volume discovery base path on node ip-10-0-156-74.us-west-2.compute.internal Nov 25 23:47:31.808: INFO: Running \\\\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-156-74.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c mkdir -p /tmp/e2e-tests-persistent-local-volumes-test-5lh44 -m 777\\\\' Nov 25 23:47:33.018: INFO: stderr: \\\"\\\" Nov 25 23:47:33.018: INFO: stdout: \\\"\\\" STEP: Creating local directory at path \\\"/tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b\\\" Nov 25 23:47:33.018: INFO: Running \\\\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c mkdir /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b -m 777\\\\' Nov 25 23:47:34.199: INFO: stderr: \\\"\\\" Nov 25 23:47:34.199: INFO: stdout: \\\"\\\" STEP: Mounting local directory at path \\\"/tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b\\\" Nov 25 23:47:34.199: INFO: Running \\\\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c sudo mount --bind /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b\\\\' Nov 25 23:47:35.536: INFO: stderr: \\\"\\\" Nov 25 23:47:35.536: INFO: stdout: \\\"\\\" [It] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:488 STEP: Creating a directory, not bind mounted, in discovery directory Nov 25 23:47:35.536: INFO: Running \\\\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c mkdir -p /tmp/e2e-tests-persistent-local-volumes-test-5lh44/notbindmount -m 777\\\\' Nov 25 23:47:36.694: INFO: stderr: \\\"\\\" Nov 25 23:47:36.694: INFO: stdout: \\\"\\\" STEP: Starting a provisioner daemonset STEP: Allowing provisioner to run for 30s and discover potential local PVs STEP: Examining provisioner logs for not an actual mountpoint message [AfterEach] Local volume provisioner [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:443 STEP: Unmounting the test mount point from \\\"/tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b\\\" Nov 25 23:48:07.156: INFO: Running \\\\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c [ ! -e /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b ] || sudo umount /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b\\\\' Nov 25 23:48:08.413: INFO: stderr: \\\"\\\" Nov 25 23:48:08.413: INFO: stdout: \\\"\\\" STEP: Removing the test mount point Nov 25 23:48:08.413: INFO: Running \\\\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c [ ! -e /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b ] || rm -r /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b\\\\' Nov 25 23:48:09.597: INFO: stderr: \\\"\\\" Nov 25 23:48:09.597: INFO: stdout: \\\"\\\" STEP: Cleaning up persistent volume STEP: Cleaning up cluster role binding STEP: Unbinding priviledged Pod Security Policy to the service account local-storage-admin STEP: Removing the test discovery directory on node ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:48:12.043: INFO: Running \\\\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c [ ! -e /tmp/e2e-tests-persistent-local-volumes-test-5lh44 ] || rm -r /tmp/e2e-tests-persistent-local-volumes-test-5lh44\\\\' Nov 25 23:48:13.243: INFO: stderr: \\\"\\\" Nov 25 23:48:13.243: INFO: stdout: \\\"\\\" STEP: Removing the test discovery directory on node ip-10-0-142-135.us-west-2.compute.internal Nov 25 23:48:13.243: INFO: Running \\\\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-142-135.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c [ ! -e /tmp/e2e-tests-persistent-local-volumes-test-5lh44 ] || rm -r /tmp/e2e-tests-persistent-local-volumes-test-5lh44\\\\' Nov 25 23:48:14.560: INFO: stderr: \\\"\\\" Nov 25 23:48:14.560: INFO: stdout: \\\"\\\" STEP: Removing the test discovery directory on node ip-10-0-156-74.us-west-2.compute.internal Nov 25 23:48:14.560: INFO: Running \\\\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-156-74.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c [ ! -e /tmp/e2e-tests-persistent-local-volumes-test-5lh44 ] || rm -r /tmp/e2e-tests-persistent-local-volumes-test-5lh44\\\\' Nov 25 23:48:15.755: INFO: stderr: \\\"\\\" Nov 25 23:48:15.755: INFO: stdout: \\\"\\\" [AfterEach] [sig-storage] PersistentVolumes-local /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:154 STEP: Collecting events from namespace \\\"e2e-tests-persistent-local-volumes-test-5lh44\\\". STEP: Found 29 events. Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:19 +0000 UTC - event for hostexec-ip-10-0-138-252.us-west-2.compute.internal: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Pulled: Container image \\\"gcr.io/kubernetes-e2e-test-images/hostexec:1.1\\\" already present on machine Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:19 +0000 UTC - event for hostexec-ip-10-0-138-252.us-west-2.compute.internal: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Created: Created container hostexec Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:19 +0000 UTC - event for hostexec-ip-10-0-138-252.us-west-2.compute.internal: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Started: Started container hostexec Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:22 +0000 UTC - event for hostexec-ip-10-0-142-135.us-west-2.compute.internal: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Pulling: Pulling image \\\"gcr.io/kubernetes-e2e-test-images/hostexec:1.1\\\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:24 +0000 UTC - event for hostexec-ip-10-0-142-135.us-west-2.compute.internal: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Pulled: Successfully pulled image \\\"gcr.io/kubernetes-e2e-test-images/hostexec:1.1\\\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:24 +0000 UTC - event for hostexec-ip-10-0-142-135.us-west-2.compute.internal: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Created: Created container hostexec Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:24 +0000 UTC - event for hostexec-ip-10-0-142-135.us-west-2.compute.internal: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Started: Started container hostexec Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:28 +0000 UTC - event for hostexec-ip-10-0-156-74.us-west-2.compute.internal: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Pulling: Pulling image \\\"gcr.io/kubernetes-e2e-test-images/hostexec:1.1\\\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:30 +0000 UTC - event for hostexec-ip-10-0-156-74.us-west-2.compute.internal: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Pulled: Successfully pulled image \\\"gcr.io/kubernetes-e2e-test-images/hostexec:1.1\\\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:30 +0000 UTC - event for hostexec-ip-10-0-156-74.us-west-2.compute.internal: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Created: Created container hostexec Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:30 +0000 UTC - event for hostexec-ip-10-0-156-74.us-west-2.compute.internal: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Started: Started container hostexec Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:36 +0000 UTC - event for local-volume-provisioner: {daemonset-controller } SuccessfulCreate: Created pod: local-volume-provisioner-hksc9 Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:36 +0000 UTC - event for local-volume-provisioner: {daemonset-controller } SuccessfulCreate: Created pod: local-volume-provisioner-rgt7n Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:36 +0000 UTC - event for local-volume-provisioner: {daemonset-controller } SuccessfulCreate: Created pod: local-volume-provisioner-r4j57 Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:36 +0000 UTC - event for local-volume-provisioner-hksc9: {default-scheduler } Scheduled: Successfully assigned e2e-tests-persistent-local-volumes-test-5lh44/local-volume-provisioner-hksc9 to ip-10-0-156-74.us-west-2.compute.internal Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:36 +0000 UTC - event for local-volume-provisioner-r4j57: {default-scheduler } Scheduled: Successfully assigned e2e-tests-persistent-local-volumes-test-5lh44/local-volume-provisioner-r4j57 to ip-10-0-142-135.us-west-2.compute.internal Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:36 +0000 UTC - event for local-volume-provisioner-rgt7n: {default-scheduler } Scheduled: Successfully assigned e2e-tests-persistent-local-volumes-test-5lh44/local-volume-provisioner-rgt7n to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:44 +0000 UTC - event for local-volume-provisioner-hksc9: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Pulling: Pulling image \\\"quay.io/external_storage/local-volume-provisioner:v2.1.0\\\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:44 +0000 UTC - event for local-volume-provisioner-r4j57: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Pulling: Pulling image \\\"quay.io/external_storage/local-volume-provisioner:v2.1.0\\\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:44 +0000 UTC - event for local-volume-provisioner-rgt7n: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Pulling: Pulling image \\\"quay.io/external_storage/local-volume-provisioner:v2.1.0\\\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:55 +0000 UTC - event for local-volume-provisioner-hksc9: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Started: Started container provisioner Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:55 +0000 UTC - event for local-volume-provisioner-hksc9: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Pulled: Successfully pulled image \\\"quay.io/external_storage/local-volume-provisioner:v2.1.0\\\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:55 +0000 UTC - event for local-volume-provisioner-hksc9: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Created: Created container provisioner Nov 25 23:48:16.017: INFO: At 2020-11-25 23:48:00 +0000 UTC - event for local-volume-provisioner-r4j57: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Created: Created container provisioner Nov 25 23:48:16.017: INFO: At 2020-11-25 23:48:00 +0000 UTC - event for local-volume-provisioner-r4j57: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Pulled: Successfully pulled image \\\"quay.io/external_storage/local-volume-provisioner:v2.1.0\\\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:48:00 +0000 UTC - event for local-volume-provisioner-r4j57: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Started: Started container provisioner Nov 25 23:48:16.017: INFO: At 2020-11-25 23:48:15 +0000 UTC - event for local-volume-provisioner-rgt7n: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Pulled: Successfully pulled image \\\"quay.io/external_storage/local-volume-provisioner:v2.1.0\\\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:48:15 +0000 UTC - event for local-volume-provisioner-rgt7n: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Created: Created container provisioner Nov 25 23:48:16.017: INFO: At 2020-11-25 23:48:15 +0000 UTC - event for local-volume-provisioner-rgt7n: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Started: Started container provisioner Nov 25 23:48:16.187: INFO: skipping dumping cluster info - cluster too large Nov 25 23:48:16.188: INFO: Waiting up to 3m0s for all (but 100) nodes to be ready STEP: Destroying namespace \\\"e2e-tests-persistent-local-volumes-test-5lh44\\\" for this suite. Nov 25 23:48:38.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Nov 25 23:48:44.640: INFO: namespace e2e-tests-persistent-local-volumes-test-5lh44 deletion completed in 28.365051104s Nov 25 23:48:44.641: INFO: Running AfterSuite actions on all nodes Nov 25 23:48:44.641: INFO: Running AfterSuite actions on node 1 fail [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:505]: Error getting logs from pod local-volume-provisioner-rgt7n in namespace e2e-tests-persistent-local-volumes-test-5lh44 Expected error: <*errors.StatusError | 0xc002185b90>: { ErrStatus: { TypeMeta: {Kind: \\\"\\\", APIVersion: \\\"\\\"}, ListMeta: {SelfLink: \\\"\\\", ResourceVersion: \\\"\\\", Continue: \\\"\\\"}, Status: \\\"Failure\\\", Message: \\\"the server rejected our request for an unknown reason (get pods local-volume-provisioner-rgt7n)\\\", Reason: \\\"BadRequest\\\", Details: { Name: \\\"local-volume-provisioner-rgt7n\\\", Group: \\\"\\\", Kind: \\\"pods\\\", UID: \\\"\\\", Causes: [ { Type: \\\"UnexpectedServerResponse\\\", Message: \\\"unknown\\\", Field: \\\"\\\", }, ], RetryAfterSeconds: 0, }, Code: 400, }, } the server rejected our request for an unknown reason (get pods local-volume-provisioner-rgt7n) not to have occurred Nov 25 23:47:37.246 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \\\"pod1-system-cluster-critical_kube-system(4f8730dc-2f78-11eb-99ad-02e5c45bf6c5)\\\": timeout expired waiting for volumes to attach or mount for pod \\\"kube-system\\\"/\\\"pod1-system-cluster-critical\\\". list of unmounted volumes=[default-token-kzggn]. list of unattached volumes=[default-token-kzggn] failed: (1m37s) 2020-11-25T23:48:44 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/28/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24s) 2020-11-25T23:49:08 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/29/79) \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (42.1s) 2020-11-25T23:49:50 \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/30/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.7s) 2020-11-25T23:50:11 \\\"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/31/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m18s) 2020-11-25T23:52:28 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/32/79) \\\"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:247]: Could not get controller-manager metrics - skipping skipped: (28.8s) 2020-11-25T23:52:57 \\\"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/33/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.2s) 2020-11-25T23:53:37 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/34/79) \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m50s) 2020-11-25T23:55:27 \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/35/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.5s) 2020-11-25T23:55:52 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/36/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.4s) 2020-11-25T23:56:16 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/37/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (54.3s) 2020-11-25T23:57:10 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/38/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-25T23:57:51 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/39/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.9s) 2020-11-25T23:58:12 \\\"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/40/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.2s) 2020-11-25T23:58:52 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/41/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24s) 2020-11-25T23:59:16 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/42/79) \\\"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m3s) 2020-11-26T00:01:19 \\\"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/43/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-11-26T00:01:59 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/44/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.2s) 2020-11-26T00:02:23 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/45/79) \\\"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (3m11s) 2020-11-26T00:05:34 \\\"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/46/79) \\\"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\\\" passed: (43.9s) 2020-11-26T00:06:18 \\\"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\\\" started: (1/47/79) \\\"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (2m6s) 2020-11-26T00:08:24 \\\"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/48/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m4s) 2020-11-26T00:10:28 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/49/79) \\\"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/scheduling/taint_based_evictions.go:63]: Requires at least 2 nodes (not -1) skipped: (20.9s) 2020-11-26T00:10:49 \\\"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/50/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m6s) 2020-11-26T00:12:56 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/51/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-26T00:13:36 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/52/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (22.9s) 2020-11-26T00:13:59 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/53/79) \\\"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m14s) 2020-11-26T00:15:13 \\\"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/54/79) \\\"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:81]: Environment does not support getting controller-manager metrics - skipping skipped: (21.3s) 2020-11-26T00:15:34 \\\"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/55/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.8s) 2020-11-26T00:16:15 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/56/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m44s) 2020-11-26T00:18:59 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/57/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-11-26T00:19:40 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/58/79) \\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m15s) 2020-11-26T00:20:54 \\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/59/79) \\\"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m26s) 2020-11-26T00:22:20 \\\"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/60/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-11-26T00:23:01 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/61/79) \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (38.2s) 2020-11-26T00:23:39 \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/62/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.9s) 2020-11-26T00:24:00 \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/63/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m23s) 2020-11-26T00:25:23 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/64/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m29s) 2020-11-26T00:27:52 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/65/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.9s) 2020-11-26T00:28:13 \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/66/79) \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m44s) 2020-11-26T00:30:57 \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/67/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-26T00:31:37 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/68/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-26T00:32:18 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/69/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.9s) 2020-11-26T00:32:59 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/70/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-11-26T00:33:39 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/71/79) \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (32.7s) 2020-11-26T00:34:12 \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/72/79) \\\"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/apps/daemon_set.go:378]: Requires at least 2 nodes (not -1) skipped: (21.6s) 2020-11-26T00:34:34 \\\"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/73/79) \\\"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\\\" passed: (1m1s) 2020-11-26T00:35:35 \\\"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\\\" started: (1/74/79) \\\"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:193]: Could not get controller-manager metrics - skipping skipped: (1m4s) 2020-11-26T00:36:38 \\\"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/75/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (49s) 2020-11-26T00:37:27 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/76/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.5s) 2020-11-26T00:37:52 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/77/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-11-26T00:38:32 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/78/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-11-26T00:39:13 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/79/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m40s) 2020-11-26T00:40:53 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" Timeline: Nov 25 23:17:23.031 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-143-191.us-west-2.compute.internal node/ip-10-0-143-191.us-west-2.compute.internal created Nov 25 23:17:25.260 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-143-191.us-west-2.compute.internal Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c18ce2736a4fed8091de16f868bc46e6a9a03b7956c2418bad42fbb041ffbafb\\\" already present on machine Nov 25 23:17:25.427 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-143-191.us-west-2.compute.internal Created container pruner Nov 25 23:17:25.511 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-143-191.us-west-2.compute.internal Started container pruner Nov 25 23:19:14.205 W ns/openshift-machine-config-operator pod/machine-config-daemon-nqlh7 node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 25 23:19:14.207 W ns/openshift-image-registry pod/node-ca-xt8dh node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 25 23:19:14.210 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-nqlh7 Nov 25 23:19:14.277 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-xt8dh Nov 25 23:19:14.277 I ns/openshift-machine-config-operator pod/machine-config-daemon-nqlh7 Stopping container machine-config-daemon Nov 25 23:19:14.277 I ns/openshift-image-registry pod/node-ca-xt8dh Stopping container node-ca Nov 25 23:19:14.277 W ns/openshift-marketplace pod/redhat-operators-7849bb68d6-45gqw node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 25 23:19:14.277 I ns/openshift-machine-config-operator pod/machine-config-daemon-nqlh7 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-nqlh7 Nov 25 23:19:14.277 I ns/openshift-monitoring pod/alertmanager-main-0 Marking for deletion Pod openshift-monitoring/alertmanager-main-0 Nov 25 23:19:14.277 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf Marking for deletion Pod openshift-monitoring/kube-state-metrics-7b4d49f7bd-5h5zf Nov 25 23:19:14.277 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-45gqw Marking for deletion Pod openshift-marketplace/redhat-operators-7849bb68d6-45gqw Nov 25 23:19:14.286 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 25 23:19:14.287 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-45gqw Stopping container redhat-operators Nov 25 23:19:14.287 I ns/openshift-image-registry pod/node-ca-xt8dh Marking for deletion Pod openshift-image-registry/node-ca-xt8dh Nov 25 23:19:14.288 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 0s Nov 25 23:19:14.291 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:19:14.357 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf Stopping container kube-state-metrics Nov 25 23:19:14.357 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf Stopping container kube-rbac-proxy-self Nov 25 23:19:14.358 I ns/openshift-marketplace replicaset/redhat-operators-7849bb68d6 Created pod: redhat-operators-7849bb68d6-7g8f6 Nov 25 23:19:14.358 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf Stopping container kube-rbac-proxy-main Nov 25 23:19:14.358 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Successfully assigned openshift-marketplace/redhat-operators-7849bb68d6-7g8f6 to ip-10-0-156-74.us-west-2.compute.internal Nov 25 23:19:14.358 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 node/ created Nov 25 23:19:14.358 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy Nov 25 23:19:14.358 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader Nov 25 23:19:14.358 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager Nov 25 23:19:14.358 I ns/openshift-monitoring replicaset/kube-state-metrics-7b4d49f7bd Created pod: kube-state-metrics-7b4d49f7bd-tjw9b Nov 25 23:19:14.358 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b node/ created Nov 25 23:19:14.366 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Successfully assigned openshift-monitoring/kube-state-metrics-7b4d49f7bd-tjw9b to ip-10-0-142-135.us-west-2.compute.internal Nov 25 23:19:14.366 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy (2 times) Nov 25 23:19:14.366 I ns/openshift-monitoring statefulset/alertmanager-main create Pod alertmanager-main-0 in StatefulSet alertmanager-main successful Nov 25 23:19:14.366 I ns/openshift-monitoring pod/alertmanager-main-0 Successfully assigned openshift-monitoring/alertmanager-main-0 to ip-10-0-156-74.us-west-2.compute.internal Nov 25 23:19:14.367 I ns/openshift-monitoring pod/alertmanager-main-0 node/ created Nov 25 23:19:14.415 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager (2 times) Nov 25 23:19:14.616 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader (2 times) Nov 25 23:19:15.119 E ns/openshift-marketplace pod/redhat-operators-7849bb68d6-45gqw node/ip-10-0-138-252.us-west-2.compute.internal container=redhat-operators container exited with code 2 (Error): Nov 25 23:19:16.553 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 25 23:19:16.554 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf node/ip-10-0-138-252.us-west-2.compute.internal container=kube-state-metrics container stopped being ready Nov 25 23:19:16.554 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf node/ip-10-0-138-252.us-west-2.compute.internal container=kube-rbac-proxy-main container stopped being ready Nov 25 23:19:16.554 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf node/ip-10-0-138-252.us-west-2.compute.internal container=kube-rbac-proxy-self container stopped being ready Nov 25 23:19:16.758 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:19:17.760 W ns/openshift-image-registry pod/node-ca-xt8dh node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:19:18.363 W ns/openshift-machine-config-operator pod/machine-config-daemon-nqlh7 node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:19:20.516 W ns/openshift-marketplace pod/redhat-operators-7849bb68d6-45gqw node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:19:21.665 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Nov 25 23:19:21.811 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Created container kube-rbac-proxy-main Nov 25 23:19:21.843 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Started container kube-rbac-proxy-main Nov 25 23:19:21.847 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Nov 25 23:19:21.852 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:69e43ce2394b34a984aca4c30461087dcb4cef9eb6c9a61467ac192d11782ca6\\\" already present on machine Nov 25 23:19:22.004 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Created container kube-rbac-proxy-self Nov 25 23:19:22.011 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Created container redhat-operators Nov 25 23:19:22.031 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Started container kube-rbac-proxy-self Nov 25 23:19:22.036 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Started container redhat-operators Nov 25 23:19:22.036 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c12c8ce2ac222f2205c8ab24dffef3ef34e3505c7a52d849b637ce3a7d91a388\\\" Nov 25 23:19:22.987 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b9d07e8eed67a34b50a3afaa36cd4b9f052697894e64b5c203d0675d12ab5ec\\\" already present on machine Nov 25 23:19:23.146 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager Nov 25 23:19:23.171 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager Nov 25 23:19:23.175 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Nov 25 23:19:23.329 I ns/openshift-monitoring pod/alertmanager-main-0 Created container config-reloader Nov 25 23:19:23.351 I ns/openshift-monitoring pod/alertmanager-main-0 Started container config-reloader Nov 25 23:19:23.357 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Nov 25 23:19:23.507 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager-proxy Nov 25 23:19:23.534 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager-proxy Nov 25 23:19:29.190 W ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Liveness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ Nov 25 23:19:29.888 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c12c8ce2ac222f2205c8ab24dffef3ef34e3505c7a52d849b637ce3a7d91a388\\\" Nov 25 23:19:30.034 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Created container kube-state-metrics Nov 25 23:19:30.057 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Started container kube-state-metrics Nov 25 23:19:34.720 W ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Readiness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ Nov 25 23:19:39.189 W ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Liveness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ (2 times) Nov 25 23:19:44.744 W ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Readiness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ (2 times) Nov 25 23:20:30.702 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp node/ created Nov 25 23:20:30.707 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-mfswp Nov 25 23:20:30.716 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp Successfully assigned openshift-machine-config-operator/machine-config-daemon-mfswp to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:20:30.777 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4c7mx Nov 25 23:20:30.777 I ns/openshift-image-registry pod/node-ca-4c7mx Successfully assigned openshift-image-registry/node-ca-4c7mx to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:20:30.778 I ns/openshift-image-registry pod/node-ca-4c7mx node/ created Nov 25 23:20:31.418 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 25 23:20:31.549 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp Created container machine-config-daemon Nov 25 23:20:31.577 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp Started container machine-config-daemon Nov 25 23:20:39.453 I ns/openshift-image-registry pod/node-ca-4c7mx Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 25 23:20:39.578 I ns/openshift-image-registry pod/node-ca-4c7mx Created container node-ca Nov 25 23:20:39.603 I ns/openshift-image-registry pod/node-ca-4c7mx Started container node-ca Nov 25 23:23:41.200 W ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 25 23:23:41.201 W ns/openshift-image-registry pod/node-ca-4c7mx node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 25 23:23:41.271 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4c7mx Nov 25 23:23:41.271 I ns/openshift-image-registry pod/node-ca-4c7mx Marking for deletion Pod openshift-image-registry/node-ca-4c7mx Nov 25 23:23:41.271 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp Stopping container machine-config-daemon Nov 25 23:23:41.271 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-mfswp Nov 25 23:23:41.271 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-mfswp Nov 25 23:23:41.271 I ns/openshift-image-registry pod/node-ca-4c7mx Stopping container node-ca Nov 25 23:23:42.678 W ns/openshift-image-registry pod/node-ca-4c7mx node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 25 23:23:42.678 W ns/openshift-image-registry pod/node-ca-4c7mx node/ip-10-0-138-252.us-west-2.compute.internal container=node-ca container stopped being ready Nov 25 23:23:42.688 W ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 25 23:23:42.688 W ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp node/ip-10-0-138-252.us-west-2.compute.internal container=machine-config-daemon container stopped being ready Nov 25 23:23:50.521 W ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:23:50.596 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-glv4f Nov 25 23:23:50.596 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f Successfully assigned openshift-machine-config-operator/machine-config-daemon-glv4f to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:23:50.596 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f node/ created Nov 25 23:23:50.601 W ns/openshift-image-registry pod/node-ca-4c7mx node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:23:50.610 I ns/openshift-image-registry pod/node-ca-nzvxp node/ created Nov 25 23:23:50.676 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-nzvxp Nov 25 23:23:50.676 I ns/openshift-image-registry pod/node-ca-nzvxp Successfully assigned openshift-image-registry/node-ca-nzvxp to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:23:52.235 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 25 23:23:52.394 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f Created container machine-config-daemon Nov 25 23:23:52.413 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f Started container machine-config-daemon Nov 25 23:24:00.324 I ns/openshift-image-registry pod/node-ca-nzvxp Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 25 23:24:00.449 I ns/openshift-image-registry pod/node-ca-nzvxp Created container node-ca Nov 25 23:24:00.474 I ns/openshift-image-registry pod/node-ca-nzvxp Started container node-ca Nov 25 23:25:14.555 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (5 times) Nov 25 23:25:15.649 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (5 times) Nov 25 23:25:16.530 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (5 times) Nov 25 23:25:16.698 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (8 times) Nov 25 23:25:16.845 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (8 times) Nov 25 23:25:16.989 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (8 times) Nov 25 23:30:27.274 W ns/openshift-image-registry pod/node-ca-nzvxp node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 25 23:30:27.346 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-nzvxp Nov 25 23:30:27.346 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-glv4f Nov 25 23:30:27.346 I ns/openshift-image-registry pod/node-ca-nzvxp Stopping container node-ca Nov 25 23:30:27.346 W ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 25 23:30:27.347 I ns/openshift-image-registry pod/node-ca-nzvxp Marking for deletion Pod openshift-image-registry/node-ca-nzvxp Nov 25 23:30:27.347 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f Stopping container machine-config-daemon Nov 25 23:30:27.347 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-glv4f Nov 25 23:30:40.516 W ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:30:40.525 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 node/ created Nov 25 23:30:40.529 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-m7mr7 Nov 25 23:30:40.532 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 Successfully assigned openshift-machine-config-operator/machine-config-daemon-m7mr7 to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:30:40.593 W ns/openshift-image-registry pod/node-ca-nzvxp node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:30:40.597 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-ppmvr Nov 25 23:30:40.597 I ns/openshift-image-registry pod/node-ca-ppmvr Successfully assigned openshift-image-registry/node-ca-ppmvr to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:30:40.597 I ns/openshift-image-registry pod/node-ca-ppmvr node/ created Nov 25 23:30:42.017 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 25 23:30:42.170 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 Created container machine-config-daemon Nov 25 23:30:42.208 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 Started container machine-config-daemon Nov 25 23:30:49.670 I ns/openshift-image-registry pod/node-ca-ppmvr Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 25 23:30:49.798 I ns/openshift-image-registry pod/node-ca-ppmvr Created container node-ca Nov 25 23:30:49.825 I ns/openshift-image-registry pod/node-ca-ppmvr Started container node-ca Nov 25 23:35:14.643 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (6 times) Nov 25 23:35:15.527 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (6 times) Nov 25 23:35:16.480 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (6 times) Nov 25 23:35:16.658 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (9 times) Nov 25 23:35:16.807 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (9 times) Nov 25 23:35:16.987 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (9 times) Nov 25 23:45:13.532 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (10 times) Nov 25 23:45:13.678 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (10 times) Nov 25 23:45:13.814 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (10 times) Nov 25 23:45:15.214 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (7 times) Nov 25 23:45:16.195 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (7 times) Nov 25 23:45:17.405 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (7 times) Nov 25 23:45:34.135 I ns/kube-system pod/pod0-system-node-critical node/ created Nov 25 23:45:34.143 I ns/kube-system pod/pod0-system-node-critical Successfully assigned kube-system/pod0-system-node-critical to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:45:34.218 I ns/kube-system pod/pod1-system-cluster-critical node/ created Nov 25 23:45:34.226 I ns/kube-system pod/pod1-system-cluster-critical Successfully assigned kube-system/pod1-system-cluster-critical to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:45:34.305 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 0s Nov 25 23:45:34.308 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:45:34.394 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 0s Nov 25 23:45:34.397 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:45:34.569 W ns/kube-system pod/pod0-system-node-critical Failed create pod sandbox: rpc error: code = Unknown desc = error reading container (probably exited) json message: EOF Nov 25 23:47:07.382 - 97s I test=\\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" running Nov 25 23:47:37.246 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \\\"pod1-system-cluster-critical_kube-system(4f8730dc-2f78-11eb-99ad-02e5c45bf6c5)\\\": timeout expired waiting for volumes to attach or mount for pod \\\"kube-system\\\"/\\\"pod1-system-cluster-critical\\\". list of unmounted volumes=[default-token-kzggn]. list of unattached volumes=[default-token-kzggn] Nov 25 23:48:44.653 I test=\\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" failed Nov 25 23:54:48.105 W ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 25 23:54:48.106 W ns/openshift-image-registry pod/node-ca-ppmvr node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 25 23:54:48.113 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-m7mr7 Nov 25 23:54:48.118 I ns/openshift-image-registry pod/node-ca-ppmvr Marking for deletion Pod openshift-image-registry/node-ca-ppmvr Nov 25 23:54:48.119 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-ppmvr Nov 25 23:54:48.122 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-m7mr7 Nov 25 23:54:48.122 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 Stopping container machine-config-daemon Nov 25 23:54:48.126 I ns/openshift-image-registry pod/node-ca-ppmvr Stopping container node-ca Nov 25 23:54:49.649 W ns/openshift-image-registry pod/node-ca-ppmvr node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 25 23:54:49.649 W ns/openshift-image-registry pod/node-ca-ppmvr node/ip-10-0-138-252.us-west-2.compute.internal container=node-ca container stopped being ready Nov 25 23:54:52.179 W ns/openshift-image-registry pod/node-ca-ppmvr node/ip-10-0-138-252.us-west-2.compute.internal pod has been pending longer than a minute Nov 25 23:55:00.593 W ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:55:00.593 W ns/openshift-image-registry pod/node-ca-ppmvr node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:55:13.543 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (11 times) Nov 25 23:55:14.885 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc node/ created Nov 25 23:55:14.892 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-qkdwc Nov 25 23:55:14.957 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc Successfully assigned openshift-machine-config-operator/machine-config-daemon-qkdwc to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:55:14.957 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-jv5vl Nov 25 23:55:14.957 I ns/openshift-image-registry pod/node-ca-jv5vl Successfully assigned openshift-image-registry/node-ca-jv5vl to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:55:14.957 I ns/openshift-image-registry pod/node-ca-jv5vl node/ created Nov 25 23:55:15.111 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (8 times) Nov 25 23:55:15.562 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 25 23:55:15.708 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc Created container machine-config-daemon Nov 25 23:55:15.733 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc Started container machine-config-daemon Nov 25 23:55:15.991 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (8 times) Nov 25 23:55:16.855 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (8 times) Nov 25 23:55:17.008 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (11 times) Nov 25 23:55:17.198 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (11 times) Nov 25 23:55:22.416 I ns/openshift-image-registry pod/node-ca-jv5vl Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 25 23:55:22.538 I ns/openshift-image-registry pod/node-ca-jv5vl Created container node-ca Nov 25 23:55:22.564 I ns/openshift-image-registry pod/node-ca-jv5vl Started container node-ca Nov 26 00:05:13.462 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (12 times) Nov 26 00:05:13.599 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (12 times) Nov 26 00:05:13.749 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (12 times) Nov 26 00:05:14.751 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (9 times) Nov 26 00:05:15.650 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (9 times) Nov 26 00:05:16.510 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (9 times) Nov 26 00:12:11.026 I ns/kube-system pod/critical-pod node/ created Nov 26 00:12:11.031 W ns/kube-system pod/critical-pod 0/6 nodes are available: 3 Insufficient memory, 3 node(s) had taints that the pod didn\\\\'t tolerate. Nov 26 00:12:11.108 W ns/kube-system pod/critical-pod 0/6 nodes are available: 3 Insufficient memory, 3 node(s) had taints that the pod didn\\\\'t tolerate. (2 times) Nov 26 00:12:20.522 I ns/kube-system pod/critical-pod Successfully assigned kube-system/critical-pod to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:12:28.122 I ns/kube-system pod/critical-pod Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Nov 26 00:12:28.256 I ns/kube-system pod/critical-pod Created container critical-pod Nov 26 00:12:28.275 I ns/kube-system pod/critical-pod Started container critical-pod Nov 26 00:12:29.629 W ns/kube-system pod/critical-pod node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 0s Nov 26 00:12:29.633 W ns/kube-system pod/critical-pod node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:12:29.638 I ns/kube-system pod/critical-pod Stopping container critical-pod Nov 26 00:12:29.678 W ns/kube-system pod/critical-pod MountVolume.SetUp failed for volume \\\"default-token-kzggn\\\" : object \\\"kube-system\\\"/\\\"default-token-kzggn\\\" not registered Nov 26 00:12:30.184 W ns/kube-system pod/critical-pod MountVolume.SetUp failed for volume \\\"default-token-kzggn\\\" : object \\\"kube-system\\\"/\\\"default-token-kzggn\\\" not registered (2 times) Nov 26 00:12:31.186 W ns/kube-system pod/critical-pod MountVolume.SetUp failed for volume \\\"default-token-kzggn\\\" : object \\\"kube-system\\\"/\\\"default-token-kzggn\\\" not registered (3 times) Nov 26 00:15:13.486 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (13 times) Nov 26 00:15:13.723 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (13 times) Nov 26 00:15:13.871 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (13 times) Nov 26 00:15:14.899 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (10 times) Nov 26 00:15:15.866 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (10 times) Nov 26 00:15:17.116 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (10 times) Nov 26 00:17:25.745 W ns/openshift-image-registry pod/node-ca-jv5vl node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 26 00:17:25.747 W ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 26 00:17:25.749 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-jv5vl Nov 26 00:17:25.752 I ns/openshift-image-registry pod/node-ca-jv5vl Marking for deletion Pod openshift-image-registry/node-ca-jv5vl Nov 26 00:17:25.754 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-qkdwc Nov 26 00:17:25.815 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-qkdwc Nov 26 00:17:25.815 I ns/openshift-image-registry pod/node-ca-jv5vl Stopping container node-ca Nov 26 00:17:25.815 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc Stopping container machine-config-daemon Nov 26 00:17:27.092 W ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 26 00:17:27.092 W ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc node/ip-10-0-138-252.us-west-2.compute.internal container=machine-config-daemon container stopped being ready Nov 26 00:17:27.102 W ns/openshift-image-registry pod/node-ca-jv5vl node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 26 00:17:27.102 W ns/openshift-image-registry pod/node-ca-jv5vl node/ip-10-0-138-252.us-west-2.compute.internal container=node-ca container stopped being ready Nov 26 00:17:30.516 W ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:17:30.593 W ns/openshift-image-registry pod/node-ca-jv5vl node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:18:31.006 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 node/ created Nov 26 00:18:31.015 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 Successfully assigned openshift-machine-config-operator/machine-config-daemon-nvht9 to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:18:31.015 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-nvht9 Nov 26 00:18:31.023 I ns/openshift-image-registry pod/node-ca-749k6 node/ created Nov 26 00:18:31.076 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-749k6 Nov 26 00:18:31.076 I ns/openshift-image-registry pod/node-ca-749k6 Successfully assigned openshift-image-registry/node-ca-749k6 to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:18:31.682 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 26 00:18:31.798 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 Created container machine-config-daemon Nov 26 00:18:31.824 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 Started container machine-config-daemon Nov 26 00:18:39.118 I ns/openshift-image-registry pod/node-ca-749k6 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 26 00:18:39.190 I ns/openshift-image-registry pod/node-ca-749k6 Created container node-ca Nov 26 00:18:39.219 I ns/openshift-image-registry pod/node-ca-749k6 Started container node-ca Nov 26 00:23:24.250 W ns/openshift-image-registry pod/node-ca-749k6 node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 26 00:23:24.319 W ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 26 00:23:24.319 I ns/openshift-image-registry pod/node-ca-749k6 Marking for deletion Pod openshift-image-registry/node-ca-749k6 Nov 26 00:23:24.319 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-nvht9 Nov 26 00:23:24.319 I ns/openshift-image-registry pod/node-ca-749k6 Stopping container node-ca Nov 26 00:23:24.319 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-nvht9 Nov 26 00:23:24.319 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-749k6 Nov 26 00:23:24.319 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 Stopping container machine-config-daemon Nov 26 00:23:25.754 W ns/openshift-image-registry pod/node-ca-749k6 node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 26 00:23:25.754 W ns/openshift-image-registry pod/node-ca-749k6 node/ip-10-0-138-252.us-west-2.compute.internal container=node-ca container stopped being ready Nov 26 00:23:30.517 W ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:23:30.594 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-rnv4f Nov 26 00:23:30.594 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f Successfully assigned openshift-machine-config-operator/machine-config-daemon-rnv4f to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:23:30.594 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f node/ created Nov 26 00:23:30.598 W ns/openshift-image-registry pod/node-ca-749k6 node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:23:30.607 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4l9f2 Nov 26 00:23:30.608 I ns/openshift-image-registry pod/node-ca-4l9f2 node/ created Nov 26 00:23:30.675 I ns/openshift-image-registry pod/node-ca-4l9f2 Successfully assigned openshift-image-registry/node-ca-4l9f2 to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:23:32.017 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 26 00:23:32.152 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f Created container machine-config-daemon Nov 26 00:23:32.203 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f Started container machine-config-daemon Nov 26 00:23:39.480 I ns/openshift-image-registry pod/node-ca-4l9f2 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 26 00:23:39.585 I ns/openshift-image-registry pod/node-ca-4l9f2 Created container node-ca Nov 26 00:23:39.613 I ns/openshift-image-registry pod/node-ca-4l9f2 Started container node-ca Nov 26 00:25:14.541 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (11 times) Nov 26 00:25:15.614 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (11 times) Nov 26 00:25:15.772 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (14 times) Nov 26 00:25:15.919 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (14 times) Nov 26 00:25:16.065 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (14 times) Nov 26 00:25:17.028 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (11 times) Nov 26 00:29:23.304 W ns/openshift-image-registry pod/node-ca-4l9f2 node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 26 00:29:23.345 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4l9f2 Nov 26 00:29:23.345 W ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 26 00:29:23.346 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-rnv4f Nov 26 00:29:23.346 I ns/openshift-image-registry pod/node-ca-4l9f2 Stopping container node-ca Nov 26 00:29:23.346 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-rnv4f Nov 26 00:29:23.346 I ns/openshift-image-registry pod/node-ca-4l9f2 Marking for deletion Pod openshift-image-registry/node-ca-4l9f2 Nov 26 00:29:23.346 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f Stopping container machine-config-daemon Nov 26 00:29:30.518 W ns/openshift-image-registry pod/node-ca-4l9f2 node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:29:30.599 W ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:30:28.561 I ns/openshift-machine-config-operator pod/machine-config-daemon-zvvmz node/ created Nov 26 00:30:28.572 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-zvvmz Nov 26 00:30:28.573 I ns/openshift-machine-config-operator pod/machine-config-daemon-zvvmz Successfully assigned openshift-machine-config-operator/machine-config-daemon-zvvmz to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:30:28.576 I ns/openshift-image-registry pod/node-ca-9d8ss node/ created Nov 26 00:30:28.580 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-9d8ss Nov 26 00:30:28.633 I ns/openshift-image-registry pod/node-ca-9d8ss Successfully assigned openshift-image-registry/node-ca-9d8ss to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:30:29.235 I ns/openshift-machine-config-operator pod/machine-config-daemon-zvvmz Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Nov 26 00:30:29.363 I ns/openshift-machine-config-operator pod/machine-config-daemon-zvvmz Created container machine-config-daemon Nov 26 00:30:29.389 I ns/openshift-machine-config-operator pod/machine-config-daemon-zvvmz Started container machine-config-daemon Nov 26 00:30:36.859 I ns/openshift-image-registry pod/node-ca-9d8ss Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 26 00:30:36.979 I ns/openshift-image-registry pod/node-ca-9d8ss Created container node-ca Nov 26 00:30:37.002 I ns/openshift-image-registry pod/node-ca-9d8ss Started container node-ca Nov 26 00:35:13.465 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (15 times) Nov 26 00:35:14.452 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (12 times) Nov 26 00:35:15.349 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (12 times) Nov 26 00:35:16.552 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (12 times) Nov 26 00:35:16.714 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (15 times) Nov 26 00:35:16.852 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (15 times) Nov 26 00:36:20.736 W persistentvolume/pvc-5160dc4a-2f7f-11eb-8c06-02e62f39bc9f Error deleting EBS volume \\\"vol-0b1b3084600ba3bff\\\" since volume is currently attached to \\\"i-083ba1ff7d598fddb\\\" Failing tests: [sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201126-004053.xml error: 1 fail, 39 pass, 39 skip (1h23m31s) 2020/11/26 00:40:57 Container test in pod e2e-aws-serial failed, exit code 1, reason Error 2020/11/26 01:03:32 Container teardown in pod e2e-aws-serial completed successfully 2020/11/26 01:03:41 Copied 123.49MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/11/26 01:04:17 Container artifacts in pod e2e-aws-serial completed successfully 2020/11/26 01:04:17 Releasing leases for \\\"e2e-aws-serial\\\" 2020/11/26 01:04:17 Releasing lease \\\"37f911f5-b8d2-4f47-99a0-9a2fde2bc334\\\" for \\\"aws-quota-slice\\\" 2020/11/26 01:04:17 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/11/26 01:04:17 Ran for 2h20m22s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-yrkswzmk/e2e-aws-serial failed after 2h18m34s (failed containers: test): ContainerFailed one or more containers exited Container test exited with code 1, reason Error --- or pod/machine-config-daemon-zvvmz Started container machine-config-daemon Nov 26 00:30:36.859 I ns/openshift-image-registry pod/node-ca-9d8ss Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Nov 26 00:30:36.979 I ns/openshift-image-registry pod/node-ca-9d8ss Created container node-ca Nov 26 00:30:37.002 I ns/openshift-image-registry pod/node-ca-9d8ss Started container node-ca Nov 26 00:35:13.465 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (15 times) Nov 26 00:35:14.452 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (12 times) Nov 26 00:35:15.349 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (12 times) Nov 26 00:35:16.552 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (12 times) Nov 26 00:35:16.714 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (15 times) Nov 26 00:35:16.852 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (15 times) Nov 26 00:36:20.736 W persistentvolume/pvc-5160dc4a-2f7f-11eb-8c06-02e62f39bc9f Error deleting EBS volume \\\"vol-0b1b3084600ba3bff\\\" since volume is currently attached to \\\"i-083ba1ff7d598fddb\\\" Failing tests: [sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201126-004053.xml error: 1 fail, 39 pass, 39 skip (1h23m31s) --- '\", \"cluster_count\": 26}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 25, \"cluster_size\": 2, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Container artifacts in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 26}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 25, \"cluster_size\": 3, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Container artifacts in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 26}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 25, \"cluster_size\": 4, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Container artifacts in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 26}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 25, \"cluster_size\": 5, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Container artifacts in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 26}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 25, \"cluster_size\": 6, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring lease for \\\"aws-quota-slice\\\" <*> <*> Acquired lease <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Container artifacts in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 26}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 27, \"cluster_size\": 1, \"template_mined\": \"b'2020/12/01 22:49:34 ci-operator version v20201201-064c8f5 2020/12/01 22:49:34 No source defined 2020/12/01 22:49:34 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/01 22:49:34 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-lby5f363 2020/12/01 22:49:34 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/01 22:49:34 Creating namespace ci-op-lby5f363 2020/12/01 22:49:34 Setting up pipeline imagestream for the test 2020/12/01 22:49:34 Created secret e2e-aws-serial-cluster-profile 2020/12/01 22:49:34 Created secret pull-secret 2020/12/01 22:49:34 Created PDB for pods with openshift.io/build.name label 2020/12/01 22:49:34 Created PDB for pods with created-by-ci label 2020/12/01 22:49:34 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-lby5f363/stable:${component} 2020/12/01 22:49:36 Importing release image latest 2020/12/01 22:49:37 Executing pod \\\"release-images-latest-cli\\\" 2020/12/01 22:49:42 Executing pod \\\"release-images-latest\\\" 2020/12/01 22:50:29 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/01 22:50:29 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/12/01 22:50:29 Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" 2020/12/01 22:56:03 Acquired lease(s) [aeca4927-4e1d-407d-99f4-a405ff9a723b] for \\\"aws-quota-slice\\\" 2020/12/01 22:56:03 Executing template e2e-aws-serial 2020/12/01 22:56:03 Creating or restarting template instance 2020/12/01 22:56:03 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/01 22:56:03 Waiting for template instance to be ready 2020/12/01 22:56:05 Running pod e2e-aws-serial 2020/12/01 23:27:55 Container setup in pod e2e-aws-serial completed successfully secret/support created started: (0/1/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m45s) 2020-12-01T23:29:45 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/2/79) \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (51.6s) 2020-12-01T23:30:37 \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/3/79) \\\"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\\\" skip [github.com/openshift/origin/test/extended/images/signatures.go:25]: disable because containers/image: https://github.com/containers/image/pull/570 skipped: (8.9s) 2020-12-01T23:30:46 \\\"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\\\" started: (0/4/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (35.2s) 2020-12-01T23:31:21 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/5/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.7s) 2020-12-01T23:31:36 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/6/79) \\\"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m3s) 2020-12-01T23:32:39 \\\"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/7/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (36.7s) 2020-12-01T23:33:15 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/8/79) \\\"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:193]: Could not get controller-manager metrics - skipping skipped: (58.5s) 2020-12-01T23:34:14 \\\"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/9/79) \\\"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m45s) 2020-12-01T23:35:59 \\\"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (0/10/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (10.2s) 2020-12-01T23:36:09 \\\"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/11/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.2s) 2020-12-01T23:36:38 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/12/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.1s) 2020-12-01T23:37:07 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/13/79) \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (23s) 2020-12-01T23:37:30 \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/14/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m28s) 2020-12-01T23:39:59 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/15/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (11.3s) 2020-12-01T23:40:10 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/16/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.2s) 2020-12-01T23:40:39 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/17/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (25.1s) 2020-12-01T23:41:04 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/18/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (11.8s) 2020-12-01T23:41:16 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/19/79) \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m3s) 2020-12-01T23:42:19 \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/20/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.6s) 2020-12-01T23:42:31 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/21/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" Dec 1 23:42:32.581: INFO: >>> kubeConfig: /tmp/admin.kubeconfig Dec 1 23:42:32.583: INFO: Waiting up to 30m0s for all (but 100) nodes to be schedulable Dec 1 23:42:32.720: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace \\\\'kube-system\\\\' to be running and ready Dec 1 23:42:32.787: INFO: 0 / 0 pods in namespace \\\\'kube-system\\\\' are running and ready (0 seconds elapsed) Dec 1 23:42:32.787: INFO: expected 0 pod replicas in namespace \\\\'kube-system\\\\', 0 are Running and Ready. Dec 1 23:42:32.787: INFO: Waiting up to 5m0s for all daemonsets in namespace \\\\'kube-system\\\\' to start Dec 1 23:42:32.808: INFO: e2e test version: v1.13.4-138-g41dc99c Dec 1 23:42:32.823: INFO: kube-apiserver version: v1.13.4-138-g41dc99c [BeforeEach] [Top Level] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/test/extended/util/test.go:69 [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:153 STEP: Creating a kubernetes client Dec 1 23:42:32.827: INFO: >>> kubeConfig: /tmp/admin.kubeconfig STEP: Building a namespace api object, basename sched-priority Dec 1 23:42:33.840: INFO: About to run a Kube e2e test, ensuring namespace is privileged Dec 1 23:42:34.071: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled. STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:71 Dec 1 23:42:34.087: INFO: Waiting up to 1m0s for all nodes to be ready Dec 1 23:43:34.353: INFO: Waiting for terminating namespaces to be deleted... Dec 1 23:43:34.371: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace \\\\'kube-system\\\\' to be running and ready Dec 1 23:43:34.424: INFO: 0 / 0 pods in namespace \\\\'kube-system\\\\' are running and ready (0 seconds elapsed) Dec 1 23:43:34.424: INFO: expected 0 pod replicas in namespace \\\\'kube-system\\\\', 0 are Running and Ready. [It] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:85 STEP: Trying to launch a pod with a label to get a node which can launch it. STEP: Trying to apply a label on the found node. STEP: verifying the node has the label kubernetes.io/e2e-node-topologyKey topologyvalue Dec 1 23:43:44.553: INFO: ComputeCpuMemFraction for node: ip-10-0-140-191.ec2.internal Dec 1 23:43:44.648: INFO: Pod for on the node: pod-with-label-security-s1, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.648: INFO: Pod for on the node: tuned-7svpm, Cpu: 10, Mem: 20971520 Dec 1 23:43:44.648: INFO: Pod for on the node: dns-default-vlt84, Cpu: 110, Mem: 283115520 Dec 1 23:43:44.648: INFO: Pod for on the node: node-ca-xlfl5, Cpu: 10, Mem: 10485760 Dec 1 23:43:44.648: INFO: Pod for on the node: router-default-79b887b8c9-9xqjr, Cpu: 100, Mem: 268435456 Dec 1 23:43:44.648: INFO: Pod for on the node: machine-config-daemon-q5dkj, Cpu: 20, Mem: 52428800 Dec 1 23:43:44.648: INFO: Pod for on the node: node-exporter-cbwlb, Cpu: 110, Mem: 230686720 Dec 1 23:43:44.648: INFO: Pod for on the node: multus-b6fwc, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.648: INFO: Pod for on the node: olm-operators-vm6pv, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.648: INFO: Pod for on the node: ovs-vvw9b, Cpu: 200, Mem: 419430400 Dec 1 23:43:44.648: INFO: Pod for on the node: sdn-zfr5g, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.648: INFO: Node: ip-10-0-140-191.ec2.internal, totalRequestedCpuResource: 760, cpuAllocatableMil: 3500, cpuFraction: 0.21714285714285714 Dec 1 23:43:44.648: INFO: Node: ip-10-0-140-191.ec2.internal, totalRequestedMemResource: 1600126976, memAllocatableVal: 16181800960, memFraction: 0.09888435656546353 Dec 1 23:43:44.648: INFO: ComputeCpuMemFraction for node: ip-10-0-142-186.ec2.internal Dec 1 23:43:44.699: INFO: Pod for on the node: tuned-99wxh, Cpu: 10, Mem: 20971520 Dec 1 23:43:44.699: INFO: Pod for on the node: dns-default-hnpdr, Cpu: 110, Mem: 283115520 Dec 1 23:43:44.700: INFO: Pod for on the node: image-registry-7dd97dddc5-rmdg7, Cpu: 100, Mem: 268435456 Dec 1 23:43:44.700: INFO: Pod for on the node: node-ca-4gncf, Cpu: 10, Mem: 10485760 Dec 1 23:43:44.700: INFO: Pod for on the node: router-default-79b887b8c9-s7546, Cpu: 100, Mem: 268435456 Dec 1 23:43:44.700: INFO: Pod for on the node: machine-config-daemon-wrg7t, Cpu: 20, Mem: 52428800 Dec 1 23:43:44.700: INFO: Pod for on the node: certified-operators-6d6b4db4dc-b97lm, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: community-operators-b58cc5bd7-rbdvk, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: redhat-operators-6448849fdd-v4kx7, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Dec 1 23:43:44.700: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Dec 1 23:43:44.700: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Dec 1 23:43:44.700: INFO: Pod for on the node: grafana-649f787944-pfq4s, Cpu: 200, Mem: 314572800 Dec 1 23:43:44.700: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-m4jmq, Cpu: 300, Mem: 629145600 Dec 1 23:43:44.700: INFO: Pod for on the node: node-exporter-pgcxm, Cpu: 110, Mem: 230686720 Dec 1 23:43:44.700: INFO: Pod for on the node: prometheus-adapter-5448dfb8fb-5bljf, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: prometheus-adapter-5448dfb8fb-kj62f, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Dec 1 23:43:44.700: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Dec 1 23:43:44.700: INFO: Pod for on the node: prometheus-operator-5d4588dd6-r9d9r, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: telemeter-client-78884c9754-rtv2q, Cpu: 210, Mem: 440401920 Dec 1 23:43:44.700: INFO: Pod for on the node: multus-sp57c, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: ovs-prgfz, Cpu: 200, Mem: 419430400 Dec 1 23:43:44.700: INFO: Pod for on the node: sdn-crq7f, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Node: ip-10-0-142-186.ec2.internal, totalRequestedCpuResource: 2170, cpuAllocatableMil: 3500, cpuFraction: 0.62 Dec 1 23:43:44.700: INFO: Node: ip-10-0-142-186.ec2.internal, totalRequestedMemResource: 4510973952, memAllocatableVal: 16181792768, memFraction: 0.2787684910240966 Dec 1 23:43:44.727: INFO: Waiting for running... Dec 1 23:43:54.804: INFO: Waiting for running... STEP: Compute Cpu, Mem Fraction after create balanced pods. Dec 1 23:44:04.855: INFO: ComputeCpuMemFraction for node: ip-10-0-140-191.ec2.internal Dec 1 23:44:04.965: INFO: Pod for on the node: 0cc0d13d-342f-11eb-a9f6-0a58ac106ed2-0, Cpu: 1410, Mem: 8432589619 Dec 1 23:44:04.965: INFO: Pod for on the node: pod-with-label-security-s1, Cpu: 100, Mem: 209715200 Dec 1 23:44:04.965: INFO: Pod for on the node: tuned-7svpm, Cpu: 10, Mem: 20971520 Dec 1 23:44:04.965: INFO: Pod for on the node: dns-default-vlt84, Cpu: 110, Mem: 283115520 Dec 1 23:44:04.965: INFO: Pod for on the node: node-ca-xlfl5, Cpu: 10, Mem: 10485760 Dec 1 23:44:04.965: INFO: Pod for on the node: router-default-79b887b8c9-9xqjr, Cpu: 100, Mem: 268435456 Dec 1 23:44:04.965: INFO: Pod for on the node: machine-config-daemon-q5dkj, Cpu: 20, Mem: 52428800 Dec 1 23:44:04.965: INFO: Pod for on the node: node-exporter-cbwlb, Cpu: 110, Mem: 230686720 Dec 1 23:44:04.965: INFO: Pod for on the node: multus-b6fwc, Cpu: 100, Mem: 209715200 Dec 1 23:44:04.965: INFO: Pod for on the node: olm-operators-vm6pv, Cpu: 100, Mem: 209715200 Dec 1 23:44:04.965: INFO: Pod for on the node: ovs-vvw9b, Cpu: 200, Mem: 419430400 Dec 1 23:44:04.965: INFO: Pod for on the node: sdn-zfr5g, Cpu: 100, Mem: 209715200 Dec 1 23:44:04.965: INFO: Node: ip-10-0-140-191.ec2.internal, totalRequestedCpuResource: 2170, cpuAllocatableMil: 3500, cpuFraction: 0.62 Dec 1 23:44:04.965: INFO: Node: ip-10-0-140-191.ec2.internal, totalRequestedMemResource: 10032716595, memAllocatableVal: 16181800960, memFraction: 0.6199999999876404 STEP: Compute Cpu, Mem Fraction after create balanced pods. Dec 1 23:44:04.965: INFO: ComputeCpuMemFraction for node: ip-10-0-142-186.ec2.internal Dec 1 23:44:05.019: INFO: Pod for on the node: 12c294c9-342f-11eb-a9f6-0a58ac106ed2-0, Cpu: 0, Mem: 5521737564 Dec 1 23:44:05.019: INFO: Pod for on the node: tuned-99wxh, Cpu: 10, Mem: 20971520 Dec 1 23:44:05.019: INFO: Pod for on the node: dns-default-hnpdr, Cpu: 110, Mem: 283115520 Dec 1 23:44:05.019: INFO: Pod for on the node: image-registry-7dd97dddc5-rmdg7, Cpu: 100, Mem: 268435456 Dec 1 23:44:05.019: INFO: Pod for on the node: node-ca-4gncf, Cpu: 10, Mem: 10485760 Dec 1 23:44:05.019: INFO: Pod for on the node: router-default-79b887b8c9-s7546, Cpu: 100, Mem: 268435456 Dec 1 23:44:05.019: INFO: Pod for on the node: machine-config-daemon-wrg7t, Cpu: 20, Mem: 52428800 Dec 1 23:44:05.019: INFO: Pod for on the node: certified-operators-6d6b4db4dc-b97lm, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: community-operators-b58cc5bd7-rbdvk, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: redhat-operators-6448849fdd-v4kx7, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Dec 1 23:44:05.019: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Dec 1 23:44:05.019: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Dec 1 23:44:05.019: INFO: Pod for on the node: grafana-649f787944-pfq4s, Cpu: 200, Mem: 314572800 Dec 1 23:44:05.019: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-m4jmq, Cpu: 300, Mem: 629145600 Dec 1 23:44:05.019: INFO: Pod for on the node: node-exporter-pgcxm, Cpu: 110, Mem: 230686720 Dec 1 23:44:05.019: INFO: Pod for on the node: prometheus-adapter-5448dfb8fb-5bljf, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: prometheus-adapter-5448dfb8fb-kj62f, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Dec 1 23:44:05.019: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Dec 1 23:44:05.019: INFO: Pod for on the node: prometheus-operator-5d4588dd6-r9d9r, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: telemeter-client-78884c9754-rtv2q, Cpu: 210, Mem: 440401920 Dec 1 23:44:05.019: INFO: Pod for on the node: multus-sp57c, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: ovs-prgfz, Cpu: 200, Mem: 419430400 Dec 1 23:44:05.019: INFO: Pod for on the node: sdn-crq7f, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Node: ip-10-0-142-186.ec2.internal, totalRequestedCpuResource: 2170, cpuAllocatableMil: 3500, cpuFraction: 0.62 Dec 1 23:44:05.019: INFO: Node: ip-10-0-142-186.ec2.internal, totalRequestedMemResource: 10032711516, memAllocatableVal: 16181792768, memFraction: 0.6199999999901123 STEP: Trying to launch the pod with podAntiAffinity. STEP: Wait the pod becomes running STEP: Verify the pod was scheduled to the expected node. STEP: removing the label kubernetes.io/e2e-node-topologyKey off the node ip-10-0-140-191.ec2.internal STEP: verifying the node doesn\\\\'t have the label kubernetes.io/e2e-node-topologyKey [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:154 STEP: Collecting events from namespace \\\"e2e-tests-sched-priority-6pg47\\\". STEP: Found 14 events. Dec 1 23:44:15.183: INFO: At 2020-12-01 23:43:34 +0000 UTC - event for pod-with-label-security-s1: {default-scheduler } Scheduled: Successfully assigned e2e-tests-sched-priority-6pg47/pod-with-label-security-s1 to ip-10-0-140-191.ec2.internal Dec 1 23:44:15.183: INFO: At 2020-12-01 23:43:42 +0000 UTC - event for pod-with-label-security-s1: {kubelet ip-10-0-140-191.ec2.internal} Started: Started container pod-with-label-security-s1 Dec 1 23:44:15.183: INFO: At 2020-12-01 23:43:42 +0000 UTC - event for pod-with-label-security-s1: {kubelet ip-10-0-140-191.ec2.internal} Created: Created container pod-with-label-security-s1 Dec 1 23:44:15.183: INFO: At 2020-12-01 23:43:42 +0000 UTC - event for pod-with-label-security-s1: {kubelet ip-10-0-140-191.ec2.internal} Pulled: Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Dec 1 23:44:15.183: INFO: At 2020-12-01 23:43:52 +0000 UTC - event for 0cc0d13d-342f-11eb-a9f6-0a58ac106ed2-0: {kubelet ip-10-0-140-191.ec2.internal} Created: Created container 0cc0d13d-342f-11eb-a9f6-0a58ac106ed2-0 Dec 1 23:44:15.184: INFO: At 2020-12-01 23:43:52 +0000 UTC - event for 0cc0d13d-342f-11eb-a9f6-0a58ac106ed2-0: {kubelet ip-10-0-140-191.ec2.internal} Started: Started container 0cc0d13d-342f-11eb-a9f6-0a58ac106ed2-0 Dec 1 23:44:15.184: INFO: At 2020-12-01 23:43:52 +0000 UTC - event for 0cc0d13d-342f-11eb-a9f6-0a58ac106ed2-0: {kubelet ip-10-0-140-191.ec2.internal} Pulled: Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:02 +0000 UTC - event for 12c294c9-342f-11eb-a9f6-0a58ac106ed2-0: {kubelet ip-10-0-142-186.ec2.internal} Started: Started container 12c294c9-342f-11eb-a9f6-0a58ac106ed2-0 Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:02 +0000 UTC - event for 12c294c9-342f-11eb-a9f6-0a58ac106ed2-0: {kubelet ip-10-0-142-186.ec2.internal} Created: Created container 12c294c9-342f-11eb-a9f6-0a58ac106ed2-0 Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:02 +0000 UTC - event for 12c294c9-342f-11eb-a9f6-0a58ac106ed2-0: {kubelet ip-10-0-142-186.ec2.internal} Pulled: Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:05 +0000 UTC - event for pod-with-pod-antiaffinity: {default-scheduler } Scheduled: Successfully assigned e2e-tests-sched-priority-6pg47/pod-with-pod-antiaffinity to ip-10-0-140-191.ec2.internal Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:12 +0000 UTC - event for pod-with-pod-antiaffinity: {kubelet ip-10-0-140-191.ec2.internal} Pulled: Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:13 +0000 UTC - event for pod-with-pod-antiaffinity: {kubelet ip-10-0-140-191.ec2.internal} Created: Created container pod-with-pod-antiaffinity Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:13 +0000 UTC - event for pod-with-pod-antiaffinity: {kubelet ip-10-0-140-191.ec2.internal} Started: Started container pod-with-pod-antiaffinity Dec 1 23:44:15.216: INFO: skipping dumping cluster info - cluster too large Dec 1 23:44:15.216: INFO: Waiting up to 3m0s for all (but 100) nodes to be ready STEP: Destroying namespace \\\"e2e-tests-sched-priority-6pg47\\\" for this suite. Dec 1 23:44:39.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Dec 1 23:44:40.933: INFO: namespace e2e-tests-sched-priority-6pg47 deletion completed in 25.698566242s [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:68 Dec 1 23:44:40.934: INFO: Running AfterSuite actions on all nodes Dec 1 23:44:40.934: INFO: Running AfterSuite actions on node 1 fail [k8s.io/kubernetes/test/e2e/scheduling/priorities.go:143]: Expected <string>: ip-10-0-140-191.ec2.internal not to equal <string>: ip-10-0-140-191.ec2.internal Dec 01 23:42:52.294 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (79 times) failed: (2m9s) 2020-12-01T23:44:40 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/22/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.1s) 2020-12-01T23:45:10 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/23/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m36s) 2020-12-01T23:46:46 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/24/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29s) 2020-12-01T23:47:15 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/25/79) \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (24.6s) 2020-12-01T23:47:40 \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/26/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (10.1s) 2020-12-01T23:47:50 \\\"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/27/79) \\\"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/apps/daemon_set.go:378]: Requires at least 2 nodes (not -1) skipped: (10.3s) 2020-12-01T23:48:00 \\\"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/28/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.2s) 2020-12-01T23:48:29 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/29/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-12-01T23:48:58 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/30/79) \\\"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m52s) 2020-12-01T23:50:50 \\\"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/31/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m5s) 2020-12-01T23:52:56 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/32/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (10s) 2020-12-01T23:53:06 \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/33/79) \\\"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m13s) 2020-12-01T23:54:19 \\\"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/34/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-12-01T23:54:47 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/35/79) \\\"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m47s) 2020-12-01T23:56:35 \\\"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/36/79) \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m54s) 2020-12-01T23:58:29 \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/37/79) \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (19.6s) 2020-12-01T23:58:48 \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/38/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.6s) 2020-12-01T23:59:01 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/39/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.1s) 2020-12-01T23:59:30 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/40/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m34s) 2020-12-02T00:02:03 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/41/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (11.4s) 2020-12-02T00:02:15 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/42/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (11.4s) 2020-12-02T00:02:26 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/43/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-12-02T00:02:55 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/44/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (11.2s) 2020-12-02T00:03:06 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/45/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.1s) 2020-12-02T00:03:35 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/46/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m13s) 2020-12-02T00:05:48 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/47/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.8s) 2020-12-02T00:06:01 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/48/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m45s) 2020-12-02T00:07:46 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/49/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.3s) 2020-12-02T00:08:15 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/50/79) \\\"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/scheduling/taint_based_evictions.go:63]: Requires at least 2 nodes (not -1) skipped: (9.9s) 2020-12-02T00:08:25 \\\"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/51/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.8s) 2020-12-02T00:08:38 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/52/79) \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m27s) 2020-12-02T00:11:05 \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/53/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m41s) 2020-12-02T00:12:46 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/54/79) \\\"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (2m14s) 2020-12-02T00:15:00 \\\"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/55/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (9.9s) 2020-12-02T00:15:10 \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/56/79) \\\"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:247]: Could not get controller-manager metrics - skipping skipped: (20.3s) 2020-12-02T00:15:30 \\\"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/57/79) \\\"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m45s) 2020-12-02T00:17:14 \\\"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/58/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m14s) 2020-12-02T00:18:28 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/59/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (36.6s) 2020-12-02T00:19:05 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/60/79) \\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (59.2s) 2020-12-02T00:20:04 \\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/61/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (42.6s) 2020-12-02T00:20:47 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/62/79) \\\"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\\\" passed: (55.7s) 2020-12-02T00:21:43 \\\"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\\\" started: (1/63/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (11.1s) 2020-12-02T00:21:54 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/64/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-12-02T00:22:23 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/65/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (21.7s) 2020-12-02T00:22:44 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/66/79) \\\"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (10.1s) 2020-12-02T00:22:54 \\\"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/67/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m11s) 2020-12-02T00:24:05 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/68/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (26.7s) 2020-12-02T00:24:32 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/69/79) \\\"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\\\" passed: (31.8s) 2020-12-02T00:25:04 \\\"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\\\" started: (1/70/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (58.2s) 2020-12-02T00:26:02 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/71/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.6s) 2020-12-02T00:26:14 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/72/79) \\\"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:81]: Environment does not support getting controller-manager metrics - skipping skipped: (10s) 2020-12-02T00:26:24 \\\"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/73/79) \\\"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (59.4s) 2020-12-02T00:27:24 \\\"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/74/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (12.2s) 2020-12-02T00:27:36 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/75/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.5s) 2020-12-02T00:27:49 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/76/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m43s) 2020-12-02T00:30:32 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/77/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.3s) 2020-12-02T00:31:01 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/78/79) \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m47s) 2020-12-02T00:32:48 \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/79/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.5s) 2020-12-02T00:33:01 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" Timeline: Dec 01 23:28:01.238 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-136-103.ec2.internal node/ip-10-0-136-103.ec2.internal created Dec 01 23:28:01.239 I ns/openshift-console pod/console-57f8778695-qgcjq node/ip-10-0-148-134.ec2.internal created Dec 01 23:28:04.988 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-136-103.ec2.internal Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c18ce2736a4fed8091de16f868bc46e6a9a03b7956c2418bad42fbb041ffbafb\\\" already present on machine Dec 01 23:28:05.167 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-136-103.ec2.internal Created container pruner Dec 01 23:28:05.205 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-136-103.ec2.internal Started container pruner Dec 01 23:32:46.624 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (48 times) Dec 01 23:34:01.505 W persistentvolume/pvc-9769da37-342d-11eb-8d36-0afa806ccb57 Error deleting EBS volume \\\"vol-0c8c55087bd2498e5\\\" since volume is currently attached to \\\"i-009bfdb4b8165959d\\\" Dec 01 23:37:21.752 W ns/openshift-machine-config-operator pod/machine-config-daemon-bpw7t node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 01 23:37:21.753 W ns/openshift-image-registry pod/node-ca-k86sx node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.762 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-k86sx Dec 01 23:37:21.763 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf Marking for deletion Pod openshift-marketplace/redhat-operators-6448849fdd-txhwf Dec 01 23:37:21.771 W ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.772 I ns/openshift-machine-config-operator pod/machine-config-daemon-bpw7t Stopping container machine-config-daemon Dec 01 23:37:21.772 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-bpw7t Dec 01 23:37:21.772 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-lnbfq Marking for deletion Pod openshift-monitoring/prometheus-adapter-5448dfb8fb-lnbfq Dec 01 23:37:21.779 W ns/openshift-ingress pod/router-default-79b887b8c9-567nl node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.780 W ns/openshift-monitoring pod/alertmanager-main-1 node/ip-10-0-140-191.ec2.internal graceful deletion within 0s Dec 01 23:37:21.781 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.782 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.785 W ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-lnbfq node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.785 W ns/openshift-monitoring pod/grafana-649f787944-qr6mr node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.793 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 01 23:37:21.795 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq Marking for deletion Pod openshift-image-registry/image-registry-7dd97dddc5-5pgkq Dec 01 23:37:21.795 I ns/openshift-image-registry pod/node-ca-k86sx Stopping container node-ca Dec 01 23:37:21.798 W ns/openshift-monitoring pod/alertmanager-main-1 node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:21.804 I ns/openshift-monitoring pod/alertmanager-main-1 Marking for deletion Pod openshift-monitoring/alertmanager-main-1 Dec 01 23:37:21.818 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq Stopping container registry Dec 01 23:37:21.824 I ns/openshift-ingress pod/router-default-79b887b8c9-567nl Stopping container router Dec 01 23:37:21.831 I ns/openshift-ingress pod/router-default-79b887b8c9-567nl Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-567nl Dec 01 23:37:21.832 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager-proxy Dec 01 23:37:21.840 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container config-reloader Dec 01 23:37:21.847 I ns/openshift-monitoring pod/grafana-649f787944-qr6mr Marking for deletion Pod openshift-monitoring/grafana-649f787944-qr6mr Dec 01 23:37:21.848 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf Stopping container redhat-operators Dec 01 23:37:21.860 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager Dec 01 23:37:21.863 I ns/openshift-machine-config-operator pod/machine-config-daemon-bpw7t Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-bpw7t Dec 01 23:37:21.865 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 Stopping container kube-state-metrics Dec 01 23:37:21.872 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-rmdg7 node/ created Dec 01 23:37:21.874 I ns/openshift-image-registry pod/node-ca-k86sx Marking for deletion Pod openshift-image-registry/node-ca-k86sx Dec 01 23:37:21.874 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 Stopping container kube-rbac-proxy-self Dec 01 23:37:21.880 I ns/openshift-image-registry replicaset/image-registry-7dd97dddc5 Created pod: image-registry-7dd97dddc5-rmdg7 Dec 01 23:37:21.884 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 Marking for deletion Pod openshift-monitoring/kube-state-metrics-7b4d49f7bd-jr977 Dec 01 23:37:21.887 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-rmdg7 Successfully assigned openshift-image-registry/image-registry-7dd97dddc5-rmdg7 to ip-10-0-142-186.ec2.internal Dec 01 23:37:21.890 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 node/ created Dec 01 23:37:21.892 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-kj62f node/ created Dec 01 23:37:21.903 I ns/openshift-monitoring pod/prometheus-k8s-0 Marking for deletion Pod openshift-monitoring/prometheus-k8s-0 Dec 01 23:37:21.904 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 node/ created Dec 01 23:37:21.908 I ns/openshift-marketplace replicaset/redhat-operators-6448849fdd Created pod: redhat-operators-6448849fdd-v4kx7 Dec 01 23:37:21.918 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. Dec 01 23:37:21.924 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-kj62f Successfully assigned openshift-monitoring/prometheus-adapter-5448dfb8fb-kj62f to ip-10-0-142-186.ec2.internal Dec 01 23:37:21.932 I ns/openshift-monitoring replicaset/prometheus-adapter-5448dfb8fb Created pod: prometheus-adapter-5448dfb8fb-kj62f Dec 01 23:37:21.932 I ns/openshift-monitoring pod/alertmanager-main-1 Cancelling deletion of Pod openshift-monitoring/alertmanager-main-1 Dec 01 23:37:21.938 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 Successfully assigned openshift-marketplace/redhat-operators-6448849fdd-v4kx7 to ip-10-0-142-186.ec2.internal Dec 01 23:37:21.944 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-62gn2 Dec 01 23:37:21.949 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq node/ created Dec 01 23:37:21.960 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (2 times) Dec 01 23:37:21.972 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 Stopping container kube-rbac-proxy-main Dec 01 23:37:21.975 I ns/openshift-monitoring replicaset/kube-state-metrics-7b4d49f7bd Created pod: kube-state-metrics-7b4d49f7bd-m4jmq Dec 01 23:37:21.985 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (3 times) Dec 01 23:37:22.005 I ns/openshift-monitoring pod/alertmanager-main-1 node/ created Dec 01 23:37:22.013 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (4 times) Dec 01 23:37:22.016 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s node/ created Dec 01 23:37:22.022 I ns/openshift-monitoring statefulset/alertmanager-main create Pod alertmanager-main-1 in StatefulSet alertmanager-main successful Dec 01 23:37:22.022 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Successfully assigned openshift-monitoring/kube-state-metrics-7b4d49f7bd-m4jmq to ip-10-0-142-186.ec2.internal Dec 01 23:37:22.031 I ns/openshift-monitoring replicaset/grafana-649f787944 Created pod: grafana-649f787944-pfq4s Dec 01 23:37:22.038 I ns/openshift-monitoring pod/alertmanager-main-1 Successfully assigned openshift-monitoring/alertmanager-main-1 to ip-10-0-142-186.ec2.internal Dec 01 23:37:22.049 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Successfully assigned openshift-monitoring/grafana-649f787944-pfq4s to ip-10-0-142-186.ec2.internal Dec 01 23:37:22.083 W clusteroperator/image-registry changed Available to False: NoReplicasAvailable: The deployment does not have available replicas Dec 01 23:37:22.083 W clusteroperator/image-registry changed Progressing to True: DeploymentNotCompleted: The deployment has not completed Dec 01 23:37:22.168 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-lnbfq Stopping container prometheus-adapter Dec 01 23:37:22.194 W clusteroperator/image-registry changed Available to True: Ready: The registry is ready Dec 01 23:37:22.194 W clusteroperator/image-registry changed Progressing to False: Ready: The registry is ready Dec 01 23:37:22.205 W clusteroperator/image-registry changed Available to False: NoReplicasAvailable: The deployment does not have available replicas Dec 01 23:37:22.205 W clusteroperator/image-registry changed Progressing to True: DeploymentNotCompleted: The deployment has not completed Dec 01 23:37:22.374 I ns/openshift-monitoring pod/grafana-649f787944-qr6mr Stopping container grafana Dec 01 23:37:22.455 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 0/5 nodes are available: 2 node(s) didn\\\\'t match pod affinity/anti-affinity, 2 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 3 node(s) didn\\\\'t match node selector. Dec 01 23:37:22.568 I ns/openshift-monitoring pod/grafana-649f787944-qr6mr Stopping container grafana-proxy Dec 01 23:37:22.773 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container prometheus Dec 01 23:37:22.968 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container rules-configmap-reloader Dec 01 23:37:23.169 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container prom-label-proxy Dec 01 23:37:23.368 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager (2 times) Dec 01 23:37:23.568 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager-proxy (2 times) Dec 01 23:37:23.771 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container config-reloader (2 times) Dec 01 23:37:23.969 W ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:23.969 W ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq node/ip-10-0-140-191.ec2.internal container=registry container stopped being ready Dec 01 23:37:24.371 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:24.371 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf node/ip-10-0-140-191.ec2.internal container=redhat-operators container stopped being ready Dec 01 23:37:25.169 W ns/openshift-ingress pod/router-default-79b887b8c9-567nl node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:25.169 W ns/openshift-ingress pod/router-default-79b887b8c9-567nl node/ip-10-0-140-191.ec2.internal container=router container stopped being ready Dec 01 23:37:25.571 E ns/openshift-ingress pod/router-default-79b887b8c9-567nl node/ip-10-0-140-191.ec2.internal container=router container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:37:25.776 W ns/openshift-ingress pod/router-default-79b887b8c9-567nl node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:25.786 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 Successfully assigned openshift-ingress/router-default-79b887b8c9-62gn2 to ip-10-0-140-191.ec2.internal Dec 01 23:37:26.169 W ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-lnbfq node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:26.169 W ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-lnbfq node/ip-10-0-140-191.ec2.internal container=prometheus-adapter container stopped being ready Dec 01 23:37:26.975 W ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-lnbfq node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:27.976 W ns/openshift-image-registry pod/node-ca-k86sx node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:28.015 I ns/openshift-image-registry pod/node-ca-qpb24 node/ created Dec 01 23:37:28.021 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-qpb24 Dec 01 23:37:28.027 I ns/openshift-image-registry pod/node-ca-qpb24 Successfully assigned openshift-image-registry/node-ca-qpb24 to ip-10-0-140-191.ec2.internal Dec 01 23:37:28.969 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:28.969 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 node/ip-10-0-140-191.ec2.internal container=kube-state-metrics container stopped being ready Dec 01 23:37:28.969 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 node/ip-10-0-140-191.ec2.internal container=kube-rbac-proxy-self container stopped being ready Dec 01 23:37:28.969 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 node/ip-10-0-140-191.ec2.internal container=kube-rbac-proxy-main container stopped being ready Dec 01 23:37:29.378 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prom-label-proxy container stopped being ready Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus container stopped being ready Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus-config-reloader container stopped being ready Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=rules-configmap-reloader container stopped being ready Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=kube-rbac-proxy container stopped being ready Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus-proxy container stopped being ready Dec 01 23:37:29.976 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:30.057 I ns/openshift-monitoring pod/prometheus-k8s-0 node/ created Dec 01 23:37:30.070 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-0 in StatefulSet prometheus-k8s successful Dec 01 23:37:30.073 I ns/openshift-monitoring pod/prometheus-k8s-0 Successfully assigned openshift-monitoring/prometheus-k8s-0 to ip-10-0-140-191.ec2.internal Dec 01 23:37:31.046 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf node/ip-10-0-140-191.ec2.internal pod has been pending longer than a minute Dec 01 23:37:31.046 W ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq node/ip-10-0-140-191.ec2.internal pod has been pending longer than a minute Dec 01 23:37:32.571 W ns/openshift-monitoring pod/grafana-649f787944-qr6mr node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:32.571 W ns/openshift-monitoring pod/grafana-649f787944-qr6mr node/ip-10-0-140-191.ec2.internal container=grafana container stopped being ready Dec 01 23:37:32.571 W ns/openshift-monitoring pod/grafana-649f787944-qr6mr node/ip-10-0-140-191.ec2.internal container=grafana-proxy container stopped being ready Dec 01 23:37:32.986 W ns/openshift-monitoring pod/grafana-649f787944-qr6mr node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:33.976 W ns/openshift-machine-config-operator pod/machine-config-daemon-bpw7t node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:33.990 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh node/ created Dec 01 23:37:33.998 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-hs2xh Dec 01 23:37:34.001 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh Successfully assigned openshift-machine-config-operator/machine-config-daemon-hs2xh to ip-10-0-140-191.ec2.internal Dec 01 23:37:34.890 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" already present on machine Dec 01 23:37:35.373 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:35.748 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 Created container router Dec 01 23:37:35.772 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 Started container router Dec 01 23:37:36.122 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 01 23:37:36.268 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh Created container machine-config-daemon Dec 01 23:37:36.293 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh Started container machine-config-daemon Dec 01 23:37:36.309 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-rmdg7 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 01 23:37:36.377 W ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:36.465 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-rmdg7 Created container registry Dec 01 23:37:36.491 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-rmdg7 Started container registry Dec 01 23:37:36.577 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn (8 times) Dec 01 23:37:36.731 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:69e43ce2394b34a984aca4c30461087dcb4cef9eb6c9a61467ac192d11782ca6\\\" already present on machine Dec 01 23:37:36.743 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm (8 times) Dec 01 23:37:36.825 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Dec 01 23:37:36.934 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-kj62f Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\\\" already present on machine Dec 01 23:37:37.060 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 Created container redhat-operators Dec 01 23:37:37.109 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 Started container redhat-operators Dec 01 23:37:37.174 I ns/openshift-image-registry pod/node-ca-qpb24 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 01 23:37:37.196 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Created container kube-rbac-proxy-main Dec 01 23:37:37.207 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-kj62f Created container prometheus-adapter Dec 01 23:37:37.264 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Started container kube-rbac-proxy-main Dec 01 23:37:37.270 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Dec 01 23:37:37.309 I ns/openshift-image-registry pod/node-ca-qpb24 Created container node-ca Dec 01 23:37:37.336 I ns/openshift-image-registry pod/node-ca-qpb24 Started container node-ca Dec 01 23:37:37.355 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-kj62f Started container prometheus-adapter Dec 01 23:37:37.391 I ns/openshift-monitoring pod/alertmanager-main-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b9d07e8eed67a34b50a3afaa36cd4b9f052697894e64b5c203d0675d12ab5ec\\\" already present on machine Dec 01 23:37:37.602 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Created container kube-rbac-proxy-self Dec 01 23:37:37.614 I ns/openshift-monitoring pod/alertmanager-main-1 Created container alertmanager Dec 01 23:37:37.709 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Started container kube-rbac-proxy-self Dec 01 23:37:37.909 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c12c8ce2ac222f2205c8ab24dffef3ef34e3505c7a52d849b637ce3a7d91a388\\\" Dec 01 23:37:37.919 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-0 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-0 (5 times) Dec 01 23:37:38.109 I ns/openshift-monitoring pod/alertmanager-main-1 Started container alertmanager Dec 01 23:37:38.309 I ns/openshift-monitoring pod/alertmanager-main-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Dec 01 23:37:38.514 I ns/openshift-monitoring pod/alertmanager-main-1 Created container config-reloader Dec 01 23:37:38.709 I ns/openshift-monitoring pod/alertmanager-main-1 Started container config-reloader Dec 01 23:37:38.909 I ns/openshift-monitoring pod/alertmanager-main-1 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Dec 01 23:37:38.919 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-1 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-1 (5 times) Dec 01 23:37:39.109 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c229ac6674c217359c243e4621aae565c2af46caaf231a077b7f7e3d6f07ef5a\\\" Dec 01 23:37:39.134 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (63 times) Dec 01 23:37:39.311 I ns/openshift-monitoring pod/alertmanager-main-1 Created container alertmanager-proxy Dec 01 23:37:39.509 I ns/openshift-monitoring pod/alertmanager-main-1 Started container alertmanager-proxy Dec 01 23:37:39.719 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c12c8ce2ac222f2205c8ab24dffef3ef34e3505c7a52d849b637ce3a7d91a388\\\" Dec 01 23:37:39.910 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Created container kube-state-metrics Dec 01 23:37:39.982 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine Dec 01 23:37:40.070 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-2 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-2 (5 times) Dec 01 23:37:40.108 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Started container kube-state-metrics Dec 01 23:37:40.137 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus Dec 01 23:37:40.168 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus Dec 01 23:37:40.175 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\\\" already present on machine Dec 01 23:37:40.336 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus-config-reloader Dec 01 23:37:40.360 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus-config-reloader Dec 01 23:37:40.366 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Dec 01 23:37:40.513 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus-proxy Dec 01 23:37:40.545 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus-proxy Dec 01 23:37:40.551 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Dec 01 23:37:40.757 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container kube-rbac-proxy Dec 01 23:37:40.790 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container kube-rbac-proxy Dec 01 23:37:40.796 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\\\" already present on machine Dec 01 23:37:40.954 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prom-label-proxy Dec 01 23:37:40.981 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prom-label-proxy Dec 01 23:37:41.182 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Dec 01 23:37:41.382 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container rules-configmap-reloader Dec 01 23:37:41.582 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container rules-configmap-reloader Dec 01 23:37:41.976 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine (2 times) Dec 01 23:37:42.157 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus (2 times) Dec 01 23:37:42.183 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus (2 times) Dec 01 23:37:43.890 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c229ac6674c217359c243e4621aae565c2af46caaf231a077b7f7e3d6f07ef5a\\\" Dec 01 23:37:44.114 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Created container grafana Dec 01 23:37:44.151 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Started container grafana Dec 01 23:37:44.157 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Dec 01 23:37:44.354 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Created container grafana-proxy Dec 01 23:37:44.395 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Started container grafana-proxy Dec 01 23:37:45.835 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 Liveness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ Dec 01 23:37:46.000 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus container exited with code 1 (Error): Dec 01 23:37:46.390 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus container restarted Dec 01 23:37:48.623 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 Readiness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ Dec 01 23:37:48.719 W clusteroperator/image-registry changed Available to True: Ready: The registry is ready Dec 01 23:37:48.719 W clusteroperator/image-registry changed Progressing to False: Ready: The registry is ready Dec 01 23:38:33.395 W ns/openshift-image-registry pod/node-ca-qpb24 node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:38:33.395 W ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 01 23:38:33.400 I ns/openshift-image-registry pod/node-ca-qpb24 Stopping container node-ca Dec 01 23:38:33.407 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh Stopping container machine-config-daemon Dec 01 23:38:33.411 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-qpb24 Dec 01 23:38:33.412 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-62gn2 Dec 01 23:38:33.416 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-hs2xh Dec 01 23:38:33.421 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 01 23:38:33.424 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:38:33.429 I ns/openshift-monitoring pod/prometheus-k8s-0 Marking for deletion Pod openshift-monitoring/prometheus-k8s-0 (2 times) Dec 01 23:38:33.434 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-hs2xh Dec 01 23:38:33.441 I ns/openshift-image-registry pod/node-ca-qpb24 Marking for deletion Pod openshift-image-registry/node-ca-qpb24 Dec 01 23:38:33.443 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container prom-label-proxy Dec 01 23:38:33.451 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container prometheus Dec 01 23:38:33.457 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container rules-configmap-reloader Dec 01 23:38:33.467 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container prometheus-proxy Dec 01 23:38:33.473 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ created Dec 01 23:38:33.474 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 Stopping container router Dec 01 23:38:33.484 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-9xqjr Dec 01 23:38:33.491 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. Dec 01 23:38:33.513 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (2 times) Dec 01 23:38:34.882 W ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:38:34.882 W ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh node/ip-10-0-140-191.ec2.internal container=machine-config-daemon container stopped being ready Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus container stopped being ready Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus-config-reloader container stopped being ready Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus-proxy container stopped being ready Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=rules-configmap-reloader container stopped being ready Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=kube-rbac-proxy container stopped being ready Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prom-label-proxy container stopped being ready Dec 01 23:38:35.095 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:38:35.095 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:38:35.095 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=rules-configmap-reloader container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:38:35.095 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus-config-reloader container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:38:35.095 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=kube-rbac-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:38:35.095 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prom-label-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:38:35.493 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:38:35.493 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 node/ip-10-0-140-191.ec2.internal container=router container stopped being ready Dec 01 23:38:35.899 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:38:35.910 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (3 times) Dec 01 23:38:36.292 W ns/openshift-image-registry pod/node-ca-qpb24 node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:38:36.292 W ns/openshift-image-registry pod/node-ca-qpb24 node/ip-10-0-140-191.ec2.internal container=node-ca container stopped being ready Dec 01 23:38:36.496 W ns/openshift-image-registry pod/node-ca-qpb24 node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:38:36.508 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (4 times) Dec 01 23:38:41.267 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:38:41.269 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (5 times) Dec 01 23:38:41.279 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (6 times) Dec 01 23:38:41.284 W ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:38:41.293 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (7 times) Dec 01 23:38:41.355 I ns/openshift-monitoring pod/prometheus-k8s-0 node/ created Dec 01 23:38:41.367 I ns/openshift-monitoring pod/prometheus-k8s-0 Successfully assigned openshift-monitoring/prometheus-k8s-0 to ip-10-0-142-186.ec2.internal Dec 01 23:38:41.368 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-0 in StatefulSet prometheus-k8s successful (2 times) Dec 01 23:38:49.733 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine Dec 01 23:38:49.910 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus Dec 01 23:38:49.937 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus Dec 01 23:38:49.944 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\\\" already present on machine Dec 01 23:38:50.133 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus-config-reloader Dec 01 23:38:50.168 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus-config-reloader Dec 01 23:38:50.174 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Dec 01 23:38:50.471 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus-proxy Dec 01 23:38:50.508 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus-proxy Dec 01 23:38:50.513 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Dec 01 23:38:50.698 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container kube-rbac-proxy Dec 01 23:38:50.725 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container kube-rbac-proxy Dec 01 23:38:50.732 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\\\" already present on machine Dec 01 23:38:50.907 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prom-label-proxy Dec 01 23:38:50.934 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prom-label-proxy Dec 01 23:38:50.940 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Dec 01 23:38:51.133 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container rules-configmap-reloader Dec 01 23:38:51.333 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container rules-configmap-reloader Dec 01 23:38:51.834 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\\\" already present on machine (2 times) Dec 01 23:38:51.835 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-142-186.ec2.internal container=prometheus container exited with code 1 (Error): Dec 01 23:38:52.002 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus (2 times) Dec 01 23:38:52.041 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus (2 times) Dec 01 23:38:53.843 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-142-186.ec2.internal container=prometheus container restarted Dec 01 23:39:46.046 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ pod has been pending longer than a minute Dec 01 23:39:51.261 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (8 times) Dec 01 23:39:51.304 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr Successfully assigned openshift-ingress/router-default-79b887b8c9-9xqjr to ip-10-0-140-191.ec2.internal Dec 01 23:39:51.319 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj node/ created Dec 01 23:39:51.327 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-q5dkj Dec 01 23:39:51.329 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj Successfully assigned openshift-machine-config-operator/machine-config-daemon-q5dkj to ip-10-0-140-191.ec2.internal Dec 01 23:39:51.339 I ns/openshift-image-registry pod/node-ca-xlfl5 node/ created Dec 01 23:39:51.344 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-xlfl5 Dec 01 23:39:51.350 I ns/openshift-image-registry pod/node-ca-xlfl5 Successfully assigned openshift-image-registry/node-ca-xlfl5 to ip-10-0-140-191.ec2.internal Dec 01 23:39:53.354 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 01 23:39:53.474 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj Created container machine-config-daemon Dec 01 23:39:53.510 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj Started container machine-config-daemon Dec 01 23:40:00.175 I ns/openshift-image-registry pod/node-ca-xlfl5 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 01 23:40:00.302 I ns/openshift-image-registry pod/node-ca-xlfl5 Created container node-ca Dec 01 23:40:00.331 I ns/openshift-image-registry pod/node-ca-xlfl5 Started container node-ca Dec 01 23:40:00.343 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" already present on machine Dec 01 23:40:00.505 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr Created container router Dec 01 23:40:00.530 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr Started container router Dec 01 23:40:01.046 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ip-10-0-140-191.ec2.internal pod has been pending longer than a minute Dec 01 23:42:31.730 - 129s I test=\\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" running Dec 01 23:42:52.294 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (79 times) Dec 01 23:44:40.950 I test=\\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" failed Dec 01 23:47:35.078 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (93 times) Dec 01 23:47:37.655 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-2 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-2 (6 times) Dec 01 23:47:37.812 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn (9 times) Dec 01 23:47:37.993 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm (9 times) Dec 01 23:47:39.291 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-0 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-0 (6 times) Dec 01 23:47:40.591 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-1 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-1 (6 times) Dec 01 23:52:38.436 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (109 times) Dec 01 23:57:37.750 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-1 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-1 (7 times) Dec 01 23:57:37.956 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (124 times) Dec 01 23:57:39.198 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-2 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-2 (7 times) Dec 01 23:57:39.447 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn (10 times) Dec 01 23:57:39.687 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm (10 times) Dec 01 23:57:41.247 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-0 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-0 (7 times) Dec 02 00:00:33.074 W ns/openshift-image-registry pod/node-ca-xlfl5 node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:00:33.075 W ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 02 00:00:33.080 I ns/openshift-image-registry pod/node-ca-xlfl5 Stopping container node-ca Dec 02 00:00:33.086 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-xlfl5 Dec 02 00:00:33.088 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-q5dkj Dec 02 00:00:33.089 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj Stopping container machine-config-daemon Dec 02 00:00:33.093 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:00:33.096 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-q5dkj Dec 02 00:00:33.096 I ns/openshift-image-registry pod/node-ca-xlfl5 Marking for deletion Pod openshift-image-registry/node-ca-xlfl5 Dec 02 00:00:33.104 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr Stopping container router Dec 02 00:00:33.107 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-9xqjr Dec 02 00:00:33.136 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x node/ created Dec 02 00:00:33.156 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-8j89x Dec 02 00:00:33.156 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. Dec 02 00:00:33.196 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (2 times) Dec 02 00:00:34.656 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 02 00:00:34.656 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ip-10-0-140-191.ec2.internal container=router container stopped being ready Dec 02 00:00:34.667 W ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 02 00:00:34.667 W ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj node/ip-10-0-140-191.ec2.internal container=machine-config-daemon container stopped being ready Dec 02 00:00:34.681 W ns/openshift-image-registry pod/node-ca-xlfl5 node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 02 00:00:34.681 W ns/openshift-image-registry pod/node-ca-xlfl5 node/ip-10-0-140-191.ec2.internal container=node-ca container stopped being ready Dec 02 00:00:35.670 W ns/openshift-image-registry pod/node-ca-xlfl5 node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:00:35.682 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (3 times) Dec 02 00:00:35.695 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (4 times) Dec 02 00:00:35.705 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:00:35.715 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (5 times) Dec 02 00:00:41.251 W ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:00:41.265 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (6 times) Dec 02 00:00:41.273 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (7 times) Dec 02 00:01:38.129 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x Successfully assigned openshift-ingress/router-default-79b887b8c9-8j89x to ip-10-0-140-191.ec2.internal Dec 02 00:01:38.133 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq node/ created Dec 02 00:01:38.143 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq Successfully assigned openshift-machine-config-operator/machine-config-daemon-64zgq to ip-10-0-140-191.ec2.internal Dec 02 00:01:38.143 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-64zgq Dec 02 00:01:38.159 I ns/openshift-image-registry pod/node-ca-gk2lg node/ created Dec 02 00:01:38.169 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-gk2lg Dec 02 00:01:38.172 I ns/openshift-image-registry pod/node-ca-gk2lg Successfully assigned openshift-image-registry/node-ca-gk2lg to ip-10-0-140-191.ec2.internal Dec 02 00:01:39.494 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 02 00:01:39.636 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq Created container machine-config-daemon Dec 02 00:01:39.652 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq Started container machine-config-daemon Dec 02 00:01:46.046 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x node/ip-10-0-140-191.ec2.internal pod has been pending longer than a minute Dec 02 00:01:46.485 I ns/openshift-image-registry pod/node-ca-gk2lg Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 02 00:01:46.598 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" already present on machine Dec 02 00:01:46.628 I ns/openshift-image-registry pod/node-ca-gk2lg Created container node-ca Dec 02 00:01:46.663 I ns/openshift-image-registry pod/node-ca-gk2lg Started container node-ca Dec 02 00:01:46.781 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x Created container router Dec 02 00:01:46.809 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x Started container router Dec 02 00:02:45.194 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (140 times) Dec 02 00:07:37.820 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-2 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-2 (8 times) Dec 02 00:07:37.994 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn (11 times) Dec 02 00:07:38.145 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm (11 times) Dec 02 00:07:39.307 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-0 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-0 (8 times) Dec 02 00:07:40.526 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-1 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-1 (8 times) Dec 02 00:07:40.670 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (155 times) Dec 02 00:09:40.824 W ns/openshift-image-registry pod/node-ca-gk2lg node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:09:40.824 W ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 02 00:09:40.828 I ns/openshift-image-registry pod/node-ca-gk2lg Stopping container node-ca Dec 02 00:09:40.833 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-gk2lg Dec 02 00:09:40.836 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq Stopping container machine-config-daemon Dec 02 00:09:40.836 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-8j89x Dec 02 00:09:40.841 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:09:40.841 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-64zgq Dec 02 00:09:40.848 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x Stopping container router Dec 02 00:09:40.851 I ns/openshift-image-registry pod/node-ca-gk2lg Marking for deletion Pod openshift-image-registry/node-ca-gk2lg Dec 02 00:09:40.856 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-64zgq Dec 02 00:09:40.903 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn node/ created Dec 02 00:09:40.911 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-4ghjn Dec 02 00:09:40.920 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. Dec 02 00:09:40.941 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (2 times) Dec 02 00:09:41.555 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (3 times) Dec 02 00:09:42.345 E ns/openshift-ingress pod/router-default-79b887b8c9-8j89x node/ip-10-0-140-191.ec2.internal container=router container exited with code 2 (Error): I1202 00:01:46.839651 1 template.go:299] Starting template router (v4.1.46-202005230309)\\\\ I1202 00:01:46.842084 1 metrics.go:147] Router health and metrics port listening at 0.0.0.0:1936 on HTTP and HTTPS\\\\ E1202 00:01:46.849070 1 haproxy.go:392] can\\\\'t scrape HAProxy: dial unix /var/lib/haproxy/run/haproxy.sock: connect: no such file or directory\\\\ I1202 00:01:46.873381 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:01:46.873408 1 router.go:255] Router is including routes in all namespaces\\\\ I1202 00:01:47.112459 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:01:57.637461 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:02:02.634040 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ W1202 00:09:14.955570 1 reflector.go:341] github.com/openshift/router/pkg/router/controller/factory/factory.go:112: watch of *v1.Route ended with: The resourceVersion for the provided watch is too old.\\\\ Dec 02 00:09:43.748 W ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:09:43.757 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (4 times) Dec 02 00:09:44.353 W ns/openshift-image-registry pod/node-ca-gk2lg node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:09:44.364 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (5 times) Dec 02 00:09:44.958 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:09:44.960 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (6 times) Dec 02 00:10:45.879 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn Successfully assigned openshift-ingress/router-default-79b887b8c9-4ghjn to ip-10-0-140-191.ec2.internal Dec 02 00:10:45.888 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw node/ created Dec 02 00:10:45.897 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-ghvjw Dec 02 00:10:45.902 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw Successfully assigned openshift-machine-config-operator/machine-config-daemon-ghvjw to ip-10-0-140-191.ec2.internal Dec 02 00:10:45.914 I ns/openshift-image-registry pod/node-ca-86jzh node/ created Dec 02 00:10:45.922 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-86jzh Dec 02 00:10:45.925 I ns/openshift-image-registry pod/node-ca-86jzh Successfully assigned openshift-image-registry/node-ca-86jzh to ip-10-0-140-191.ec2.internal Dec 02 00:10:46.046 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn node/ip-10-0-140-191.ec2.internal pod has been pending longer than a minute Dec 02 00:10:47.029 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 02 00:10:47.182 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw Created container machine-config-daemon Dec 02 00:10:47.215 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw Started container machine-config-daemon Dec 02 00:10:54.260 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" already present on machine Dec 02 00:10:54.426 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn Created container router Dec 02 00:10:54.456 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn Started container router Dec 02 00:10:54.927 I ns/openshift-image-registry pod/node-ca-86jzh Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 02 00:10:55.136 I ns/openshift-image-registry pod/node-ca-86jzh Created container node-ca Dec 02 00:10:55.168 I ns/openshift-image-registry pod/node-ca-86jzh Started container node-ca Dec 02 00:12:18.351 I ns/kube-system pod/critical-pod node/ created Dec 02 00:12:18.359 W ns/kube-system pod/critical-pod 0/5 nodes are available: 1 Insufficient cpu, 2 Insufficient memory, 3 node(s) had taints that the pod didn\\\\'t tolerate. Dec 02 00:12:18.384 W ns/kube-system pod/critical-pod 0/5 nodes are available: 1 Insufficient cpu, 2 Insufficient memory, 3 node(s) had taints that the pod didn\\\\'t tolerate. (2 times) Dec 02 00:12:21.260 I ns/kube-system pod/critical-pod Successfully assigned kube-system/critical-pod to ip-10-0-140-191.ec2.internal Dec 02 00:12:29.408 I ns/kube-system pod/critical-pod Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Dec 02 00:12:29.564 I ns/kube-system pod/critical-pod Created container critical-pod Dec 02 00:12:29.587 I ns/kube-system pod/critical-pod Started container critical-pod Dec 02 00:12:30.463 W ns/kube-system pod/critical-pod node/ip-10-0-140-191.ec2.internal graceful deletion within 0s Dec 02 00:12:30.472 W ns/kube-system pod/critical-pod node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:12:30.481 I ns/kube-system pod/critical-pod Stopping container critical-pod Dec 02 00:12:30.976 I ns/kube-system pod/critical-pod Pod sandbox changed, it will be killed and re-created. Dec 02 00:12:40.662 W ns/kube-system pod/critical-pod Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_critical-pod_kube-system_0a28c3c8-3433-11eb-8d36-0afa806ccb57_1(fb139e2781023c573ee27f21aabb092c36cbace677387e64dc57426d89baf264): Multus: Err adding pod to network \\\"openshift-sdn\\\": cannot set \\\"openshift-sdn\\\" ifname to \\\"eth0\\\": no netns: failed to Statfs \\\"/proc/97569/ns/net\\\": no such file or directory Dec 02 00:12:51.675 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (171 times) Dec 02 00:17:34.686 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (185 times) Dec 02 00:17:37.752 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-2 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-2 (9 times) Dec 02 00:17:37.930 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn (12 times) Dec 02 00:17:38.083 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm (12 times) Dec 02 00:17:39.254 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-0 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-0 (9 times) Dec 02 00:17:40.311 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-1 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-1 (9 times) Dec 02 00:18:17.550 W ns/openshift-image-registry pod/node-ca-86jzh node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:18:17.550 W ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 02 00:18:17.557 I ns/openshift-image-registry pod/node-ca-86jzh Stopping container node-ca Dec 02 00:18:17.559 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-ghvjw Dec 02 00:18:17.568 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-86jzh Dec 02 00:18:17.569 I ns/openshift-image-registry pod/node-ca-86jzh Marking for deletion Pod openshift-image-registry/node-ca-86jzh Dec 02 00:18:17.574 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:18:17.575 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-4ghjn Dec 02 00:18:17.583 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn Stopping container router Dec 02 00:18:17.585 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-ghvjw Dec 02 00:18:17.617 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh node/ created Dec 02 00:18:17.623 W ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. Dec 02 00:18:17.625 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-mgmrh Dec 02 00:18:17.635 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw Stopping container machine-config-daemon Dec 02 00:18:17.648 W ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (2 times) Dec 02 00:18:19.439 E ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn node/ip-10-0-140-191.ec2.internal container=router container exited with code 2 (Error): I1202 00:10:54.474576 1 template.go:299] Starting template router (v4.1.46-202005230309)\\\\ I1202 00:10:54.477889 1 metrics.go:147] Router health and metrics port listening at 0.0.0.0:1936 on HTTP and HTTPS\\\\ E1202 00:10:54.485444 1 haproxy.go:392] can\\\\'t scrape HAProxy: dial unix /var/lib/haproxy/run/haproxy.sock: connect: no such file or directory\\\\ I1202 00:10:54.509522 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:10:54.509559 1 router.go:255] Router is including routes in all namespaces\\\\ I1202 00:10:54.746997 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:10:59.745754 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:11:06.198267 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:11:11.175543 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ Dec 02 00:18:21.264 W ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (3 times) Dec 02 00:18:21.295 W ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh 0/5 nodes are available: 2 node(s) didn\\\\'t match pod affinity/anti-affinity, 2 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 3 node(s) didn\\\\'t match node selector. Dec 02 00:18:31.269 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:18:31.277 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh Successfully assigned openshift-ingress/router-default-79b887b8c9-mgmrh to ip-10-0-140-191.ec2.internal Dec 02 00:18:31.288 W ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:18:31.303 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 node/ created Dec 02 00:18:31.312 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-nmb27 Dec 02 00:18:31.313 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 Successfully assigned openshift-machine-config-operator/machine-config-daemon-nmb27 to ip-10-0-140-191.ec2.internal Dec 02 00:18:31.320 W ns/openshift-image-registry pod/node-ca-86jzh node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:18:31.364 I ns/openshift-image-registry pod/node-ca-4hb9w node/ created Dec 02 00:18:31.374 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4hb9w Dec 02 00:18:31.374 I ns/openshift-image-registry pod/node-ca-4hb9w Successfully assigned openshift-image-registry/node-ca-4hb9w to ip-10-0-140-191.ec2.internal Dec 02 00:18:34.017 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 02 00:18:34.189 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 Created container machine-config-daemon Dec 02 00:18:34.237 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 Started container machine-config-daemon Dec 02 00:18:40.481 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" already present on machine Dec 02 00:18:40.663 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh Created container router Dec 02 00:18:40.693 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh Started container router Dec 02 00:18:41.625 I ns/openshift-image-registry pod/node-ca-4hb9w Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 02 00:18:41.787 I ns/openshift-image-registry pod/node-ca-4hb9w Created container node-ca Dec 02 00:18:41.814 I ns/openshift-image-registry pod/node-ca-4hb9w Started container node-ca Dec 02 00:22:38.157 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (201 times) Dec 02 00:22:47.067 I ns/kube-system pod/pod0-system-node-critical node/ created Dec 02 00:22:47.078 I ns/kube-system pod/pod0-system-node-critical Successfully assigned kube-system/pod0-system-node-critical to ip-10-0-140-191.ec2.internal Dec 02 00:22:47.086 I ns/kube-system pod/pod1-system-cluster-critical node/ created Dec 02 00:22:47.096 I ns/kube-system pod/pod1-system-cluster-critical Successfully assigned kube-system/pod1-system-cluster-critical to ip-10-0-140-191.ec2.internal Dec 02 00:22:47.105 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-140-191.ec2.internal graceful deletion within 0s Dec 02 00:22:47.108 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:22:47.126 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-140-191.ec2.internal graceful deletion within 0s Dec 02 00:22:47.130 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:24:50.095 W ns/kube-system pod/pod0-system-node-critical Unable to mount volumes for pod \\\"pod0-system-node-critical_kube-system(80e7893d-3434-11eb-bd30-0afb4b6e93b5)\\\": timeout expired waiting for volumes to attach or mount for pod \\\"kube-system\\\"/\\\"pod0-system-node-critical\\\". list of unmounted volumes=[default-token-hkpkk]. list of unattached volumes=[default-token-hkpkk] Dec 02 00:24:50.122 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \\\"pod1-system-cluster-critical_kube-system(80eabf7a-3434-11eb-bd30-0afb4b6e93b5)\\\": timeout expired waiting for volumes to attach or mount for pod \\\"kube-system\\\"/\\\"pod1-system-cluster-critical\\\". list of unmounted volumes=[default-token-hkpkk]. list of unattached volumes=[default-token-hkpkk] Dec 02 00:27:37.749 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-1 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-1 (10 times) Dec 02 00:27:37.973 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (216 times) Dec 02 00:27:39.308 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-2 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-2 (10 times) Dec 02 00:27:39.475 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn (13 times) Dec 02 00:27:39.635 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm (13 times) Dec 02 00:27:40.728 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-0 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-0 (10 times) Dec 02 00:28:51.696 W ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 02 00:28:51.714 W ns/openshift-image-registry pod/node-ca-4hb9w node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:28:51.721 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 Stopping container machine-config-daemon Dec 02 00:28:51.741 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-mgmrh Dec 02 00:28:51.741 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-nmb27 Dec 02 00:28:51.741 I ns/openshift-image-registry pod/node-ca-4hb9w Stopping container node-ca Dec 02 00:28:51.768 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-nmb27 Dec 02 00:28:51.768 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4hb9w Dec 02 00:28:51.768 W ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:28:51.787 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh Stopping container router Dec 02 00:28:51.792 I ns/openshift-image-registry pod/node-ca-4hb9w Marking for deletion Pod openshift-image-registry/node-ca-4hb9w Dec 02 00:28:51.844 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf node/ created Dec 02 00:28:51.854 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-4njgf Dec 02 00:28:51.858 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf 0/5 nodes are available: 2 node(s) didn\\\\'t match pod affinity/anti-affinity, 2 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 3 node(s) didn\\\\'t match node selector. Dec 02 00:28:51.879 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf 0/5 nodes are available: 2 node(s) didn\\\\'t match pod affinity/anti-affinity, 2 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 3 node(s) didn\\\\'t match node selector. (2 times) Dec 02 00:28:53.247 E ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh node/ip-10-0-140-191.ec2.internal container=router container exited with code 2 (Error): I1202 00:18:40.715027 1 template.go:299] Starting template router (v4.1.46-202005230309)\\\\ I1202 00:18:40.718399 1 metrics.go:147] Router health and metrics port listening at 0.0.0.0:1936 on HTTP and HTTPS\\\\ E1202 00:18:40.725316 1 haproxy.go:392] can\\\\'t scrape HAProxy: dial unix /var/lib/haproxy/run/haproxy.sock: connect: no such file or directory\\\\ I1202 00:18:40.749875 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:18:40.749920 1 router.go:255] Router is including routes in all namespaces\\\\ I1202 00:18:40.988609 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:18:51.118949 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:18:56.115382 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:24:43.997744 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:24:48.987123 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:24:53.995867 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1202 00:24:58.997010 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ Dec 02 00:28:53.856 W ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:28:53.866 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf 0/5 nodes are available: 2 node(s) didn\\\\'t match pod affinity/anti-affinity, 2 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 3 node(s) didn\\\\'t match node selector. (3 times) Dec 02 00:28:53.872 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk node/ created Dec 02 00:28:53.881 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-j7hqk Dec 02 00:28:53.886 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk Successfully assigned openshift-machine-config-operator/machine-config-daemon-j7hqk to ip-10-0-140-191.ec2.internal Dec 02 00:28:54.651 W ns/openshift-image-registry pod/node-ca-4hb9w node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:28:54.661 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf 0/5 nodes are available: 2 node(s) didn\\\\'t match pod affinity/anti-affinity, 2 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 3 node(s) didn\\\\'t match node selector. (4 times) Dec 02 00:28:54.685 I ns/openshift-image-registry pod/node-ca-l2ztl node/ created Dec 02 00:28:54.691 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-l2ztl Dec 02 00:28:54.695 I ns/openshift-image-registry pod/node-ca-l2ztl Successfully assigned openshift-image-registry/node-ca-l2ztl to ip-10-0-140-191.ec2.internal Dec 02 00:28:55.163 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 02 00:28:55.322 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk Created container machine-config-daemon Dec 02 00:28:55.355 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk Started container machine-config-daemon Dec 02 00:28:56.068 W ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:28:56.071 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf Successfully assigned openshift-ingress/router-default-79b887b8c9-4njgf to ip-10-0-140-191.ec2.internal Dec 02 00:29:03.159 I ns/openshift-image-registry pod/node-ca-l2ztl Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 02 00:29:03.333 I ns/openshift-image-registry pod/node-ca-l2ztl Created container node-ca Dec 02 00:29:03.358 I ns/openshift-image-registry pod/node-ca-l2ztl Started container node-ca Dec 02 00:29:05.524 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" already present on machine Dec 02 00:29:05.684 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf Created container router Dec 02 00:29:05.717 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf Started container router Dec 02 00:32:04.465 W ns/openshift-image-registry pod/node-ca-l2ztl node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:32:04.466 W ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 02 00:32:04.474 I ns/openshift-image-registry pod/node-ca-l2ztl Stopping container node-ca Dec 02 00:32:04.479 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-l2ztl Dec 02 00:32:04.481 I ns/openshift-image-registry pod/node-ca-l2ztl Marking for deletion Pod openshift-image-registry/node-ca-l2ztl Dec 02 00:32:04.482 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk Stopping container machine-config-daemon Dec 02 00:32:04.484 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-j7hqk Dec 02 00:32:04.486 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-j7hqk Dec 02 00:32:04.488 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:32:04.495 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-4njgf Dec 02 00:32:04.497 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf Stopping container router Dec 02 00:32:04.538 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd node/ created Dec 02 00:32:04.542 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-qbrcd Dec 02 00:32:04.561 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. Dec 02 00:32:04.583 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (2 times) Dec 02 00:32:06.193 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 02 00:32:06.193 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf node/ip-10-0-140-191.ec2.internal container=router container stopped being ready Dec 02 00:32:06.579 W ns/openshift-image-registry pod/node-ca-l2ztl node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 02 00:32:06.579 W ns/openshift-image-registry pod/node-ca-l2ztl node/ip-10-0-140-191.ec2.internal container=node-ca container stopped being ready Dec 02 00:32:11.254 W ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:32:11.263 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (3 times) Dec 02 00:32:11.284 W ns/openshift-image-registry pod/node-ca-l2ztl node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:32:11.291 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (4 times) Dec 02 00:32:11.312 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:32:11.320 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (5 times) Dec 02 00:32:12.613 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (6 times) Dec 02 00:32:41.267 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\\\'t match pod affinity/anti-affinity, 1 node(s) didn\\\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\\\'t tolerate, 3 node(s) didn\\\\'t match node selector. (7 times) Dec 02 00:32:41.303 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Successfully assigned openshift-ingress/router-default-79b887b8c9-qbrcd to ip-10-0-140-191.ec2.internal Dec 02 00:32:41.311 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 node/ created Dec 02 00:32:41.317 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-jrcl2 Dec 02 00:32:41.321 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Successfully assigned openshift-machine-config-operator/machine-config-daemon-jrcl2 to ip-10-0-140-191.ec2.internal Dec 02 00:32:41.336 I ns/openshift-image-registry pod/node-ca-8rl2j node/ created Dec 02 00:32:41.342 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-8rl2j Dec 02 00:32:41.345 I ns/openshift-image-registry pod/node-ca-8rl2j Successfully assigned openshift-image-registry/node-ca-8rl2j to ip-10-0-140-191.ec2.internal Dec 02 00:32:43.551 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 02 00:32:43.689 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Created container machine-config-daemon Dec 02 00:32:43.721 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Started container machine-config-daemon Dec 02 00:32:44.597 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (232 times) Dec 02 00:32:49.885 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" already present on machine Dec 02 00:32:50.045 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Created container router Dec 02 00:32:50.071 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Started container router Dec 02 00:32:50.587 I ns/openshift-image-registry pod/node-ca-8rl2j Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 02 00:32:50.720 I ns/openshift-image-registry pod/node-ca-8rl2j Created container node-ca Dec 02 00:32:50.745 I ns/openshift-image-registry pod/node-ca-8rl2j Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201202-003301.xml error: 1 fail, 39 pass, 39 skip (1h5m0s) 2020/12/02 00:33:05 Container test in pod e2e-aws-serial failed, exit code 1, reason Error 2020/12/02 00:50:24 Copied 89.36MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/12/02 00:50:24 Releasing leases for \\\"e2e-aws-serial\\\" 2020/12/02 00:50:24 Releasing lease aeca4927-4e1d-407d-99f4-a405ff9a723b for \\\"aws-quota-slice\\\" 2020/12/02 00:50:25 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/02 00:50:25 Ran for 2h0m50s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-lby5f363/e2e-aws-serial failed after 1h54m14s (failed containers: test): ContainerFailed one or more containers exited Container test exited with code 1, reason Error --- ca-8rl2j Dec 02 00:32:41.345 I ns/openshift-image-registry pod/node-ca-8rl2j Successfully assigned openshift-image-registry/node-ca-8rl2j to ip-10-0-140-191.ec2.internal Dec 02 00:32:43.551 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 02 00:32:43.689 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Created container machine-config-daemon Dec 02 00:32:43.721 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Started container machine-config-daemon Dec 02 00:32:44.597 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (232 times) Dec 02 00:32:49.885 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" already present on machine Dec 02 00:32:50.045 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Created container router Dec 02 00:32:50.071 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Started container router Dec 02 00:32:50.587 I ns/openshift-image-registry pod/node-ca-8rl2j Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 02 00:32:50.720 I ns/openshift-image-registry pod/node-ca-8rl2j Created container node-ca Dec 02 00:32:50.745 I ns/openshift-image-registry pod/node-ca-8rl2j Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201202-003301.xml error: 1 fail, 39 pass, 39 skip (1h5m0s) --- '\", \"cluster_count\": 27}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 28, \"cluster_size\": 1, \"template_mined\": \"b'2020/12/02 22:50:26 ci-operator version v20201202-337afb4 2020/12/02 22:50:26 No source defined 2020/12/02 22:50:26 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/02 22:50:26 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-infqit17 2020/12/02 22:50:26 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/02 22:50:26 Creating namespace ci-op-infqit17 2020/12/02 22:50:27 Setting up pipeline imagestream for the test 2020/12/02 22:50:27 Created secret e2e-aws-serial-cluster-profile 2020/12/02 22:50:27 Created secret pull-secret 2020/12/02 22:50:27 Created PDB for pods with openshift.io/build.name label 2020/12/02 22:50:27 Created PDB for pods with created-by-ci label 2020/12/02 22:50:27 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-infqit17/stable:${component} 2020/12/02 22:50:29 Importing release image latest 2020/12/02 22:50:30 Executing pod \\\"release-images-latest-cli\\\" 2020/12/02 22:50:40 Executing pod \\\"release-images-latest\\\" 2020/12/02 22:51:32 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/02 22:51:32 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/12/02 22:51:32 Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" 2020/12/02 22:55:12 Acquired lease(s) [ac75a510-a9cb-4103-b019-8cc35b0dafa2] for \\\"aws-quota-slice\\\" 2020/12/02 22:55:12 Executing template e2e-aws-serial 2020/12/02 22:55:12 Creating or restarting template instance 2020/12/02 22:55:12 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/02 22:55:12 Waiting for template instance to be ready 2020/12/02 22:55:15 Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-2 (zones: us-east-2a us-east-2b) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=warning msg=\\\"Found override for ReleaseImage. Please be warned, this is not advised\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Openshift Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Common Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Master Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Worker Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Creating infrastructure resources...\\\" level=error level=error msg=\\\"Error: Error applying plan:\\\" level=error level=error msg=\\\"1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* module.vpc.aws_vpc.new_vpc: 1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* aws_vpc.new_vpc: Error creating VPC: VpcLimitExceeded: The maximum number of VPCs has been reached.\\\" level=error msg=\\\"\\\\\\\\tstatus code: 400, request id: e23a1f18-c7b8-4c1b-9189-8c66150640f9\\\" level=error level=error level=error level=error level=error level=error msg=\\\"Terraform does not automatically rollback in the face of errors.\\\" level=error msg=\\\"Instead, your Terraform state file has been partially updated with\\\" level=error msg=\\\"any resources that successfully completed. Please address the error\\\" level=error msg=\\\"above and apply again to incrementally change your infrastructure.\\\" level=error level=error level=fatal msg=\\\"failed to fetch Cluster: failed to generate asset \\\\\\\\\\\"Cluster\\\\\\\\\\\": failed to create cluster: failed to apply using Terraform\\\" 2020/12/02 23:01:05 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2020/12/02 23:17:09 Copied 1.98MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/12/02 23:17:10 Releasing leases for \\\"e2e-aws-serial\\\" 2020/12/02 23:17:10 Releasing lease ac75a510-a9cb-4103-b019-8cc35b0dafa2 for \\\"aws-quota-slice\\\" 2020/12/02 23:17:10 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/02 23:17:10 Ran for 26m43s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-infqit17/e2e-aws-serial failed after 21m54s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-2 (zones: us-east-2a us-east-2b) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=warning msg=\\\"Found override for ReleaseImage. Please be warned, this is not advised\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Openshift Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Common Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Master Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Worker Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Creating infrastructure resources...\\\" level=error level=error msg=\\\"Error: Error applying plan:\\\" level=error level=error msg=\\\"1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* module.vpc.aws_vpc.new_vpc: 1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* aws_vpc.new_vpc: Error creating VPC: VpcLimitExceeded: The maximum number of VPCs has been reached.\\\" level=error msg=\\\"\\\\\\\\tstatus code: 400, request id: e23a1f18-c7b8-4c1b-9189-8c66150640f9\\\" level=error level=error level=error level=error level=error level=error msg=\\\"Terraform does not automatically rollback in the face of errors.\\\" level=error msg=\\\"Instead, your Terraform state file has been partially updated with\\\" level=error msg=\\\"any resources that successfully completed. Please address the error\\\" level=error msg=\\\"above and apply again to incrementally change your infrastructure.\\\" level=error level=error level=fatal msg=\\\"failed to fetch Cluster: failed to generate asset \\\\\\\\\\\"Cluster\\\\\\\\\\\": failed to create cluster: failed to apply using Terraform\\\" --- '\", \"cluster_count\": 28}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 29, \"cluster_size\": 1, \"template_mined\": \"b'2020/12/03 22:51:11 ci-operator version v20201203-a1e69ac 2020/12/03 22:51:11 No source defined 2020/12/03 22:51:11 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/03 22:51:11 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-cbkmt7yh 2020/12/03 22:51:11 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/03 22:51:11 Creating namespace ci-op-cbkmt7yh 2020/12/03 22:51:12 Setting up pipeline imagestream for the test 2020/12/03 22:51:12 Created secret e2e-aws-serial-cluster-profile 2020/12/03 22:51:12 Created secret pull-secret 2020/12/03 22:51:12 Created PDB for pods with openshift.io/build.name label 2020/12/03 22:51:12 Created PDB for pods with created-by-ci label 2020/12/03 22:51:12 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-cbkmt7yh/stable:${component} 2020/12/03 22:52:14 Importing release image latest 2020/12/03 22:52:15 Executing pod \\\"release-images-latest-cli\\\" 2020/12/03 22:55:00 Executing pod \\\"release-images-latest\\\" 2020/12/03 22:55:52 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/03 22:55:52 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/12/03 22:55:52 Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" 2020/12/03 22:55:53 Acquired lease(s) [1e1adb94-b3ee-4a28-9527-e4c48d90eb19] for \\\"aws-quota-slice\\\" 2020/12/03 22:55:53 Executing template e2e-aws-serial 2020/12/03 22:55:53 Creating or restarting template instance 2020/12/03 22:55:53 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/03 22:55:53 Waiting for template instance to be ready 2020/12/03 22:55:55 Running pod e2e-aws-serial 2020/12/03 23:28:05 Container setup in pod e2e-aws-serial completed successfully 2020/12/04 00:35:15 Container test in pod e2e-aws-serial completed successfully 2020/12/04 00:44:15 Container teardown in pod e2e-aws-serial completed successfully 2020/12/04 00:44:15 Pod e2e-aws-serial succeeded after 1h48m18s 2020/12/04 00:44:22 Copied 103.56MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/12/04 00:44:22 Releasing leases for \\\"e2e-aws-serial\\\" 2020/12/04 00:44:22 Releasing lease 1e1adb94-b3ee-4a28-9527-e4c48d90eb19 for \\\"aws-quota-slice\\\" 2020/12/04 00:44:22 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/04 00:44:22 Ran for 1h53m11s '\", \"cluster_count\": 29}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 30, \"cluster_size\": 1, \"template_mined\": \"b'2020/12/04 22:52:06 ci-operator version v20201204-48e5852 2020/12/04 22:52:06 No source defined 2020/12/04 22:52:06 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/04 22:52:06 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-cydrph0x 2020/12/04 22:52:06 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/04 22:52:06 Creating namespace ci-op-cydrph0x 2020/12/04 22:52:07 Setting up pipeline imagestream for the test 2020/12/04 22:52:07 Created secret e2e-aws-serial-cluster-profile 2020/12/04 22:52:07 Created secret pull-secret 2020/12/04 22:52:07 Created PDB for pods with openshift.io/build.name label 2020/12/04 22:52:07 Created PDB for pods with created-by-ci label 2020/12/04 22:52:07 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-cydrph0x/stable:${component} 2020/12/04 22:53:09 Importing release image latest 2020/12/04 22:53:09 Executing pod \\\"release-images-latest-cli\\\" 2020/12/04 22:53:19 Executing pod \\\"release-images-latest\\\" 2020/12/04 22:54:17 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/04 22:54:17 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/12/04 22:54:17 Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" 2020/12/04 23:13:56 Acquired lease(s) [b315453f-673a-4c36-a4d3-64dd02315ee8] for \\\"aws-quota-slice\\\" 2020/12/04 23:13:56 Executing template e2e-aws-serial 2020/12/04 23:13:56 Creating or restarting template instance 2020/12/04 23:13:56 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/04 23:13:56 Waiting for template instance to be ready 2020/12/04 23:13:58 Running pod e2e-aws-serial 2020/12/04 23:43:58 Container setup in pod e2e-aws-serial completed successfully 2020/12/05 00:39:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:40:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:41:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:42:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:43:07 warning: Failed to get namespace ci-op-cydrph0x for heartbeating: the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces ci-op-cydrph0x) 2020/12/05 00:43:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:44:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:45:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:46:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:50:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:51:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:52:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:53:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:54:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:55:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:56:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:57:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:58:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:59:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:59:49 Container teardown in pod e2e-aws-serial completed successfully 2020/12/05 00:59:49 Container test in pod e2e-aws-serial completed successfully 2020/12/05 00:59:49 Pod e2e-aws-serial succeeded after 1h43m21s 2020/12/05 00:59:58 Copied 110.59MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/12/05 00:59:58 Releasing leases for \\\"e2e-aws-serial\\\" 2020/12/05 00:59:58 Releasing lease b315453f-673a-4c36-a4d3-64dd02315ee8 for \\\"aws-quota-slice\\\" 2020/12/05 00:59:58 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/05 00:59:59 Ran for 2h7m52s '\", \"cluster_count\": 30}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 29, \"cluster_size\": 2, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 30}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 31, \"cluster_size\": 1, \"template_mined\": \"b'2020/12/07 14:25:12 ci-operator version v20201205-32c093e 2020/12/07 14:25:12 No source defined 2020/12/07 14:25:12 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/07 14:25:12 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-f70d4d2b 2020/12/07 14:25:12 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/07 14:25:12 Creating namespace ci-op-f70d4d2b 2020/12/07 14:25:12 Setting up pipeline imagestream for the test 2020/12/07 14:25:12 Created secret e2e-aws-serial-cluster-profile 2020/12/07 14:25:12 Created secret pull-secret 2020/12/07 14:25:12 Created PDB for pods with openshift.io/build.name label 2020/12/07 14:25:12 Created PDB for pods with created-by-ci label 2020/12/07 14:25:12 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-f70d4d2b/stable:${component} 2020/12/07 14:26:14 Importing release image latest 2020/12/07 14:26:14 Executing pod \\\"release-images-latest-cli\\\" 2020/12/07 14:26:25 Executing pod \\\"release-images-latest\\\" 2020/12/07 14:27:17 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/07 14:27:17 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/12/07 14:27:17 Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" 2020/12/07 14:27:17 Acquired lease(s) [6ae2b5d6-3410-4bfc-b78d-30e51990464b] for \\\"aws-quota-slice\\\" 2020/12/07 14:27:17 Executing template e2e-aws-serial 2020/12/07 14:27:17 Creating or restarting template instance 2020/12/07 14:27:17 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/07 14:27:17 Waiting for template instance to be ready 2020/12/07 14:27:19 Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-2 (zones: us-east-2a us-east-2b) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=fatal msg=\\\"failed to fetch Common Manifests: failed to fetch dependency of \\\\\\\\\\\"Common Manifests\\\\\\\\\\\": failed to generate asset \\\\\\\\\\\"DNS Config\\\\\\\\\\\": getting public zone for \\\\\\\\\\\"origin-ci-int-aws.dev.rhcloud.com\\\\\\\\\\\": listing hosted zones: Throttling: Rate exceeded\\\\ \\\\\\\\tstatus code: 400, request id: 490a799e-917b-460c-b363-8b02ee82550e\\\" 2020/12/07 14:27:39 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2020/12/07 14:28:50 Releasing leases for \\\"e2e-aws-serial\\\" 2020/12/07 14:28:50 Releasing lease 6ae2b5d6-3410-4bfc-b78d-30e51990464b for \\\"aws-quota-slice\\\" 2020/12/07 14:28:50 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/07 14:28:50 Ran for 3m38s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-f70d4d2b/e2e-aws-serial failed after 1m31s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-2 (zones: us-east-2a us-east-2b) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=fatal msg=\\\"failed to fetch Common Manifests: failed to fetch dependency of \\\\\\\\\\\"Common Manifests\\\\\\\\\\\": failed to generate asset \\\\\\\\\\\"DNS Config\\\\\\\\\\\": getting public zone for \\\\\\\\\\\"origin-ci-int-aws.dev.rhcloud.com\\\\\\\\\\\": listing hosted zones: Throttling: Rate exceeded\\\\ \\\\\\\\tstatus code: 400, request id: 490a799e-917b-460c-b363-8b02ee82550e\\\" --- '\", \"cluster_count\": 31}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 3, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 31}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 32, \"cluster_size\": 1, \"template_mined\": \"b'2020/12/09 14:26:35 ci-operator version v20201208-a4e5d83 2020/12/09 14:26:35 No source defined 2020/12/09 14:26:35 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/09 14:26:35 could not get routes in namespace openshift-console: routes.route.openshift.io is forbidden: User \\\"system:serviceaccount:ci:ci-operator\\\" cannot list routes.route.openshift.io in the namespace \\\"openshift-console\\\": no RBAC policy matched 2020/12/09 14:26:35 Using namespace ci-op-ixrvw8d1 2020/12/09 14:26:35 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/09 14:26:35 Creating namespace ci-op-ixrvw8d1 2020/12/09 14:26:35 Setting up pipeline imagestream for the test 2020/12/09 14:26:35 Created secret e2e-aws-serial-cluster-profile 2020/12/09 14:26:35 Created secret pull-secret 2020/12/09 14:26:35 Created PDB for pods with openshift.io/build.name label 2020/12/09 14:26:35 Created PDB for pods with created-by-ci label 2020/12/09 14:26:35 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-ixrvw8d1/stable:${component} 2020/12/09 14:27:38 Importing release image latest 2020/12/09 14:27:39 Executing pod \\\"release-images-latest-cli\\\" 2020/12/09 14:27:54 Executing pod \\\"release-images-latest\\\" 2020/12/09 14:29:02 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/09 14:29:02 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/12/09 14:29:02 Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" 2020/12/09 14:29:02 Acquired lease(s) [2ac7db5a-35b5-4a53-b989-346f5c1cd386] for \\\"aws-quota-slice\\\" 2020/12/09 14:29:02 Executing template e2e-aws-serial 2020/12/09 14:29:02 Creating or restarting template instance 2020/12/09 14:29:02 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/09 14:29:02 Waiting for template instance to be ready 2020/12/09 14:29:04 Running pod e2e-aws-serial 2020/12/09 15:02:59 Container setup in pod e2e-aws-serial completed successfully 2020/12/09 15:56:35 warning: Failed to patch the ci-op-ixrvw8d1 namespace to update the ci.openshift.io/active annotation: namespaces \\\"ci-op-ixrvw8d1\\\" is forbidden: User \\\"system:serviceaccount:ci:ci-operator\\\" cannot patch namespaces in the namespace \\\"ci-op-ixrvw8d1\\\": no RBAC policy matched 2020/12/09 16:10:34 Container test in pod e2e-aws-serial completed successfully 2020/12/09 16:16:39 Container teardown in pod e2e-aws-serial completed successfully 2020/12/09 16:16:39 Pod e2e-aws-serial succeeded after 1h47m35s 2020/12/09 16:16:49 Copied 114.49MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/12/09 16:16:49 Releasing leases for \\\"e2e-aws-serial\\\" 2020/12/09 16:16:49 Releasing lease 2ac7db5a-35b5-4a53-b989-346f5c1cd386 for \\\"aws-quota-slice\\\" 2020/12/09 16:16:49 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/09 16:16:49 Ran for 1h50m14s '\", \"cluster_count\": 32}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 33, \"cluster_size\": 1, \"template_mined\": \"b'2020/12/10 14:27:48 ci-operator version v20201209-f7e3f4a 2020/12/10 14:27:48 No source defined 2020/12/10 14:27:48 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/10 14:27:49 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-zic826rl 2020/12/10 14:27:49 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/10 14:27:49 Creating namespace ci-op-zic826rl 2020/12/10 14:27:49 Setting up pipeline imagestream for the test 2020/12/10 14:27:49 Created secret e2e-aws-serial-cluster-profile 2020/12/10 14:27:49 Created secret pull-secret 2020/12/10 14:27:49 Created PDB for pods with openshift.io/build.name label 2020/12/10 14:27:49 Created PDB for pods with created-by-ci label 2020/12/10 14:27:49 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-zic826rl/stable:${component} 2020/12/10 14:28:51 Importing release image latest 2020/12/10 14:28:52 Executing pod \\\"release-images-latest-cli\\\" 2020/12/10 14:28:57 Executing pod \\\"release-images-latest\\\" 2020/12/10 14:29:44 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/10 14:29:44 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/12/10 14:29:44 Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" 2020/12/10 14:29:44 Acquired lease(s) [3e6c4d90-c379-4469-901f-55328eca22b9] for \\\"aws-quota-slice\\\" 2020/12/10 14:29:44 Executing template e2e-aws-serial 2020/12/10 14:29:44 Creating or restarting template instance 2020/12/10 14:29:44 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/10 14:29:44 Waiting for template instance to be ready 2020/12/10 14:29:46 Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-2 (zones: us-east-2a us-east-2b) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=fatal msg=\\\"failed to fetch Common Manifests: failed to fetch dependency of \\\\\\\\\\\"Common Manifests\\\\\\\\\\\": failed to generate asset \\\\\\\\\\\"DNS Config\\\\\\\\\\\": getting public zone for \\\\\\\\\\\"origin-ci-int-aws.dev.rhcloud.com\\\\\\\\\\\": listing hosted zones: Throttling: Rate exceeded\\\\ \\\\\\\\tstatus code: 400, request id: 18ad2a89-7c71-461b-9613-bcf78f153d37\\\" 2020/12/10 14:30:11 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2020/12/10 14:30:11 Container test in pod e2e-aws-serial completed successfully 2020/12/10 14:31:18 Releasing leases for \\\"e2e-aws-serial\\\" 2020/12/10 14:31:18 Releasing lease 3e6c4d90-c379-4469-901f-55328eca22b9 for \\\"aws-quota-slice\\\" 2020/12/10 14:31:18 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/10 14:31:18 Ran for 3m29s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-zic826rl/e2e-aws-serial failed after 1m31s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-2 (zones: us-east-2a us-east-2b) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=fatal msg=\\\"failed to fetch Common Manifests: failed to fetch dependency of \\\\\\\\\\\"Common Manifests\\\\\\\\\\\": failed to generate asset \\\\\\\\\\\"DNS Config\\\\\\\\\\\": getting public zone for \\\\\\\\\\\"origin-ci-int-aws.dev.rhcloud.com\\\\\\\\\\\": listing hosted zones: Throttling: Rate exceeded\\\\ \\\\\\\\tstatus code: 400, request id: 18ad2a89-7c71-461b-9613-bcf78f153d37\\\" --- '\", \"cluster_count\": 33}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 4, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 33}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 5, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 33}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 6, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 33}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 34, \"cluster_size\": 1, \"template_mined\": \"b'2020/12/14 14:31:51 ci-operator version v20201212-78272b9 2020/12/14 14:31:51 No source defined 2020/12/14 14:31:51 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/14 14:31:51 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-3p9nymqb 2020/12/14 14:31:51 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/14 14:31:51 Creating namespace ci-op-3p9nymqb 2020/12/14 14:31:51 Setting up pipeline imagestream for the test 2020/12/14 14:31:51 Created secret e2e-aws-serial-cluster-profile 2020/12/14 14:31:51 Created secret pull-secret 2020/12/14 14:31:51 Created PDB for pods with openshift.io/build.name label 2020/12/14 14:31:51 Created PDB for pods with created-by-ci label 2020/12/14 14:31:51 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-3p9nymqb/stable:${component} 2020/12/14 14:32:53 Importing release image latest 2020/12/14 14:32:54 Executing pod \\\"release-images-latest-cli\\\" 2020/12/14 14:32:59 Executing pod \\\"release-images-latest\\\" 2020/12/14 14:33:52 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/14 14:33:52 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/12/14 14:33:52 Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" 2020/12/14 14:33:52 Acquired lease(s) [us-west-2--34] for \\\"aws-quota-slice\\\" 2020/12/14 14:33:52 Executing template e2e-aws-serial 2020/12/14 14:33:52 Creating or restarting template instance 2020/12/14 14:33:52 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/14 14:33:52 Waiting for template instance to be ready 2020/12/14 14:33:54 Running pod e2e-aws-serial 2020/12/14 15:04:45 Container setup in pod e2e-aws-serial completed successfully {\\\"component\\\":\\\"entrypoint\\\",\\\"file\\\":\\\"prow/entrypoint/run.go:169\\\",\\\"func\\\":\\\"k8s.io/test-infra/prow/entrypoint.Options.ExecuteProcess\\\",\\\"level\\\":\\\"error\\\",\\\"msg\\\":\\\"Entrypoint received interrupt: terminated\\\",\\\"severity\\\":\\\"error\\\",\\\"time\\\":\\\"2020-12-14T16:13:37Z\\\"} time=\\\"2020-12-14T16:13:37Z\\\" level=info msg=\\\"Received signal.\\\" signal=interrupt '\", \"cluster_count\": 34}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 33, \"cluster_size\": 2, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: <*> (zones: <*> <*> level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=fatal msg=\\\"failed to fetch Common Manifests: failed to fetch dependency of \\\\\\\\\\\"Common Manifests\\\\\\\\\\\": failed to generate asset \\\\\\\\\\\"DNS Config\\\\\\\\\\\": getting public zone for \\\\\\\\\\\"origin-ci-int-aws.dev.rhcloud.com\\\\\\\\\\\": listing hosted zones: Throttling: Rate exceeded\\\\ \\\\\\\\tstatus code: 400, request id: <*> <*> <*> Container setup in pod e2e-aws-serial failed, exit code 1, reason Error <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod <*> failed after <*> (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: <*> (zones: <*> <*> level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=fatal msg=\\\"failed to fetch Common Manifests: failed to fetch dependency of \\\\\\\\\\\"Common Manifests\\\\\\\\\\\": failed to generate asset \\\\\\\\\\\"DNS Config\\\\\\\\\\\": getting public zone for \\\\\\\\\\\"origin-ci-int-aws.dev.rhcloud.com\\\\\\\\\\\": listing hosted zones: Throttling: Rate exceeded\\\\ \\\\\\\\tstatus code: 400, request id: <*> --- '\", \"cluster_count\": 34}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 35, \"cluster_size\": 1, \"template_mined\": \"b'2020/12/16 14:33:43 ci-operator version v20201215-fbe31de 2020/12/16 14:33:43 No source defined 2020/12/16 14:33:43 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/16 14:33:43 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-16pnc85m 2020/12/16 14:33:43 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/16 14:33:43 Creating namespace ci-op-16pnc85m 2020/12/16 14:33:43 Setting up pipeline imagestream for the test 2020/12/16 14:33:43 Created secret e2e-aws-serial-cluster-profile 2020/12/16 14:33:43 Created secret pull-secret 2020/12/16 14:33:43 Created PDB for pods with openshift.io/build.name label 2020/12/16 14:33:43 Created PDB for pods with created-by-ci label 2020/12/16 14:33:43 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-16pnc85m/stable:${component} 2020/12/16 14:34:46 Importing release image latest 2020/12/16 14:34:46 Executing pod \\\"release-images-latest-cli\\\" 2020/12/16 14:34:57 Executing pod \\\"release-images-latest\\\" 2020/12/16 14:35:45 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/16 14:35:45 Acquiring leases for \\\"e2e-aws-serial\\\" 2020/12/16 14:35:45 Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" 2020/12/16 14:35:45 Acquired lease(s) [us-west-2--06] for \\\"aws-quota-slice\\\" 2020/12/16 14:35:45 Executing template e2e-aws-serial 2020/12/16 14:35:45 Creating or restarting template instance 2020/12/16 14:35:45 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/16 14:35:45 Waiting for template instance to be ready 2020/12/16 14:35:47 Running pod e2e-aws-serial 2020/12/16 15:05:07 Container setup in pod e2e-aws-serial completed successfully secret/support created started: (0/1/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.1s) 2020-12-16T15:05:53 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (0/2/79) \\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" Dec 16 15:05:54.612: INFO: >>> kubeConfig: /tmp/admin.kubeconfig Dec 16 15:05:54.614: INFO: Waiting up to 30m0s for all (but 100) nodes to be schedulable Dec 16 15:05:54.783: INFO: Unexpected error listing nodes: Get https://api.ci-op-16pnc85m-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443/api/v1/nodes?fieldSelector=spec.unschedulable%3Dfalse&resourceVersion=0: dial tcp 44.241.182.81:6443: connect: connection refused Dec 16 15:05:54.783: INFO: Unexpected error occurred: Get https://api.ci-op-16pnc85m-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443/api/v1/nodes?fieldSelector=spec.unschedulable%3Dfalse&resourceVersion=0: dial tcp 44.241.182.81:6443: connect: connection refused Dec 16 15:05:54.783: INFO: Running AfterSuite actions on all nodes Dec 16 15:05:54.783: INFO: Running AfterSuite actions on node 1 fail [k8s.io/kubernetes/test/e2e/e2e.go:101]: Expected error: <*url.Error | 0xc002903230>: { Op: \\\"Get\\\", URL: \\\"https://api.ci-op-16pnc85m-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443/api/v1/nodes?fieldSelector=spec.unschedulable%3Dfalse&resourceVersion=0\\\", Err: { Op: \\\"dial\\\", Net: \\\"tcp\\\", Source: nil, Addr: {IP: \\\",\\\\\\\\xf1\\\\\\\\xb6Q\\\", Port: 6443, Zone: \\\"\\\"}, Err: {Syscall: \\\"connect\\\", Err: 0x6f}, }, } Get https://api.ci-op-16pnc85m-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443/api/v1/nodes?fieldSelector=spec.unschedulable%3Dfalse&resourceVersion=0: dial tcp 44.241.182.81:6443: connect: connection refused not to have occurred Dec 16 15:05:54.560 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5af7bf25b394983f3ddba543e031d0cf9b89af18f637d5fbe7e1142172ab0cb\\\" already present on machine Dec 16 15:05:54.708 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Created container kube-apiserver-6 Dec 16 15:05:54.741 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Started container kube-apiserver-6 Dec 16 15:05:54.747 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c18ce2736a4fed8091de16f868bc46e6a9a03b7956c2418bad42fbb041ffbafb\\\" already present on machine failed: (1.1s) 2020-12-16T15:05:54 \\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/3/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m44s) 2020-12-16T15:08:38 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/4/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.1s) 2020-12-16T15:09:18 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/5/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.8s) 2020-12-16T15:09:59 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/6/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (39.2s) 2020-12-16T15:10:38 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/7/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.9s) 2020-12-16T15:11:19 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/8/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m52s) 2020-12-16T15:13:11 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/9/79) \\\"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\\\" skip [github.com/openshift/origin/test/extended/images/signatures.go:25]: disable because containers/image: https://github.com/containers/image/pull/570 skipped: (15.6s) 2020-12-16T15:13:26 \\\"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\\\" started: (1/10/79) \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m17s) 2020-12-16T15:14:43 \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/11/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-12-16T15:15:24 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/12/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (34.5s) 2020-12-16T15:15:58 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/13/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-12-16T15:16:38 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/14/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-12-16T15:17:19 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/15/79) \\\"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/apps/daemon_set.go:378]: Requires at least 2 nodes (not -1) skipped: (21.3s) 2020-12-16T15:17:40 \\\"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/16/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (26.3s) 2020-12-16T15:18:07 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/17/79) \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m7s) 2020-12-16T15:19:14 \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/18/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m3s) 2020-12-16T15:21:17 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/19/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (50.4s) 2020-12-16T15:22:08 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/20/79) \\\"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:247]: Could not get controller-manager metrics - skipping skipped: (28.7s) 2020-12-16T15:22:36 \\\"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/21/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-12-16T15:23:17 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/22/79) \\\"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m50s) 2020-12-16T15:25:07 \\\"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/23/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24s) 2020-12-16T15:25:31 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/24/79) \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (36.6s) 2020-12-16T15:26:08 \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/25/79) \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (41.2s) 2020-12-16T15:26:49 \\\"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/26/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.1s) 2020-12-16T15:27:13 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/27/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24s) 2020-12-16T15:27:37 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/28/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.8s) 2020-12-16T15:28:18 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/29/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.8s) 2020-12-16T15:28:58 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/30/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m38s) 2020-12-16T15:30:36 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/31/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (22.6s) 2020-12-16T15:30:59 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/32/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m28s) 2020-12-16T15:33:27 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/33/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m54s) 2020-12-16T15:36:21 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/34/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.8s) 2020-12-16T15:37:01 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/35/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.9s) 2020-12-16T15:37:22 \\\"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/36/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (21.2s) 2020-12-16T15:37:44 \\\"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/37/79) \\\"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m0s) 2020-12-16T15:39:43 \\\"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/38/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.4s) 2020-12-16T15:40:08 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/39/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (50.9s) 2020-12-16T15:40:59 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/40/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-12-16T15:41:39 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/41/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-12-16T15:42:20 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/42/79) \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m53s) 2020-12-16T15:44:13 \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/43/79) \\\"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m12s) 2020-12-16T15:45:25 \\\"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/44/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (22.6s) 2020-12-16T15:45:47 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/45/79) \\\"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m14s) 2020-12-16T15:48:01 \\\"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/46/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m36s) 2020-12-16T15:50:37 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/47/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-12-16T15:51:18 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/48/79) \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (35.1s) 2020-12-16T15:51:53 \\\"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/49/79) \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (54.4s) 2020-12-16T15:52:47 \\\"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/50/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-12-16T15:53:28 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/51/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.9s) 2020-12-16T15:53:49 \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/52/79) \\\"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:81]: Environment does not support getting controller-manager metrics - skipping skipped: (21s) 2020-12-16T15:54:10 \\\"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/53/79) \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m32s) 2020-12-16T15:56:42 \\\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/54/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-12-16T15:57:23 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/55/79) \\\"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m57s) 2020-12-16T15:59:20 \\\"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/56/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m33s) 2020-12-16T16:00:53 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/57/79) \\\"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\\\" passed: (48.4s) 2020-12-16T16:01:41 \\\"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\\\" started: (1/58/79) \\\"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m22s) 2020-12-16T16:03:03 \\\"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/59/79) \\\"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\\\" passed: (1m2s) 2020-12-16T16:04:05 \\\"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\\\" started: (1/60/79) \\\"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (3m18s) 2020-12-16T16:07:23 \\\"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/61/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.7s) 2020-12-16T16:07:48 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/62/79) \\\"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m16s) 2020-12-16T16:09:03 \\\"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/63/79) \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m44s) 2020-12-16T16:11:47 \\\"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/64/79) \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.8s) 2020-12-16T16:12:08 \\\"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/65/79) \\\"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/scheduling/taint_based_evictions.go:63]: Requires at least 2 nodes (not -1) skipped: (21s) 2020-12-16T16:12:29 \\\"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/66/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-12-16T16:13:10 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/67/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-12-16T16:13:50 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/68/79) \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m53s) 2020-12-16T16:15:43 \\\"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/69/79) \\\"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (21.4s) 2020-12-16T16:16:05 \\\"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/70/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-12-16T16:16:45 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/71/79) \\\"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" passed: (1m58s) 2020-12-16T16:18:43 \\\"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\\\" started: (1/72/79) \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (2m40s) 2020-12-16T16:21:23 \\\"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/73/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m16s) 2020-12-16T16:22:39 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/74/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-12-16T16:23:19 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/75/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.9s) 2020-12-16T16:23:44 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/76/79) \\\"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:193]: Could not get controller-manager metrics - skipping skipped: (57.9s) 2020-12-16T16:24:42 \\\"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/77/79) \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" passed: (1m50s) 2020-12-16T16:26:33 \\\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/78/79) \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-12-16T16:27:13 \\\"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\\\" started: (1/79/79) \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.2s) 2020-12-16T16:27:37 \\\"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\\\" Timeline: Dec 16 15:05:14.483 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d58cec13cbff1c54a3ccacde2a1727765493d8f785a98aea335d6b5baf931eeb\\\" already present on machine Dec 16 15:05:14.483 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal Created container pruner Dec 16 15:05:14.483 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal Started container pruner Dec 16 15:05:14.509 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal node/ip-10-0-153-107.us-west-2.compute.internal created Dec 16 15:05:14.509 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal node/ip-10-0-137-23.us-west-2.compute.internal created Dec 16 15:05:14.509 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-137-23.us-west-2.compute.internal node/ip-10-0-137-23.us-west-2.compute.internal created Dec 16 15:05:14.509 I ns/openshift-kube-apiserver pod/installer-6-ip-10-0-153-107.us-west-2.compute.internal node/ip-10-0-153-107.us-west-2.compute.internal created Dec 16 15:05:43.644 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal node/ip-10-0-153-107.us-west-2.compute.internal pod has been pending longer than a minute Dec 16 15:05:53.601 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d58cec13cbff1c54a3ccacde2a1727765493d8f785a98aea335d6b5baf931eeb\\\" already present on machine Dec 16 15:05:53.601 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal Created container pruner Dec 16 15:05:53.601 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal Started container pruner Dec 16 15:05:53.725 - 1s I test=\\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" running Dec 16 15:05:54.560 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5af7bf25b394983f3ddba543e031d0cf9b89af18f637d5fbe7e1142172ab0cb\\\" already present on machine Dec 16 15:05:54.708 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Created container kube-apiserver-6 Dec 16 15:05:54.741 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Started container kube-apiserver-6 Dec 16 15:05:54.747 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c18ce2736a4fed8091de16f868bc46e6a9a03b7956c2418bad42fbb041ffbafb\\\" already present on machine Dec 16 15:05:54.795 I test=\\\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\\\" failed Dec 16 15:05:54.916 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Created container kube-apiserver-cert-syncer-6 Dec 16 15:05:54.948 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Started container kube-apiserver-cert-syncer-6 Dec 16 15:05:55.559 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (5 times) Dec 16 15:05:56.414 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (5 times) Dec 16 15:05:57.245 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (5 times) Dec 16 15:05:57.486 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (8 times) Dec 16 15:05:57.680 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (8 times) Dec 16 15:05:57.842 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (8 times) Dec 16 15:06:09.721 W clusteroperator/kube-apiserver changed Progressing to False: AsExpected: Progressing: 3 nodes are at revision 6 Dec 16 15:06:09.724 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Updated node \\\"ip-10-0-153-107.us-west-2.compute.internal\\\" from revision 3 to 6 Dec 16 15:06:09.773 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Updated ConfigMap/revision-status-6 -n openshift-kube-apiserver: cause by changes in data.status Dec 16 15:06:12.504 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-153-107.us-west-2.compute.internal node/ip-10-0-153-107.us-west-2.compute.internal created Dec 16 15:06:12.508 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Created Pod/revision-pruner-6-ip-10-0-153-107.us-west-2.compute.internal -n openshift-kube-apiserver because it was missing Dec 16 15:06:20.415 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-153-107.us-west-2.compute.internal Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c18ce2736a4fed8091de16f868bc46e6a9a03b7956c2418bad42fbb041ffbafb\\\" already present on machine Dec 16 15:06:20.552 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-153-107.us-west-2.compute.internal Created container pruner Dec 16 15:06:20.577 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-153-107.us-west-2.compute.internal Started container pruner Dec 16 15:07:04.607 W ns/openshift-image-registry pod/node-ca-cv7r9 node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:07:04.609 W ns/openshift-machine-config-operator pod/machine-config-daemon-w7xx2 node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 15:07:04.684 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-cv7r9 Dec 16 15:07:04.684 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-mf6bl Marking for deletion Pod openshift-marketplace/community-operators-6c6dcf4d66-mf6bl Dec 16 15:07:04.684 I ns/openshift-image-registry pod/node-ca-cv7r9 Stopping container node-ca Dec 16 15:07:04.684 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-w7xx2 Dec 16 15:07:04.684 I ns/openshift-image-registry pod/node-ca-cv7r9 Marking for deletion Pod openshift-image-registry/node-ca-cv7r9 Dec 16 15:07:04.684 W ns/openshift-ingress pod/router-default-5cd9b66f55-t6qsz node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:07:04.692 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-wm9g4 Marking for deletion Pod openshift-monitoring/prometheus-adapter-57c689dc7-wm9g4 Dec 16 15:07:04.692 I ns/openshift-machine-config-operator pod/machine-config-daemon-w7xx2 Stopping container machine-config-daemon Dec 16 15:07:04.693 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:07:04.693 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-mf6bl node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:07:04.694 I ns/openshift-ingress pod/router-default-5cd9b66f55-t6qsz Marking for deletion Pod openshift-ingress/router-default-5cd9b66f55-t6qsz Dec 16 15:07:04.695 W ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-wm9g4 node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:07:04.769 I ns/openshift-machine-config-operator pod/machine-config-daemon-w7xx2 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-w7xx2 Dec 16 15:07:04.770 I ns/openshift-ingress pod/router-default-5cd9b66f55-t6qsz Stopping container router Dec 16 15:07:04.770 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk Marking for deletion Pod openshift-monitoring/kube-state-metrics-7b4d49f7bd-xpvhk Dec 16 15:07:04.770 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-mf6bl Stopping container community-operators Dec 16 15:07:04.770 I ns/openshift-monitoring pod/alertmanager-main-0 Marking for deletion Pod openshift-monitoring/alertmanager-main-0 Dec 16 15:07:04.770 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk Stopping container kube-rbac-proxy-main Dec 16 15:07:04.770 I ns/openshift-marketplace replicaset/community-operators-6c6dcf4d66 Created pod: community-operators-6c6dcf4d66-wjdqs Dec 16 15:07:04.770 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 0s Dec 16 15:07:04.771 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:04.771 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs node/ created Dec 16 15:07:04.771 I ns/openshift-ingress pod/router-default-5cd9b66f55-4bsvp node/ created Dec 16 15:07:04.771 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-nrbvw node/ created Dec 16 15:07:04.778 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk Stopping container kube-state-metrics Dec 16 15:07:04.778 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Successfully assigned openshift-marketplace/community-operators-6c6dcf4d66-wjdqs to ip-10-0-140-148.us-west-2.compute.internal Dec 16 15:07:04.778 I ns/openshift-ingress pod/router-default-5cd9b66f55-4bsvp Successfully assigned openshift-ingress/router-default-5cd9b66f55-4bsvp to ip-10-0-151-168.us-west-2.compute.internal Dec 16 15:07:04.778 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk Stopping container kube-rbac-proxy-self Dec 16 15:07:04.778 I ns/openshift-monitoring replicaset/prometheus-adapter-57c689dc7 Created pod: prometheus-adapter-57c689dc7-nrbvw Dec 16 15:07:04.780 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-nrbvw Successfully assigned openshift-monitoring/prometheus-adapter-57c689dc7-nrbvw to ip-10-0-140-148.us-west-2.compute.internal Dec 16 15:07:04.780 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Successfully assigned openshift-monitoring/kube-state-metrics-7b4d49f7bd-m8r8n to ip-10-0-140-148.us-west-2.compute.internal Dec 16 15:07:04.780 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager Dec 16 15:07:04.780 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n node/ created Dec 16 15:07:04.855 I ns/openshift-ingress replicaset/router-default-5cd9b66f55 Created pod: router-default-5cd9b66f55-4bsvp Dec 16 15:07:04.855 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-wm9g4 Stopping container prometheus-adapter Dec 16 15:07:04.855 I ns/openshift-monitoring replicaset/kube-state-metrics-7b4d49f7bd Created pod: kube-state-metrics-7b4d49f7bd-m8r8n Dec 16 15:07:04.855 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy Dec 16 15:07:04.855 I ns/openshift-monitoring statefulset/alertmanager-main create Pod alertmanager-main-0 in StatefulSet alertmanager-main successful Dec 16 15:07:04.855 I ns/openshift-monitoring pod/alertmanager-main-0 Successfully assigned openshift-monitoring/alertmanager-main-0 to ip-10-0-151-168.us-west-2.compute.internal Dec 16 15:07:04.855 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader Dec 16 15:07:04.856 I ns/openshift-monitoring pod/alertmanager-main-0 node/ created Dec 16 15:07:05.017 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager (2 times) Dec 16 15:07:05.221 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy (2 times) Dec 16 15:07:05.417 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader (2 times) Dec 16 15:07:06.155 E ns/openshift-marketplace pod/community-operators-6c6dcf4d66-mf6bl node/ip-10-0-131-181.us-west-2.compute.internal container=community-operators container exited with code 2 (Error): Dec 16 15:07:08.153 W ns/openshift-image-registry pod/node-ca-cv7r9 node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:07:08.153 W ns/openshift-image-registry pod/node-ca-cv7r9 node/ip-10-0-131-181.us-west-2.compute.internal container=node-ca container stopped being ready Dec 16 15:07:08.552 W ns/openshift-machine-config-operator pod/machine-config-daemon-w7xx2 node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:07:08.552 W ns/openshift-machine-config-operator pod/machine-config-daemon-w7xx2 node/ip-10-0-131-181.us-west-2.compute.internal container=machine-config-daemon container stopped being ready Dec 16 15:07:08.818 W ns/openshift-machine-config-operator pod/machine-config-daemon-w7xx2 node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:09.154 E ns/openshift-ingress pod/router-default-5cd9b66f55-t6qsz node/ip-10-0-131-181.us-west-2.compute.internal container=router container exited with code 2 (Error): check ok : 0 retry attempt(s).\\\\ I1216 15:04:13.241408 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1216 15:04:18.234601 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1216 15:04:23.238772 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1216 15:04:33.295498 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1216 15:04:38.287234 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1216 15:04:43.283336 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1216 15:04:57.599531 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ E1216 15:05:52.927963 1 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=39, ErrCode=NO_ERROR, debug=\\\"\\\"\\\\ E1216 15:05:52.928002 1 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=39, ErrCode=NO_ERROR, debug=\\\"\\\"\\\\ E1216 15:05:52.927966 1 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=39, ErrCode=NO_ERROR, debug=\\\"\\\"\\\\ I1216 15:05:59.741117 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ I1216 15:06:09.696023 1 router.go:482] Router reloaded:\\\\ - Proxy protocol on, checking http://localhost:80 ...\\\\ - Health check ok : 0 retry attempt(s).\\\\ Dec 16 15:07:09.382 W ns/openshift-ingress pod/router-default-5cd9b66f55-t6qsz node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:09.753 E ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-wm9g4 node/ip-10-0-131-181.us-west-2.compute.internal container=prometheus-adapter container exited with code 2 (Error): Dec 16 15:07:09.957 W ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-wm9g4 node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:10.952 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:07:10.952 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk node/ip-10-0-131-181.us-west-2.compute.internal container=kube-state-metrics container stopped being ready Dec 16 15:07:10.952 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk node/ip-10-0-131-181.us-west-2.compute.internal container=kube-rbac-proxy-main container stopped being ready Dec 16 15:07:10.952 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk node/ip-10-0-131-181.us-west-2.compute.internal container=kube-rbac-proxy-self container stopped being ready Dec 16 15:07:11.158 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:11.817 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-mf6bl node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:12.356 W ns/openshift-image-registry pod/node-ca-cv7r9 node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:12.536 I ns/openshift-ingress pod/router-default-5cd9b66f55-4bsvp Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" Dec 16 15:07:12.767 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b9d07e8eed67a34b50a3afaa36cd4b9f052697894e64b5c203d0675d12ab5ec\\\" already present on machine Dec 16 15:07:12.913 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager Dec 16 15:07:12.937 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager Dec 16 15:07:12.942 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\\\" already present on machine Dec 16 15:07:13.109 I ns/openshift-monitoring pod/alertmanager-main-0 Created container config-reloader Dec 16 15:07:13.134 I ns/openshift-monitoring pod/alertmanager-main-0 Started container config-reloader Dec 16 15:07:13.144 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\\\" already present on machine Dec 16 15:07:13.264 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-nrbvw Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\\\" Dec 16 15:07:13.275 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:69e43ce2394b34a984aca4c30461087dcb4cef9eb6c9a61467ac192d11782ca6\\\" already present on machine Dec 16 15:07:13.282 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager-proxy Dec 16 15:07:13.307 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager-proxy Dec 16 15:07:13.432 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Created container community-operators Dec 16 15:07:13.461 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Started container community-operators Dec 16 15:07:13.959 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Dec 16 15:07:14.152 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Created container kube-rbac-proxy-main Dec 16 15:07:14.213 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Started container kube-rbac-proxy-main Dec 16 15:07:14.218 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\\\" already present on machine Dec 16 15:07:14.406 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Created container kube-rbac-proxy-self Dec 16 15:07:14.433 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Started container kube-rbac-proxy-self Dec 16 15:07:14.439 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Pulling image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c12c8ce2ac222f2205c8ab24dffef3ef34e3505c7a52d849b637ce3a7d91a388\\\" Dec 16 15:07:17.482 I ns/openshift-ingress pod/router-default-5cd9b66f55-4bsvp Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\\\" Dec 16 15:07:17.636 I ns/openshift-ingress pod/router-default-5cd9b66f55-4bsvp Created container router Dec 16 15:07:17.664 I ns/openshift-ingress pod/router-default-5cd9b66f55-4bsvp Started container router Dec 16 15:07:18.271 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-nrbvw Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\\\" Dec 16 15:07:18.462 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-nrbvw Created container prometheus-adapter Dec 16 15:07:18.524 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-nrbvw Started container prometheus-adapter Dec 16 15:07:18.819 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Successfully pulled image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c12c8ce2ac222f2205c8ab24dffef3ef34e3505c7a52d849b637ce3a7d91a388\\\" Dec 16 15:07:19.003 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Created container kube-state-metrics Dec 16 15:07:19.034 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Started container kube-state-metrics Dec 16 15:07:22.153 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Readiness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ Dec 16 15:07:23.257 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Liveness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 Dec 16 15:07:32.154 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Readiness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ (2 times) Dec 16 15:07:33.258 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Liveness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 (2 times) Dec 16 15:07:42.151 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Readiness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ (3 times) Dec 16 15:07:43.259 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Liveness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 (3 times) Dec 16 15:07:52.160 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Readiness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ (4 times) Dec 16 15:07:53.259 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Liveness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 (4 times) Dec 16 15:08:02.163 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Readiness probe failed: timeout: failed to connect service \\\"localhost:50051\\\" within 1s\\\\ (5 times) Dec 16 15:08:03.258 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Liveness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 (5 times) Dec 16 15:08:09.876 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg node/ created Dec 16 15:08:09.953 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-v6jzg Dec 16 15:08:09.953 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg Successfully assigned openshift-machine-config-operator/machine-config-daemon-v6jzg to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:08:09.953 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4wd4w Dec 16 15:08:09.953 I ns/openshift-image-registry pod/node-ca-4wd4w node/ created Dec 16 15:08:09.961 I ns/openshift-image-registry pod/node-ca-4wd4w Successfully assigned openshift-image-registry/node-ca-4wd4w to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:08:10.612 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 16 15:08:10.729 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg Created container machine-config-daemon Dec 16 15:08:10.766 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg Started container machine-config-daemon Dec 16 15:08:13.258 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Liveness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 (6 times) Dec 16 15:08:14.256 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Readiness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 Dec 16 15:08:17.740 I ns/openshift-image-registry pod/node-ca-4wd4w Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 16 15:08:17.854 I ns/openshift-image-registry pod/node-ca-4wd4w Created container node-ca Dec 16 15:08:17.877 I ns/openshift-image-registry pod/node-ca-4wd4w Started container node-ca Dec 16 15:15:55.575 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (6 times) Dec 16 15:15:56.478 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (6 times) Dec 16 15:15:57.524 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (6 times) Dec 16 15:15:57.687 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (9 times) Dec 16 15:15:57.832 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (9 times) Dec 16 15:15:57.968 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (9 times) Dec 16 15:25:52.549 W ns/openshift-image-registry pod/node-ca-4wd4w node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:25:52.626 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4wd4w Dec 16 15:25:52.626 I ns/openshift-image-registry pod/node-ca-4wd4w Stopping container node-ca Dec 16 15:25:52.626 W ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 15:25:52.626 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-v6jzg Dec 16 15:25:52.626 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-v6jzg Dec 16 15:25:52.626 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg Stopping container machine-config-daemon Dec 16 15:25:52.626 I ns/openshift-image-registry pod/node-ca-4wd4w Marking for deletion Pod openshift-image-registry/node-ca-4wd4w Dec 16 15:25:53.898 W ns/openshift-image-registry pod/node-ca-4wd4w node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:25:53.898 W ns/openshift-image-registry pod/node-ca-4wd4w node/ip-10-0-131-181.us-west-2.compute.internal container=node-ca container stopped being ready Dec 16 15:25:53.905 W ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:25:53.905 W ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg node/ip-10-0-131-181.us-west-2.compute.internal container=machine-config-daemon container stopped being ready Dec 16 15:25:55.682 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (7 times) Dec 16 15:25:56.787 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (7 times) Dec 16 15:25:56.954 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (10 times) Dec 16 15:25:57.093 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (10 times) Dec 16 15:25:57.312 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (10 times) Dec 16 15:25:58.311 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (7 times) Dec 16 15:25:58.644 W ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg node/ip-10-0-131-181.us-west-2.compute.internal pod has been pending longer than a minute Dec 16 15:25:58.644 W ns/openshift-image-registry pod/node-ca-4wd4w node/ip-10-0-131-181.us-west-2.compute.internal pod has been pending longer than a minute Dec 16 15:26:00.435 W ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:26:00.516 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-hxc2m Dec 16 15:26:00.516 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m Successfully assigned openshift-machine-config-operator/machine-config-daemon-hxc2m to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:26:00.516 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m node/ created Dec 16 15:26:00.521 W ns/openshift-image-registry pod/node-ca-4wd4w node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:26:00.528 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-gbg6f Dec 16 15:26:00.528 I ns/openshift-image-registry pod/node-ca-gbg6f node/ created Dec 16 15:26:00.601 I ns/openshift-image-registry pod/node-ca-gbg6f Successfully assigned openshift-image-registry/node-ca-gbg6f to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:26:01.942 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 16 15:26:02.063 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m Created container machine-config-daemon Dec 16 15:26:02.095 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m Started container machine-config-daemon Dec 16 15:26:09.604 I ns/openshift-image-registry pod/node-ca-gbg6f Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 16 15:26:09.735 I ns/openshift-image-registry pod/node-ca-gbg6f Created container node-ca Dec 16 15:26:09.757 I ns/openshift-image-registry pod/node-ca-gbg6f Started container node-ca Dec 16 15:34:37.292 W ns/openshift-image-registry pod/node-ca-gbg6f node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:34:37.292 W ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 15:34:37.359 I ns/openshift-image-registry pod/node-ca-gbg6f Stopping container node-ca Dec 16 15:34:37.359 I ns/openshift-image-registry pod/node-ca-gbg6f Marking for deletion Pod openshift-image-registry/node-ca-gbg6f Dec 16 15:34:37.359 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-hxc2m Dec 16 15:34:37.359 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m Stopping container machine-config-daemon Dec 16 15:34:37.359 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-hxc2m Dec 16 15:34:37.359 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-gbg6f Dec 16 15:34:38.774 W ns/openshift-image-registry pod/node-ca-gbg6f node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:34:38.774 W ns/openshift-image-registry pod/node-ca-gbg6f node/ip-10-0-131-181.us-west-2.compute.internal container=node-ca container stopped being ready Dec 16 15:34:38.809 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-pv6t6 Dec 16 15:34:38.809 W ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:34:38.810 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 node/ created Dec 16 15:34:38.855 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 Successfully assigned openshift-machine-config-operator/machine-config-daemon-pv6t6 to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:34:39.415 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 16 15:34:39.518 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 Created container machine-config-daemon Dec 16 15:34:39.544 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 Started container machine-config-daemon Dec 16 15:34:43.644 W ns/openshift-image-registry pod/node-ca-gbg6f node/ip-10-0-131-181.us-west-2.compute.internal pod has been pending longer than a minute Dec 16 15:34:50.430 W ns/openshift-image-registry pod/node-ca-gbg6f node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:34:50.508 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4fhbr Dec 16 15:34:50.508 I ns/openshift-image-registry pod/node-ca-4fhbr Successfully assigned openshift-image-registry/node-ca-4fhbr to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:34:50.508 I ns/openshift-image-registry pod/node-ca-4fhbr node/ created Dec 16 15:34:58.794 I ns/openshift-image-registry pod/node-ca-4fhbr Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 16 15:34:58.947 I ns/openshift-image-registry pod/node-ca-4fhbr Created container node-ca Dec 16 15:34:58.986 I ns/openshift-image-registry pod/node-ca-4fhbr Started container node-ca Dec 16 15:35:55.827 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (8 times) Dec 16 15:35:56.702 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (8 times) Dec 16 15:35:57.535 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (8 times) Dec 16 15:35:57.688 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (11 times) Dec 16 15:35:57.887 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (11 times) Dec 16 15:35:58.060 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (11 times) Dec 16 15:39:04.823 I ns/kube-system pod/critical-pod node/ created Dec 16 15:39:04.828 W ns/kube-system pod/critical-pod 0/6 nodes are available: 3 Insufficient memory, 3 node(s) had taints that the pod didn\\\\'t tolerate. Dec 16 15:39:04.904 W ns/kube-system pod/critical-pod 0/6 nodes are available: 3 Insufficient memory, 3 node(s) had taints that the pod didn\\\\'t tolerate. (2 times) Dec 16 15:39:07.196 I ns/kube-system pod/critical-pod Successfully assigned kube-system/critical-pod to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:39:14.702 I ns/kube-system pod/critical-pod Container image \\\"k8s.gcr.io/pause:3.1\\\" already present on machine Dec 16 15:39:14.837 I ns/kube-system pod/critical-pod Created container critical-pod Dec 16 15:39:14.857 I ns/kube-system pod/critical-pod Started container critical-pod Dec 16 15:39:17.408 W ns/kube-system pod/critical-pod node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 0s Dec 16 15:39:17.411 W ns/kube-system pod/critical-pod node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:39:17.415 I ns/kube-system pod/critical-pod Stopping container critical-pod Dec 16 15:39:17.423 I ns/kube-system pod/critical-pod Stopping container critical-pod (2 times) Dec 16 15:43:30.258 W ns/openshift-image-registry pod/node-ca-4fhbr node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:43:30.260 W ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 15:43:30.266 I ns/openshift-image-registry pod/node-ca-4fhbr Stopping container node-ca Dec 16 15:43:30.268 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4fhbr Dec 16 15:43:30.268 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-pv6t6 Dec 16 15:43:30.272 I ns/openshift-image-registry pod/node-ca-4fhbr Marking for deletion Pod openshift-image-registry/node-ca-4fhbr Dec 16 15:43:30.272 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-pv6t6 Dec 16 15:43:30.273 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 Stopping container machine-config-daemon Dec 16 15:43:31.596 W ns/openshift-image-registry pod/node-ca-4fhbr node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:43:31.596 W ns/openshift-image-registry pod/node-ca-4fhbr node/ip-10-0-131-181.us-west-2.compute.internal container=node-ca container stopped being ready Dec 16 15:43:40.430 W ns/openshift-image-registry pod/node-ca-4fhbr node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:43:40.509 W ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:44:00.613 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp node/ created Dec 16 15:44:00.619 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-ms9vp Dec 16 15:44:00.682 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp Successfully assigned openshift-machine-config-operator/machine-config-daemon-ms9vp to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:44:00.682 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-rd77v Dec 16 15:44:00.682 I ns/openshift-image-registry pod/node-ca-rd77v Successfully assigned openshift-image-registry/node-ca-rd77v to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:44:00.683 I ns/openshift-image-registry pod/node-ca-rd77v node/ created Dec 16 15:44:01.284 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 16 15:44:01.414 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp Created container machine-config-daemon Dec 16 15:44:01.437 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp Started container machine-config-daemon Dec 16 15:44:08.337 I ns/openshift-image-registry pod/node-ca-rd77v Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 16 15:44:08.459 I ns/openshift-image-registry pod/node-ca-rd77v Created container node-ca Dec 16 15:44:08.482 I ns/openshift-image-registry pod/node-ca-rd77v Started container node-ca Dec 16 15:45:55.557 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (9 times) Dec 16 15:45:56.605 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (9 times) Dec 16 15:45:57.647 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (9 times) Dec 16 15:45:57.798 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (12 times) Dec 16 15:45:57.942 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (12 times) Dec 16 15:45:58.086 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (12 times) Dec 16 15:55:55.562 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (10 times) Dec 16 15:55:56.411 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (10 times) Dec 16 15:55:57.262 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (10 times) Dec 16 15:55:57.415 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (13 times) Dec 16 15:55:57.553 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (13 times) Dec 16 15:55:57.711 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (13 times) Dec 16 16:00:30.393 W ns/openshift-image-registry pod/node-ca-rd77v node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 16:00:30.396 W ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 16:00:30.466 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-rd77v Dec 16 16:00:30.466 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-ms9vp Dec 16 16:00:30.466 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-ms9vp Dec 16 16:00:30.466 I ns/openshift-image-registry pod/node-ca-rd77v Marking for deletion Pod openshift-image-registry/node-ca-rd77v Dec 16 16:00:30.466 I ns/openshift-image-registry pod/node-ca-rd77v Stopping container node-ca Dec 16 16:00:30.466 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp Stopping container machine-config-daemon Dec 16 16:00:40.511 W ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:00:40.525 W ns/openshift-image-registry pod/node-ca-rd77v node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:00:40.617 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb node/ created Dec 16 16:00:40.624 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-sz5hb Dec 16 16:00:40.625 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb Successfully assigned openshift-machine-config-operator/machine-config-daemon-sz5hb to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:00:40.634 I ns/openshift-image-registry pod/node-ca-h28qx node/ created Dec 16 16:00:40.638 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-h28qx Dec 16 16:00:40.642 I ns/openshift-image-registry pod/node-ca-h28qx Successfully assigned openshift-image-registry/node-ca-h28qx to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:00:42.317 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 16 16:00:42.431 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb Created container machine-config-daemon Dec 16 16:00:42.462 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb Started container machine-config-daemon Dec 16 16:00:50.470 I ns/openshift-image-registry pod/node-ca-h28qx Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 16 16:00:50.600 I ns/openshift-image-registry pod/node-ca-h28qx Created container node-ca Dec 16 16:00:50.633 I ns/openshift-image-registry pod/node-ca-h28qx Started container node-ca Dec 16 16:05:55.777 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (11 times) Dec 16 16:05:55.925 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (14 times) Dec 16 16:05:56.055 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (14 times) Dec 16 16:05:56.187 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (14 times) Dec 16 16:05:57.077 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (11 times) Dec 16 16:05:57.947 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (11 times) Dec 16 16:10:14.051 W ns/openshift-image-registry pod/node-ca-h28qx node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 16:10:14.057 W ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 16:10:14.064 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-h28qx Dec 16 16:10:14.064 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-sz5hb Dec 16 16:10:14.125 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-sz5hb Dec 16 16:10:14.126 I ns/openshift-image-registry pod/node-ca-h28qx Stopping container node-ca Dec 16 16:10:14.126 I ns/openshift-image-registry pod/node-ca-h28qx Marking for deletion Pod openshift-image-registry/node-ca-h28qx Dec 16 16:10:14.126 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb Stopping container machine-config-daemon Dec 16 16:10:20.438 W ns/openshift-image-registry pod/node-ca-h28qx node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:10:20.515 W ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:11:19.316 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j node/ created Dec 16 16:11:19.389 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-vc42j Dec 16 16:11:19.389 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j Successfully assigned openshift-machine-config-operator/machine-config-daemon-vc42j to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:11:19.389 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-s9hls Dec 16 16:11:19.390 I ns/openshift-image-registry pod/node-ca-s9hls node/ created Dec 16 16:11:19.400 I ns/openshift-image-registry pod/node-ca-s9hls Successfully assigned openshift-image-registry/node-ca-s9hls to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:11:19.983 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 16 16:11:20.101 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j Created container machine-config-daemon Dec 16 16:11:20.129 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j Started container machine-config-daemon Dec 16 16:11:27.070 I ns/openshift-image-registry pod/node-ca-s9hls Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 16 16:11:27.189 I ns/openshift-image-registry pod/node-ca-s9hls Created container node-ca Dec 16 16:11:27.212 I ns/openshift-image-registry pod/node-ca-s9hls Started container node-ca Dec 16 16:15:52.229 I ns/kube-system pod/pod0-system-node-critical node/ created Dec 16 16:15:52.238 I ns/kube-system pod/pod0-system-node-critical Successfully assigned kube-system/pod0-system-node-critical to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:15:52.317 I ns/kube-system pod/pod1-system-cluster-critical node/ created Dec 16 16:15:52.324 I ns/kube-system pod/pod1-system-cluster-critical Successfully assigned kube-system/pod1-system-cluster-critical to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:15:52.406 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 0s Dec 16 16:15:52.410 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:15:52.500 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 0s Dec 16 16:15:52.503 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:15:55.596 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (12 times) Dec 16 16:15:56.418 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (12 times) Dec 16 16:15:57.249 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (12 times) Dec 16 16:15:57.399 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (15 times) Dec 16 16:15:57.531 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (15 times) Dec 16 16:15:57.683 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (15 times) Dec 16 16:16:02.182 W ns/kube-system pod/pod0-system-node-critical Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_pod0-system-node-critical_kube-system_f7ae53a2-3fb9-11eb-8ef4-067d63babeb5_0(5743c7b17a06e4df1af00ebf05c99f1fb06df242a80c8e05c82210ffa0c68931): Multus: Err adding pod to network \\\"openshift-sdn\\\": cannot set \\\"openshift-sdn\\\" ifname to \\\"eth0\\\": no netns: failed to Statfs \\\"/proc/102608/ns/net\\\": no such file or directory Dec 16 16:17:55.347 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \\\"pod1-system-cluster-critical_kube-system(f7bc2192-3fb9-11eb-8ef4-067d63babeb5)\\\": timeout expired waiting for volumes to attach or mount for pod \\\"kube-system\\\"/\\\"pod1-system-cluster-critical\\\". list of unmounted volumes=[default-token-7q7hv]. list of unattached volumes=[default-token-7q7hv] Dec 16 16:19:53.824 W ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 16:19:53.824 W ns/openshift-image-registry pod/node-ca-s9hls node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 16:19:53.834 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-s9hls Dec 16 16:19:53.835 I ns/openshift-image-registry pod/node-ca-s9hls Marking for deletion Pod openshift-image-registry/node-ca-s9hls Dec 16 16:19:53.837 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j Stopping container machine-config-daemon Dec 16 16:19:53.898 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-vc42j Dec 16 16:19:53.898 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-vc42j Dec 16 16:19:53.898 I ns/openshift-image-registry pod/node-ca-s9hls Stopping container node-ca Dec 16 16:20:00.433 W ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:20:00.516 W ns/openshift-image-registry pod/node-ca-s9hls node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:21:10.621 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv7kx node/ created Dec 16 16:21:10.626 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-pv7kx Dec 16 16:21:10.693 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv7kx Successfully assigned openshift-machine-config-operator/machine-config-daemon-pv7kx to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:21:10.693 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-cdgwk Dec 16 16:21:10.693 I ns/openshift-image-registry pod/node-ca-cdgwk Successfully assigned openshift-image-registry/node-ca-cdgwk to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:21:10.694 I ns/openshift-image-registry pod/node-ca-cdgwk node/ created Dec 16 16:21:11.273 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv7kx Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\\\" already present on machine Dec 16 16:21:11.410 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv7kx Created container machine-config-daemon Dec 16 16:21:11.434 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv7kx Started container machine-config-daemon Dec 16 16:21:19.142 I ns/openshift-image-registry pod/node-ca-cdgwk Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 16 16:21:19.258 I ns/openshift-image-registry pod/node-ca-cdgwk Created container node-ca Dec 16 16:21:19.280 I ns/openshift-image-registry pod/node-ca-cdgwk Started container node-ca Dec 16 16:24:30.571 W persistentvolume/pvc-169dd196-3fbb-11eb-a83f-02e495b1b803 Error deleting EBS volume \\\"vol-0f2193f218508c930\\\" since volume is currently attached to \\\"i-0294833eaa2b7d0bc\\\" Dec 16 16:25:55.628 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (13 times) Dec 16 16:25:56.465 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (13 times) Dec 16 16:25:57.460 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (13 times) Dec 16 16:25:57.661 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (16 times) Dec 16 16:25:57.816 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (16 times) Dec 16 16:25:57.946 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (16 times) Failing tests: [sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201216-162737.xml error: 1 fail, 39 pass, 39 skip (1h22m24s) 2020/12/16 16:27:42 Container test in pod e2e-aws-serial failed, exit code 1, reason Error 2020/12/16 16:35:02 Copied 113.22MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/12/16 16:35:03 Releasing leases for \\\"e2e-aws-serial\\\" 2020/12/16 16:35:03 Releasing lease us-west-2--06 for \\\"aws-quota-slice\\\" 2020/12/16 16:35:03 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/16 16:35:03 Ran for 2h1m19s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-16pnc85m/e2e-aws-serial failed after 1h59m7s (failed containers: test): ContainerFailed one or more containers exited Container test exited with code 1, reason Error --- ine-config-operator pod/machine-config-daemon-pv7kx Started container machine-config-daemon Dec 16 16:21:19.142 I ns/openshift-image-registry pod/node-ca-cdgwk Container image \\\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\\\" already present on machine Dec 16 16:21:19.258 I ns/openshift-image-registry pod/node-ca-cdgwk Created container node-ca Dec 16 16:21:19.280 I ns/openshift-image-registry pod/node-ca-cdgwk Started container node-ca Dec 16 16:24:30.571 W persistentvolume/pvc-169dd196-3fbb-11eb-a83f-02e495b1b803 Error deleting EBS volume \\\"vol-0f2193f218508c930\\\" since volume is currently attached to \\\"i-0294833eaa2b7d0bc\\\" Dec 16 16:25:55.628 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (13 times) Dec 16 16:25:56.465 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (13 times) Dec 16 16:25:57.460 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (13 times) Dec 16 16:25:57.661 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (16 times) Dec 16 16:25:57.816 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (16 times) Dec 16 16:25:57.946 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (16 times) Failing tests: [sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201216-162737.xml error: 1 fail, 39 pass, 39 skip (1h22m24s) --- '\", \"cluster_count\": 35}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 7, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 35}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 8, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 35}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 9, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 35}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 36, \"cluster_size\": 1, \"template_mined\": \"b' <!doctype html> \\\\t<html> \\\\t<head> \\\\t <link rel=\\\"stylesheet\\\" type=\\\"text/css\\\" href=\\\"/styles/style.css\\\"> \\\\t <meta charset=\\\"utf-8\\\"> \\\\t <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\"> \\\\t <title>GCS browser: origin-ci-test</title> \\\\t\\\\t<style> \\\\t\\\\theader { \\\\t\\\\t\\\\tmargin-left: 10px; \\\\t\\\\t} \\\\t\\\\t.next-button { \\\\t\\\\t\\\\tmargin: 10px 0; \\\\t\\\\t} \\\\t\\\\t.grid-head { \\\\t\\\\t\\\\tborder-bottom: 1px solid black; \\\\t\\\\t} \\\\t\\\\t.resource-grid { \\\\t\\\\t\\\\tmargin-right: 20px; \\\\t\\\\t} \\\\t\\\\tli.grid-row:nth-child(even) { \\\\t\\\\t\\\\tbackground-color: #ddd; \\\\t\\\\t} \\\\t\\\\tli div { \\\\t\\\\t\\\\tbox-sizing: border-box; \\\\t\\\\t\\\\tborder-left: 1px solid black; \\\\t\\\\t\\\\tpadding-left: 5px; \\\\t\\\\t\\\\toverflow-wrap: break-word; \\\\t\\\\t} \\\\t\\\\tli div:first-child { \\\\t\\\\t\\\\tborder-left: none; \\\\t\\\\t} \\\\t\\\\t</style> \\\\t</head> \\\\t<body> <header> <h1>origin-ci-test</h1> <h3>/origin-ci-test/logs/release-openshift-ocp-installer-e2e-aws-serial-4.1/1340667363283963904/build-log.txt/</h3> </header> <ul class=\\\"resource-grid\\\"> \\\\t<li class=\\\"pure-g\\\"> \\\\t\\\\t<div class=\\\"pure-u-2-5 grid-head\\\">Name</div> \\\\t\\\\t<div class=\\\"pure-u-1-5 grid-head\\\">Size</div> \\\\t\\\\t<div class=\\\"pure-u-2-5 grid-head\\\">Modified</div> \\\\t</li> <li class=\\\"pure-g grid-row\\\"> \\\\t <div class=\\\"pure-u-2-5\\\"><a href=\\\"/gcs/origin-ci-test/logs/release-openshift-ocp-installer-e2e-aws-serial-4.1/1340667363283963904/\\\"><img src=\\\"/icons/back.png\\\"> ..</a></div> \\\\t <div class=\\\"pure-u-1-5\\\">-</div> \\\\t <div class=\\\"pure-u-2-5\\\">-</div> \\\\t</li> </ul></body></html>'\", \"cluster_count\": 36}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 10, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 36}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 11, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 36}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 12, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 36}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 13, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 36}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 14, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 36}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 15, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 36}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 16, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 36}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 17, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 36}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 18, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 36}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 19, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 36}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 29, \"cluster_size\": 20, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" <*> <*> Executing pod \\\"release-images-latest\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 36}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 37, \"cluster_size\": 1, \"template_mined\": \"b'2021/01/01 14:46:39 ci-operator version v20201231-4891b38 2021/01/01 14:46:39 No source defined 2021/01/01 14:46:39 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2021/01/01 14:46:39 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-0s9rlm2b 2021/01/01 14:46:39 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2021/01/01 14:46:39 Creating namespace ci-op-0s9rlm2b 2021/01/01 14:46:39 Setting up pipeline imagestream for the test 2021/01/01 14:46:39 Created secret e2e-aws-serial-cluster-profile 2021/01/01 14:46:39 Created secret pull-secret 2021/01/01 14:46:39 Created PDB for pods with openshift.io/build.name label 2021/01/01 14:46:39 Created PDB for pods with created-by-ci label 2021/01/01 14:46:39 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-0s9rlm2b/stable:${component} 2021/01/01 14:47:41 Importing release image latest 2021/01/01 14:47:42 Executing pod \\\"release-images-latest-cli\\\" running image \\\"release:latest\\\" 2021/01/01 14:47:47 Executing pod \\\"release-images-latest\\\" running image \\\"stable:cli\\\" 2021/01/01 14:48:40 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2021/01/01 14:48:40 Acquiring leases for \\\"e2e-aws-serial\\\" 2021/01/01 14:48:40 Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" 2021/01/01 14:48:40 Acquired lease(s) [us-east-1--46] for \\\"aws-quota-slice\\\" 2021/01/01 14:48:40 Executing template e2e-aws-serial 2021/01/01 14:48:40 Creating or restarting template instance 2021/01/01 14:48:40 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2021/01/01 14:48:40 Waiting for template instance to be ready 2021/01/01 14:48:42 Running pod e2e-aws-serial 2021/01/01 15:15:02 Container setup in pod e2e-aws-serial completed successfully 2021/01/01 16:36:27 Container test in pod e2e-aws-serial completed successfully 2021/01/01 16:42:32 Container teardown in pod e2e-aws-serial completed successfully 2021/01/01 16:42:32 Pod e2e-aws-serial succeeded after 1h53m50s 2021/01/01 16:42:40 Copied 115.71MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2021/01/01 16:42:40 Releasing leases for \\\"e2e-aws-serial\\\" 2021/01/01 16:42:40 Releasing lease us-east-1--46 for \\\"aws-quota-slice\\\" 2021/01/01 16:42:40 No custom metadata found and prow metadata already exists. Not updating the metadata. 2021/01/01 16:42:40 Ran for 1h56m1s '\", \"cluster_count\": 37}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 38, \"cluster_size\": 1, \"template_mined\": \"b'2021/01/02 14:47:33 ci-operator version v20201231-4891b38 2021/01/02 14:47:33 No source defined 2021/01/02 14:47:33 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2021/01/02 14:47:33 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-6jxq6mqt 2021/01/02 14:47:33 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2021/01/02 14:47:33 Creating namespace ci-op-6jxq6mqt 2021/01/02 14:47:34 Setting up pipeline imagestream for the test 2021/01/02 14:47:34 Created secret e2e-aws-serial-cluster-profile 2021/01/02 14:47:34 Created secret pull-secret 2021/01/02 14:47:34 Created PDB for pods with openshift.io/build.name label 2021/01/02 14:47:34 Created PDB for pods with created-by-ci label 2021/01/02 14:47:34 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-6jxq6mqt/stable:${component} 2021/01/02 14:48:36 Importing release image latest 2021/01/02 14:48:37 Executing pod \\\"release-images-latest-cli\\\" running image \\\"release:latest\\\" 2021/01/02 14:48:47 Executing pod \\\"release-images-latest\\\" running image \\\"stable:cli\\\" 2021/01/02 14:49:35 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2021/01/02 14:49:35 Acquiring leases for \\\"e2e-aws-serial\\\" 2021/01/02 14:49:35 Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" 2021/01/02 14:49:35 Acquired lease(s) [us-west-2--02] for \\\"aws-quota-slice\\\" 2021/01/02 14:49:35 Executing template e2e-aws-serial 2021/01/02 14:49:35 Creating or restarting template instance 2021/01/02 14:49:35 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2021/01/02 14:49:35 Waiting for template instance to be ready 2021/01/02 14:49:37 Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-1 (zones: us-east-1b us-east-1c) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=warning msg=\\\"Found override for ReleaseImage. Please be warned, this is not advised\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Common Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Worker Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Openshift Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Master Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Creating infrastructure resources...\\\" level=error level=error msg=\\\"Error: Error applying plan:\\\" level=error level=error msg=\\\"1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* module.dns.aws_route53_record.api_external: 1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* aws_route53_record.api_external: [ERR]: Error building changeset: timeout while waiting for state to become \\\\'accepted\\\\' (timeout: 5m0s)\\\" level=error level=error level=error level=error level=error level=error msg=\\\"Terraform does not automatically rollback in the face of errors.\\\" level=error msg=\\\"Instead, your Terraform state file has been partially updated with\\\" level=error msg=\\\"any resources that successfully completed. Please address the error\\\" level=error msg=\\\"above and apply again to incrementally change your infrastructure.\\\" level=error level=error level=fatal msg=\\\"failed to fetch Cluster: failed to generate asset \\\\\\\\\\\"Cluster\\\\\\\\\\\": failed to create cluster: failed to apply using Terraform\\\" 2021/01/02 15:05:47 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2021/01/02 15:08:26 Copied 7.04MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2021/01/02 15:08:26 Releasing leases for \\\"e2e-aws-serial\\\" 2021/01/02 15:08:26 Releasing lease us-west-2--02 for \\\"aws-quota-slice\\\" 2021/01/02 15:08:26 No custom metadata found and prow metadata already exists. Not updating the metadata. 2021/01/02 15:08:26 Ran for 20m52s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-6jxq6mqt/e2e-aws-serial failed after 18m49s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-1 (zones: us-east-1b us-east-1c) level=info msg=\\\"Consuming \\\\\\\\\\\"Install Config\\\\\\\\\\\" from target directory\\\" level=warning msg=\\\"Found override for ReleaseImage. Please be warned, this is not advised\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Common Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Worker Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Openshift Manifests\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Consuming \\\\\\\\\\\"Master Machines\\\\\\\\\\\" from target directory\\\" level=info msg=\\\"Creating infrastructure resources...\\\" level=error level=error msg=\\\"Error: Error applying plan:\\\" level=error level=error msg=\\\"1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* module.dns.aws_route53_record.api_external: 1 error occurred:\\\" level=error msg=\\\"\\\\\\\\t* aws_route53_record.api_external: [ERR]: Error building changeset: timeout while waiting for state to become \\\\'accepted\\\\' (timeout: 5m0s)\\\" level=error level=error level=error level=error level=error level=error msg=\\\"Terraform does not automatically rollback in the face of errors.\\\" level=error msg=\\\"Instead, your Terraform state file has been partially updated with\\\" level=error msg=\\\"any resources that successfully completed. Please address the error\\\" level=error msg=\\\"above and apply again to incrementally change your infrastructure.\\\" level=error level=error level=fatal msg=\\\"failed to fetch Cluster: failed to generate asset \\\\\\\\\\\"Cluster\\\\\\\\\\\": failed to create cluster: failed to apply using Terraform\\\" --- '\", \"cluster_count\": 38}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 37, \"cluster_size\": 2, \"template_mined\": \"<*> <*> ci-operator version v20201231-4891b38 <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" running image \\\"release:latest\\\" <*> <*> Executing pod \\\"release-images-latest\\\" running image \\\"stable:cli\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 38}\n",
            "{\"change_type\": \"none\", \"cluster_id\": 37, \"cluster_size\": 3, \"template_mined\": \"<*> <*> ci-operator version v20201231-4891b38 <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" running image \\\"release:latest\\\" <*> <*> Executing pod \\\"release-images-latest\\\" running image \\\"stable:cli\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> for \\\"aws-quota-slice\\\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> for \\\"aws-quota-slice\\\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 38}\n",
            "{\"change_type\": \"cluster_created\", \"cluster_id\": 39, \"cluster_size\": 1, \"template_mined\": \"b'2021/01/05 14:50:25 ci-operator version v20210105-a274f63 2021/01/05 14:50:25 No source defined 2021/01/05 14:50:25 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2021/01/05 14:50:25 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-n2904x8n 2021/01/05 14:50:25 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2021/01/05 14:50:25 Creating namespace ci-op-n2904x8n 2021/01/05 14:50:25 Setting up pipeline imagestream for the test 2021/01/05 14:50:25 Created secret e2e-aws-serial-cluster-profile 2021/01/05 14:50:25 Created secret pull-secret 2021/01/05 14:50:25 Created PDB for pods with openshift.io/build.name label 2021/01/05 14:50:25 Created PDB for pods with created-by-ci label 2021/01/05 14:50:25 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-n2904x8n/stable:${component} 2021/01/05 14:51:28 Importing release image latest 2021/01/05 14:51:28 Executing pod \\\"release-images-latest-cli\\\" running image \\\"release:latest\\\" 2021/01/05 14:52:03 Executing pod \\\"release-images-latest\\\" running image \\\"stable:cli\\\" 2021/01/05 14:54:17 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2021/01/05 14:54:17 Acquiring leases for \\\"e2e-aws-serial\\\" 2021/01/05 14:54:17 Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" 2021/01/05 14:54:17 Acquired lease(s) for \\\"aws-quota-slice\\\": [us-west-1--26] 2021/01/05 14:54:17 Executing template e2e-aws-serial 2021/01/05 14:54:17 Creating or restarting template instance 2021/01/05 14:54:17 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2021/01/05 14:54:17 Waiting for template instance to be ready 2021/01/05 14:54:19 Running pod e2e-aws-serial 2021/01/05 15:20:25 warning: Failed to patch the ci-op-n2904x8n namespace to update the ci.openshift.io/active annotation: namespaces \\\"ci-op-n2904x8n\\\" is forbidden: User \\\"system:serviceaccount:ci:ci-operator\\\" cannot patch namespaces in the namespace \\\"ci-op-n2904x8n\\\": no RBAC policy matched 2021/01/05 15:28:09 Container setup in pod e2e-aws-serial completed successfully 2021/01/05 16:35:49 Container test in pod e2e-aws-serial completed successfully 2021/01/05 16:42:19 Container teardown in pod e2e-aws-serial completed successfully 2021/01/05 16:42:19 Pod e2e-aws-serial succeeded after 1h47m58s 2021/01/05 16:42:27 Copied 106.34MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2021/01/05 16:42:27 Releasing leases for \\\"e2e-aws-serial\\\" 2021/01/05 16:42:27 Releasing lease for \\\"aws-quota-slice\\\": us-west-1--26 2021/01/05 16:42:27 No custom metadata found and prow metadata already exists. Not updating the metadata. 2021/01/05 16:42:27 Ran for 1h52m1s '\", \"cluster_count\": 39}\n",
            "{\"change_type\": \"cluster_template_changed\", \"cluster_id\": 37, \"cluster_size\": 4, \"template_mined\": \"<*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \\\"release-images-latest-cli\\\" running image \\\"release:latest\\\" <*> <*> Executing pod \\\"release-images-latest\\\" running image \\\"stable:cli\\\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \\\"e2e-aws-serial\\\" <*> <*> Acquiring 1 lease(s) for \\\"aws-quota-slice\\\" <*> <*> Acquired lease(s) <*> <*> <*> <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \\\"e2e-aws-serial\\\" <*> <*> Releasing lease <*> <*> <*> <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\", \"cluster_count\": 39}\n",
            "Clusters:\n",
            "ID=1     : size=2         : <*> <*> ci-operator version v20200924-c41f44a <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], [images], [release:latest], e2e-aws-serial <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \"aws-quota-slice\" <*> <*> Acquired lease <*> for \"aws-quota-slice\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \"aws-quota-slice\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> <*> <*> could not load result reporting options: failed to read file \"\": open : no such file or directory '\n",
            "ID=2     : size=1         : b'2020/09/28 21:58:42 ci-operator version v20200928-c450a36 2020/09/28 21:58:42 No source defined 2020/09/28 21:58:42 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/09/28 21:58:42 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-485l57ir 2020/09/28 21:58:42 Running [release-inputs], [images], [release:latest], e2e-aws-serial 2020/09/28 21:58:42 Creating namespace ci-op-485l57ir 2020/09/28 21:58:42 Setting up pipeline imagestream for the test 2020/09/28 21:58:42 Created secret e2e-aws-serial-cluster-profile 2020/09/28 21:58:42 Created secret pull-secret 2020/09/28 21:58:42 Created PDB for pods with openshift.io/build.name label 2020/09/28 21:58:42 Created PDB for pods with created-by-ci label 2020/09/28 21:58:42 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-485l57ir/stable:${component} 2020/09/28 21:58:45 Importing release image latest 2020/09/28 21:59:58 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/09/28 21:59:58 Acquiring lease for \"aws-quota-slice\" 2020/09/28 21:59:58 Acquired lease \"864adb84-1287-4fbf-9a0c-adfeb4c73ddb\" for \"aws-quota-slice\" 2020/09/28 21:59:58 Executing template e2e-aws-serial 2020/09/28 21:59:58 Creating or restarting template instance 2020/09/28 21:59:58 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/09/28 21:59:58 Waiting for template instance to be ready 2020/09/28 22:00:00 Running pod e2e-aws-serial 2020/09/28 22:31:33 Container setup in pod e2e-aws-serial completed successfully 2020/09/29 00:02:59 error: unable to gather container logs: [error: Unable to retrieve logs from pod container setup: container \"setup\" in pod \"e2e-aws-serial\" is terminated, error: Unable to retrieve logs from pod container teardown: container \"teardown\" in pod \"e2e-aws-serial\" is terminated, error: Unable to retrieve logs from pod container test: container \"test\" in pod \"e2e-aws-serial\" is terminated] 2020/09/29 00:02:59 error: unable to retrieve artifacts from pod e2e-aws-serial: could not read gzipped artifacts: unable to upgrade connection: container not found (\"artifacts\") 2020/09/29 00:02:59 Releasing lease for \"aws-quota-slice\" 2020/09/29 00:02:59 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/09/29 00:02:59 Ran for 2h4m17s 2020/09/29 00:02:59 could not load result reporting options: mandatory flag -report-password-file is unset '\n",
            "ID=3     : size=1         : b'2020/09/29 21:59:35 ci-operator version v20200929-3687fb5 2020/09/29 21:59:35 No source defined 2020/09/29 21:59:35 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/09/29 21:59:35 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-ct0f2jrf 2020/09/29 21:59:35 Running [release-inputs], [images], [release:latest], e2e-aws-serial 2020/09/29 21:59:35 Creating namespace ci-op-ct0f2jrf 2020/09/29 21:59:35 Setting up pipeline imagestream for the test 2020/09/29 21:59:35 Created secret e2e-aws-serial-cluster-profile 2020/09/29 21:59:35 Created secret pull-secret 2020/09/29 21:59:35 Created PDB for pods with openshift.io/build.name label 2020/09/29 21:59:35 Created PDB for pods with created-by-ci label 2020/09/29 21:59:35 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-ct0f2jrf/stable:${component} 2020/09/29 21:59:37 Importing release image latest 2020/09/29 22:00:28 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/09/29 22:00:28 Acquiring lease for \"aws-quota-slice\" 2020/09/29 22:00:28 Acquired lease \"53162864-5217-47c3-b07a-f56cf02c18e1\" for \"aws-quota-slice\" 2020/09/29 22:00:28 Executing template e2e-aws-serial 2020/09/29 22:00:28 Creating or restarting template instance 2020/09/29 22:00:28 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/09/29 22:00:28 Waiting for template instance to be ready 2020/09/29 22:00:30 Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-west-2 (zones: us-west-2a us-west-2b) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=warning msg=\"Found override for ReleaseImage. Please be warned, this is not advised\" level=info msg=\"Consuming \\\\\"Openshift Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Master Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Worker Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Common Manifests\\\\\" from target directory\" level=info msg=\"Creating infrastructure resources...\" level=error level=error msg=\"Error: Error applying plan:\" level=error level=error msg=\"1 error occurred:\" level=error msg=\"\\\\t* module.bootstrap.aws_iam_role.bootstrap: 1 error occurred:\" level=error msg=\"\\\\t* aws_iam_role.bootstrap: Error creating IAM Role ci-op-ct0f2jrf-7bc5c-fdj2p-bootstrap-role: timeout while waiting for state to become \\'success\\' (timeout: 30s)\" level=error level=error level=error level=error level=error level=error msg=\"Terraform does not automatically rollback in the face of errors.\" level=error msg=\"Instead, your Terraform state file has been partially updated with\" level=error msg=\"any resources that successfully completed. Please address the error\" level=error msg=\"above and apply again to incrementally change your infrastructure.\" level=error level=error level=fatal msg=\"failed to fetch Cluster: failed to generate asset \\\\\"Cluster\\\\\": failed to create cluster: failed to apply using Terraform\" 2020/09/29 22:09:36 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2020/09/29 22:14:52 Copied 6.95MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/09/29 22:14:52 Releasing lease for \"aws-quota-slice\" 2020/09/29 22:14:52 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/09/29 22:14:52 Ran for 15m17s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-ct0f2jrf/e2e-aws-serial failed after 14m21s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-west-2 (zones: us-west-2a us-west-2b) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=warning msg=\"Found override for ReleaseImage. Please be warned, this is not advised\" level=info msg=\"Consuming \\\\\"Openshift Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Master Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Worker Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Common Manifests\\\\\" from target directory\" level=info msg=\"Creating infrastructure resources...\" level=error level=error msg=\"Error: Error applying plan:\" level=error level=error msg=\"1 error occurred:\" level=error msg=\"\\\\t* module.bootstrap.aws_iam_role.bootstrap: 1 error occurred:\" level=error msg=\"\\\\t* aws_iam_role.bootstrap: Error creating IAM Role ci-op-ct0f2jrf-7bc5c-fdj2p-bootstrap-role: timeout while waiting for state to become \\'success\\' (timeout: 30s)\" level=error level=error level=error level=error level=error level=error msg=\"Terraform does not automatically rollback in the face of errors.\" level=error msg=\"Instead, your Terraform state file has been partially updated with\" level=error msg=\"any resources that successfully completed. Please address the error\" level=error msg=\"above and apply again to incrementally change your infrastructure.\" level=error level=error level=fatal msg=\"failed to fetch Cluster: failed to generate asset \\\\\"Cluster\\\\\": failed to create cluster: failed to apply using Terraform\" --- 2020/09/29 22:14:52 could not load result reporting options: mandatory flag -report-password-file is unset '\n",
            "ID=4     : size=1         : b'2020/09/30 22:00:30 ci-operator version v20200930-8970be3 2020/09/30 22:00:30 No source defined 2020/09/30 22:00:30 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/09/30 22:00:30 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-lgihjhzt 2020/09/30 22:00:30 Running [release-inputs], [images], [release:latest], e2e-aws-serial 2020/09/30 22:00:30 Creating namespace ci-op-lgihjhzt 2020/09/30 22:00:30 Setting up pipeline imagestream for the test 2020/09/30 22:00:30 Created secret e2e-aws-serial-cluster-profile 2020/09/30 22:00:30 Created secret pull-secret 2020/09/30 22:00:30 Created PDB for pods with openshift.io/build.name label 2020/09/30 22:00:30 Created PDB for pods with created-by-ci label 2020/09/30 22:00:30 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-lgihjhzt/stable:${component} 2020/09/30 22:00:32 Importing release image latest 2020/09/30 22:01:25 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/09/30 22:01:25 Acquiring lease for \"aws-quota-slice\" 2020/09/30 22:01:25 Acquired lease \"0574c731-0389-4b44-8a63-e0ce1b9fe94c\" for \"aws-quota-slice\" 2020/09/30 22:01:25 Executing template e2e-aws-serial 2020/09/30 22:01:25 Creating or restarting template instance 2020/09/30 22:01:25 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/09/30 22:01:25 Waiting for template instance to be ready 2020/09/30 22:01:27 Running pod e2e-aws-serial 2020/09/30 22:34:12 warning: failed to update lease \"0574c731-0389-4b44-8a63-e0ce1b9fe94c\": [Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=0574c731-0389-4b44-8a63-e0ce1b9fe94c&owner=ci-op-lgihjhzt-7bc5c&state=leased\": net/http: TLS handshake timeout, Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=0574c731-0389-4b44-8a63-e0ce1b9fe94c&owner=ci-op-lgihjhzt-7bc5c&state=leased\": dial tcp: i/o timeout] 2020/09/30 22:36:15 Container setup in pod e2e-aws-serial completed successfully 2020/09/30 22:40:38 warning: failed to update lease \"0574c731-0389-4b44-8a63-e0ce1b9fe94c\": Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=0574c731-0389-4b44-8a63-e0ce1b9fe94c&owner=ci-op-lgihjhzt-7bc5c&state=leased\": net/http: TLS handshake timeout 2020/10/01 00:06:38 Copied 119.25MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/01 00:06:38 Releasing lease for \"aws-quota-slice\" 2020/10/01 00:06:38 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/01 00:06:38 Ran for 2h6m8s 2020/10/01 00:06:38 could not load result reporting options: mandatory flag -report-password-file is unset '\n",
            "ID=5     : size=3         : <*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \"aws-quota-slice\" <*> <*> Acquired lease <*> for \"aws-quota-slice\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \"aws-quota-slice\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> <*> <*> could not load result reporting options: mandatory flag -report-password-file is unset '\n",
            "ID=6     : size=1         : b'2020/10/02 22:02:18 ci-operator version v20201002-9e95fb1 2020/10/02 22:02:18 No source defined 2020/10/02 22:02:18 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/02 22:02:18 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-8b90spl7 2020/10/02 22:02:18 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/02 22:02:18 Creating namespace ci-op-8b90spl7 2020/10/02 22:02:19 Setting up pipeline imagestream for the test 2020/10/02 22:02:19 Created secret e2e-aws-serial-cluster-profile 2020/10/02 22:02:19 Created secret pull-secret 2020/10/02 22:02:19 Created PDB for pods with openshift.io/build.name label 2020/10/02 22:02:19 Created PDB for pods with created-by-ci label 2020/10/02 22:02:19 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-8b90spl7/stable:${component} 2020/10/02 22:02:21 Importing release image latest 2020/10/02 22:03:32 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/02 22:03:32 Acquiring lease for \"aws-quota-slice\" 2020/10/02 22:03:32 Acquired lease \"42f2c087-ff9e-4455-be19-eaae74b6271e\" for \"aws-quota-slice\" 2020/10/02 22:03:32 Executing template e2e-aws-serial 2020/10/02 22:03:32 Creating or restarting template instance 2020/10/02 22:03:32 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/02 22:03:32 Waiting for template instance to be ready 2020/10/02 22:03:35 Running pod e2e-aws-serial 2020/10/02 22:37:35 Container setup in pod e2e-aws-serial completed successfully 2020/10/02 22:49:01 warning: failed to update lease \"42f2c087-ff9e-4455-be19-eaae74b6271e\": Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\": net/http: TLS handshake timeout 2020/10/02 22:54:07 warning: failed to update lease \"42f2c087-ff9e-4455-be19-eaae74b6271e\": Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\": net/http: TLS handshake timeout 2020/10/02 22:58:10 warning: failed to update lease \"42f2c087-ff9e-4455-be19-eaae74b6271e\": Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\": net/http: TLS handshake timeout 2020/10/02 23:03:30 warning: failed to update lease \"42f2c087-ff9e-4455-be19-eaae74b6271e\": Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\": net/http: TLS handshake timeout 2020/10/02 23:06:49 warning: failed to update lease \"42f2c087-ff9e-4455-be19-eaae74b6271e\": Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\": net/http: TLS handshake timeout 2020/10/02 23:08:46 warning: failed to update lease \"42f2c087-ff9e-4455-be19-eaae74b6271e\": Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\": net/http: TLS handshake timeout 2020/10/02 23:11:02 warning: failed to update lease \"42f2c087-ff9e-4455-be19-eaae74b6271e\": Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\": net/http: TLS handshake timeout 2020/10/02 23:15:30 warning: failed to update lease \"42f2c087-ff9e-4455-be19-eaae74b6271e\": Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\": net/http: TLS handshake timeout 2020/10/02 23:17:40 warning: failed to update lease \"42f2c087-ff9e-4455-be19-eaae74b6271e\": Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=42f2c087-ff9e-4455-be19-eaae74b6271e&owner=ci-op-8b90spl7-7bc5c&state=leased\": net/http: TLS handshake timeout 2020/10/03 00:08:45 Copied 120.71MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/03 00:08:45 Releasing lease for \"aws-quota-slice\" 2020/10/03 00:08:45 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/03 00:08:45 Ran for 2h6m26s 2020/10/03 00:08:45 could not load result reporting options: mandatory flag -report-password-file is unset '\n",
            "ID=7     : size=1         : b'2020/10/03 22:11:41 ci-operator version v20201002-9e95fb1 2020/10/03 22:11:41 No source defined 2020/10/03 22:11:41 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/03 22:11:41 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-ztmfi4lp 2020/10/03 22:11:41 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/03 22:11:41 Creating namespace ci-op-ztmfi4lp 2020/10/03 22:11:41 Setting up pipeline imagestream for the test 2020/10/03 22:11:41 Created secret e2e-aws-serial-cluster-profile 2020/10/03 22:11:41 Created secret pull-secret 2020/10/03 22:11:41 Created PDB for pods with openshift.io/build.name label 2020/10/03 22:11:41 Created PDB for pods with created-by-ci label 2020/10/03 22:11:41 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-ztmfi4lp/stable:${component} 2020/10/03 22:11:43 Importing release image latest 2020/10/03 22:12:47 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/03 22:12:47 Acquiring lease for \"aws-quota-slice\" 2020/10/03 22:12:47 Acquired lease \"9354bf8f-a2f6-4692-9d2d-a87a6ea02c40\" for \"aws-quota-slice\" 2020/10/03 22:12:47 Executing template e2e-aws-serial 2020/10/03 22:12:47 Creating or restarting template instance 2020/10/03 22:12:48 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/03 22:12:48 Waiting for template instance to be ready 2020/10/03 22:12:50 Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-1 (zones: us-east-1b us-east-1c) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=warning msg=\"Found override for ReleaseImage. Please be warned, this is not advised\" level=info msg=\"Consuming \\\\\"Master Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Openshift Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Worker Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Common Manifests\\\\\" from target directory\" level=info msg=\"Creating infrastructure resources...\" level=info msg=\"Waiting up to 30m0s for the Kubernetes API at https://api.ci-op-ztmfi4lp-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443...\" level=info msg=\"API v1.13.4-138-g41dc99c up\" level=info msg=\"Waiting up to 30m0s for bootstrapping to complete...\" level=info msg=\"Destroying the bootstrap resources...\" level=info msg=\"Waiting up to 30m0s for the cluster at https://api.ci-op-ztmfi4lp-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 to initialize...\" level=fatal msg=\"failed to initialize the cluster: Multiple errors are preventing progress:\\ * Cluster operator monitoring is reporting a failure: Failed to rollout the stack. Error: running task Updating Prometheus-k8s failed: waiting for Prometheus Route to become ready failed: waiting for RouteReady of prometheus-k8s: the server is currently unable to handle the request (get routes.route.openshift.io prometheus-k8s)\\ * Cluster operator openshift-apiserver has not yet reported success: timed out waiting for the condition\" 2020/10/03 23:04:48 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2020/10/03 23:29:18 Copied 43.26MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/03 23:29:18 Releasing lease for \"aws-quota-slice\" 2020/10/03 23:29:19 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/03 23:29:19 Ran for 1h17m37s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-ztmfi4lp/e2e-aws-serial failed after 1h16m22s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-1 (zones: us-east-1b us-east-1c) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=warning msg=\"Found override for ReleaseImage. Please be warned, this is not advised\" level=info msg=\"Consuming \\\\\"Master Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Openshift Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Worker Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Common Manifests\\\\\" from target directory\" level=info msg=\"Creating infrastructure resources...\" level=info msg=\"Waiting up to 30m0s for the Kubernetes API at https://api.ci-op-ztmfi4lp-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443...\" level=info msg=\"API v1.13.4-138-g41dc99c up\" level=info msg=\"Waiting up to 30m0s for bootstrapping to complete...\" level=info msg=\"Destroying the bootstrap resources...\" level=info msg=\"Waiting up to 30m0s for the cluster at https://api.ci-op-ztmfi4lp-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 to initialize...\" level=fatal msg=\"failed to initialize the cluster: Multiple errors are preventing progress:\\ * Cluster operator monitoring is reporting a failure: Failed to rollout the stack. Error: running task Updating Prometheus-k8s failed: waiting for Prometheus Route to become ready failed: waiting for RouteReady of prometheus-k8s: the server is currently unable to handle the request (get routes.route.openshift.io prometheus-k8s)\\ * Cluster operator openshift-apiserver has not yet reported success: timed out waiting for the condition\" --- 2020/10/03 23:29:19 could not load result reporting options: mandatory flag -report-password-file is unset '\n",
            "ID=8     : size=3         : <*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> to <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \"aws-quota-slice\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> <*> <*> could not load result reporting options: mandatory flag -report-password-file is unset '\n",
            "ID=9     : size=1         : b'2020/10/06 22:04:28 ci-operator version v20201006-ef0331a 2020/10/06 22:04:28 No source defined 2020/10/06 22:04:28 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/06 22:04:28 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-i0y349zs 2020/10/06 22:04:28 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/06 22:04:28 Creating namespace ci-op-i0y349zs 2020/10/06 22:04:28 Setting up pipeline imagestream for the test 2020/10/06 22:04:28 Created secret e2e-aws-serial-cluster-profile 2020/10/06 22:04:28 Created secret pull-secret 2020/10/06 22:04:29 Created PDB for pods with openshift.io/build.name label 2020/10/06 22:04:29 Created PDB for pods with created-by-ci label 2020/10/06 22:04:29 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-i0y349zs/stable:${component} 2020/10/06 22:04:31 Importing release image latest 2020/10/06 22:05:24 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/06 22:05:24 Acquiring lease for \"aws-quota-slice\" 2020/10/06 22:05:24 Acquired lease \"0c6bba54-1615-4ab3-af76-928fddc42f2a\" for \"aws-quota-slice\" 2020/10/06 22:05:24 Executing template e2e-aws-serial 2020/10/06 22:05:24 Creating or restarting template instance 2020/10/06 22:05:24 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/06 22:05:24 Waiting for template instance to be ready 2020/10/06 22:05:26 Running pod e2e-aws-serial 2020/10/06 22:31:24 Container setup in pod e2e-aws-serial completed successfully 2020/10/06 23:39:17 Container test in pod e2e-aws-serial completed successfully 2020/10/06 23:44:56 Container teardown in pod e2e-aws-serial completed successfully 2020/10/06 23:44:56 Pod e2e-aws-serial succeeded after 1h39m30s 2020/10/06 23:45:46 Copied 105.31MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/06 23:47:10 Releasing lease for \"aws-quota-slice\" 2020/10/06 23:54:10 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/06 23:54:25 Ran for 1h49m56s 2020/10/06 23:54:25 could not load result reporting options: mandatory flag -report-password-file is unset '\n",
            "ID=10    : size=1         : b'2020/10/08 22:06:31 ci-operator version v20201008-bd69a8a 2020/10/08 22:06:31 No source defined 2020/10/08 22:06:31 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/08 22:06:31 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-t0j93dv1 2020/10/08 22:06:31 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/08 22:06:31 Creating namespace ci-op-t0j93dv1 2020/10/08 22:06:31 Setting up pipeline imagestream for the test 2020/10/08 22:06:31 Created secret e2e-aws-serial-cluster-profile 2020/10/08 22:06:31 Created secret pull-secret 2020/10/08 22:06:31 Created PDB for pods with openshift.io/build.name label 2020/10/08 22:06:31 Created PDB for pods with created-by-ci label 2020/10/08 22:06:31 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-t0j93dv1/stable:${component} 2020/10/08 22:06:34 Importing release image latest 2020/10/08 22:06:35 Executing pod \"release-images-latest-cli\" 2020/10/08 22:06:52 Executing pod \"release-images-latest\" 2020/10/08 22:07:42 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/08 22:07:42 Acquiring lease for \"aws-quota-slice\" 2020/10/08 22:07:44 Acquired lease \"aa794ad8-fbab-44d7-9c7f-c5f632d23536\" for \"aws-quota-slice\" 2020/10/08 22:07:44 Executing template e2e-aws-serial 2020/10/08 22:07:48 Creating or restarting template instance 2020/10/08 22:07:48 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/08 22:07:48 Waiting for template instance to be ready 2020/10/08 22:07:50 Running pod e2e-aws-serial 2020/10/08 22:41:37 Container setup in pod e2e-aws-serial completed successfully 2020/10/08 23:19:47 warning: failed to update lease \"aa794ad8-fbab-44d7-9c7f-c5f632d23536\": Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=aa794ad8-fbab-44d7-9c7f-c5f632d23536&owner=ci-op-t0j93dv1-7bc5c&state=leased\": net/http: TLS handshake timeout I1008 23:26:40.659217 16 request.go:621] Throttling request took 1.315662144s, request: GET:https://172.30.0.1:443/api/v1/namespaces/ci-op-t0j93dv1 2020/10/08 23:34:12 warning: failed to update lease \"aa794ad8-fbab-44d7-9c7f-c5f632d23536\": Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=aa794ad8-fbab-44d7-9c7f-c5f632d23536&owner=ci-op-t0j93dv1-7bc5c&state=leased\": net/http: TLS handshake timeout 2020/10/08 23:38:13 warning: failed to update lease \"aa794ad8-fbab-44d7-9c7f-c5f632d23536\": Post \"https://boskos-ci.apps.ci.l2s4.p1.openshiftapps.com/update?name=aa794ad8-fbab-44d7-9c7f-c5f632d23536&owner=ci-op-t0j93dv1-7bc5c&state=leased\": net/http: TLS handshake timeout 2020/10/09 00:11:39 Copied 126.52MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/09 00:11:39 Releasing lease for \"aws-quota-slice\" 2020/10/09 00:11:39 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/09 00:11:39 Ran for 2h5m8s 2020/10/09 00:11:39 could not load result reporting options: mandatory flag -report-password-file is unset '\n",
            "ID=11    : size=1         : b'2020/10/10 22:07:34 ci-operator version v20201009-5784e03 2020/10/10 22:07:34 No source defined 2020/10/10 22:07:34 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/10 22:07:34 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-qx14fdfg 2020/10/10 22:07:34 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/10 22:07:34 Creating namespace ci-op-qx14fdfg 2020/10/10 22:07:34 Setting up pipeline imagestream for the test 2020/10/10 22:07:34 Created secret e2e-aws-serial-cluster-profile 2020/10/10 22:07:34 Created secret pull-secret 2020/10/10 22:07:34 Created PDB for pods with openshift.io/build.name label 2020/10/10 22:07:34 Created PDB for pods with created-by-ci label 2020/10/10 22:07:34 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-qx14fdfg/stable:${component} 2020/10/10 22:07:36 Importing release image latest 2020/10/10 22:07:37 Executing pod \"release-images-latest-cli\" 2020/10/10 22:07:44 Executing pod \"release-images-latest\" 2020/10/10 22:08:33 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/10 22:08:33 Acquiring lease for \"aws-quota-slice\" 2020/10/10 22:08:34 Acquired lease \"c5f904bd-9c28-4d45-8565-06eb1478137c\" for \"aws-quota-slice\" 2020/10/10 22:08:34 Executing template e2e-aws-serial 2020/10/10 22:08:34 Creating or restarting template instance 2020/10/10 22:08:34 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/10 22:08:34 Waiting for template instance to be ready 2020/10/10 22:08:36 Running pod e2e-aws-serial 2020/10/10 22:42:05 Container setup in pod e2e-aws-serial completed successfully I1010 23:17:42.661737 14 request.go:621] Throttling request took 4.919880382s, request: GET:https://172.30.0.1:443/api/v1/namespaces/ci-op-qx14fdfg 2020/10/10 23:52:04 Container test in pod e2e-aws-serial completed successfully 2020/10/10 23:58:02 Copied 113.21MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/10 23:58:02 Releasing lease for \"aws-quota-slice\" 2020/10/10 23:58:02 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/10 23:58:02 Ran for 1h50m28s 2020/10/10 23:58:02 could not load result reporting options: mandatory flag -report-password-file is unset '\n",
            "ID=12    : size=1         : b'2020/10/12 22:09:34 ci-operator version v20201009-5784e03 2020/10/12 22:09:34 No source defined 2020/10/12 22:09:34 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/12 22:09:34 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-qqz5nvq1 2020/10/12 22:09:34 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/12 22:09:34 Creating namespace ci-op-qqz5nvq1 2020/10/12 22:09:34 Setting up pipeline imagestream for the test 2020/10/12 22:09:34 Created secret e2e-aws-serial-cluster-profile 2020/10/12 22:09:34 Created secret pull-secret 2020/10/12 22:09:34 Created PDB for pods with openshift.io/build.name label 2020/10/12 22:09:34 Created PDB for pods with created-by-ci label 2020/10/12 22:09:34 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-qqz5nvq1/stable:${component} 2020/10/12 22:09:36 Importing release image latest 2020/10/12 22:09:37 Executing pod \"release-images-latest-cli\" 2020/10/12 22:09:43 Executing pod \"release-images-latest\" 2020/10/12 22:10:30 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/12 22:10:30 Acquiring lease for \"aws-quota-slice\" 2020/10/12 22:10:30 Acquired lease \"6fdc4895-3ff1-45be-9ef5-8a250b3bfba8\" for \"aws-quota-slice\" 2020/10/12 22:10:30 Executing template e2e-aws-serial 2020/10/12 22:10:30 Creating or restarting template instance 2020/10/12 22:10:30 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/12 22:10:30 Waiting for template instance to be ready 2020/10/12 22:10:32 Running pod e2e-aws-serial I1012 22:29:42.215381 15 request.go:621] Throttling request took 2.550226459s, request: GET:https://172.30.0.1:443/api/v1/namespaces/ci-op-qqz5nvq1 Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-west-1 (zones: us-west-1a us-west-1b) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=warning msg=\"Found override for ReleaseImage. Please be warned, this is not advised\" level=info msg=\"Consuming \\\\\"Common Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Master Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Worker Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Openshift Manifests\\\\\" from target directory\" level=info msg=\"Creating infrastructure resources...\" level=info msg=\"Waiting up to 30m0s for the Kubernetes API at https://api.ci-op-qqz5nvq1-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443...\" level=info msg=\"API v1.13.4-138-g41dc99c up\" level=info msg=\"Waiting up to 30m0s for bootstrapping to complete...\" level=info msg=\"Destroying the bootstrap resources...\" level=info msg=\"Waiting up to 30m0s for the cluster at https://api.ci-op-qqz5nvq1-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 to initialize...\" level=fatal msg=\"failed to initialize the cluster: Some cluster operators are still updating: authentication, console: timed out waiting for the condition\" 2020/10/12 22:59:35 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2020/10/12 23:05:17 Copied 51.00MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/12 23:05:17 Releasing lease for \"aws-quota-slice\" 2020/10/12 23:05:17 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/12 23:05:17 Ran for 55m43s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-qqz5nvq1/e2e-aws-serial failed after 54m39s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-west-1 (zones: us-west-1a us-west-1b) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=warning msg=\"Found override for ReleaseImage. Please be warned, this is not advised\" level=info msg=\"Consuming \\\\\"Common Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Master Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Worker Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Openshift Manifests\\\\\" from target directory\" level=info msg=\"Creating infrastructure resources...\" level=info msg=\"Waiting up to 30m0s for the Kubernetes API at https://api.ci-op-qqz5nvq1-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443...\" level=info msg=\"API v1.13.4-138-g41dc99c up\" level=info msg=\"Waiting up to 30m0s for bootstrapping to complete...\" level=info msg=\"Destroying the bootstrap resources...\" level=info msg=\"Waiting up to 30m0s for the cluster at https://api.ci-op-qqz5nvq1-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 to initialize...\" level=fatal msg=\"failed to initialize the cluster: Some cluster operators are still updating: authentication, console: timed out waiting for the condition\" --- 2020/10/12 23:05:17 could not load result reporting options: mandatory flag -report-password-file is unset '\n",
            "ID=13    : size=1         : b'2020/10/13 22:10:08 ci-operator version v20201013-e136392 2020/10/13 22:10:08 No source defined 2020/10/13 22:10:08 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/13 22:10:08 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-xh6icmvm 2020/10/13 22:10:08 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/13 22:10:08 Creating namespace ci-op-xh6icmvm 2020/10/13 22:10:08 Setting up pipeline imagestream for the test 2020/10/13 22:10:08 Created secret e2e-aws-serial-cluster-profile 2020/10/13 22:10:08 Created secret pull-secret 2020/10/13 22:10:08 Created PDB for pods with openshift.io/build.name label 2020/10/13 22:10:08 Created PDB for pods with created-by-ci label 2020/10/13 22:10:09 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-xh6icmvm/stable:${component} 2020/10/13 22:10:11 Importing release image latest 2020/10/13 22:10:11 Executing pod \"release-images-latest-cli\" 2020/10/13 22:10:20 Executing pod \"release-images-latest\" 2020/10/13 22:11:07 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/13 22:11:07 Acquiring lease for \"aws-quota-slice\" 2020/10/13 22:14:45 Acquired lease \"a70ec978-4528-4ef7-9631-4792e1e2c901\" for \"aws-quota-slice\" 2020/10/13 22:14:45 Executing template e2e-aws-serial 2020/10/13 22:14:45 Creating or restarting template instance 2020/10/13 22:14:45 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/13 22:14:45 Waiting for template instance to be ready 2020/10/13 22:14:47 Running pod e2e-aws-serial 2020/10/13 22:45:26 Container setup in pod e2e-aws-serial completed successfully secret/support created started: (0/1/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m37s) 2020-10-13T22:48:15 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/2/79) \"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m48s) 2020-10-13T22:50:02 \"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/3/79) \"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m6s) 2020-10-13T22:51:08 \"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/4/79) \"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\" skip [github.com/openshift/origin/test/extended/images/signatures.go:25]: disable because containers/image: https://github.com/containers/image/pull/570 skipped: (9.8s) 2020-10-13T22:51:18 \"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\" started: (0/5/79) \"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (2m58s) 2020-10-13T22:54:16 \"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/6/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T22:54:45 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/7/79) \"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/apps/daemon_set.go:378]: Requires at least 2 nodes (not -1) skipped: (11.8s) 2020-10-13T22:54:57 \"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/8/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" Oct 13 22:54:58.214: INFO: >>> kubeConfig: /tmp/admin.kubeconfig Oct 13 22:54:58.218: INFO: Waiting up to 30m0s for all (but 100) nodes to be schedulable Oct 13 22:54:58.393: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace \\'kube-system\\' to be running and ready Oct 13 22:54:58.484: INFO: 0 / 0 pods in namespace \\'kube-system\\' are running and ready (0 seconds elapsed) Oct 13 22:54:58.485: INFO: expected 0 pod replicas in namespace \\'kube-system\\', 0 are Running and Ready. Oct 13 22:54:58.485: INFO: Waiting up to 5m0s for all daemonsets in namespace \\'kube-system\\' to start Oct 13 22:54:58.514: INFO: e2e test version: v1.13.4-138-g41dc99c Oct 13 22:54:58.538: INFO: kube-apiserver version: v1.13.4-138-g41dc99c [BeforeEach] [Top Level] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/test/extended/util/test.go:69 [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:153 STEP: Creating a kubernetes client Oct 13 22:54:58.539: INFO: >>> kubeConfig: /tmp/admin.kubeconfig STEP: Building a namespace api object, basename sched-priority Oct 13 22:55:00.084: INFO: About to run a Kube e2e test, ensuring namespace is privileged Oct 13 22:55:00.423: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled. STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:71 Oct 13 22:55:00.448: INFO: Waiting up to 1m0s for all nodes to be ready Oct 13 22:56:00.841: INFO: Waiting for terminating namespaces to be deleted... Oct 13 22:56:00.868: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace \\'kube-system\\' to be running and ready Oct 13 22:56:00.945: INFO: 0 / 0 pods in namespace \\'kube-system\\' are running and ready (0 seconds elapsed) Oct 13 22:56:00.945: INFO: expected 0 pod replicas in namespace \\'kube-system\\', 0 are Running and Ready. [It] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:209 Oct 13 22:56:00.945: INFO: ComputeCpuMemFraction for node: ip-10-0-131-112.us-east-2.compute.internal Oct 13 22:56:00.997: INFO: Pod for on the node: tuned-ttkbd, Cpu: 10, Mem: 20971520 Oct 13 22:56:00.997: INFO: Pod for on the node: downloads-795f496c64-w8fkj, Cpu: 10, Mem: 52428800 Oct 13 22:56:00.997: INFO: Pod for on the node: dns-default-kszjp, Cpu: 110, Mem: 283115520 Oct 13 22:56:00.997: INFO: Pod for on the node: node-ca-gdcq5, Cpu: 10, Mem: 10485760 Oct 13 22:56:00.997: INFO: Pod for on the node: machine-config-daemon-w5j8c, Cpu: 20, Mem: 52428800 Oct 13 22:56:00.997: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Oct 13 22:56:00.997: INFO: Pod for on the node: grafana-649f787944-8znfk, Cpu: 200, Mem: 314572800 Oct 13 22:56:00.997: INFO: Pod for on the node: node-exporter-h68hq, Cpu: 110, Mem: 230686720 Oct 13 22:56:00.997: INFO: Pod for on the node: prometheus-adapter-b6fb584c8-877np, Cpu: 100, Mem: 209715200 Oct 13 22:56:00.997: INFO: Pod for on the node: multus-n7fm4, Cpu: 100, Mem: 209715200 Oct 13 22:56:00.997: INFO: Pod for on the node: olm-operators-kt5zd, Cpu: 100, Mem: 209715200 Oct 13 22:56:00.997: INFO: Pod for on the node: ovs-clnlv, Cpu: 200, Mem: 419430400 Oct 13 22:56:00.997: INFO: Pod for on the node: sdn-rgvn4, Cpu: 100, Mem: 209715200 Oct 13 22:56:00.997: INFO: Node: ip-10-0-131-112.us-east-2.compute.internal, totalRequestedCpuResource: 1170, cpuAllocatableMil: 3500, cpuFraction: 0.3342857142857143 Oct 13 22:56:00.997: INFO: Node: ip-10-0-131-112.us-east-2.compute.internal, totalRequestedMemResource: 2327838720, memAllocatableVal: 16181800960, memFraction: 0.14385535489864287 Oct 13 22:56:00.997: INFO: ComputeCpuMemFraction for node: ip-10-0-141-37.us-east-2.compute.internal Oct 13 22:56:01.043: INFO: Pod for on the node: tuned-bd4jd, Cpu: 10, Mem: 20971520 Oct 13 22:56:01.043: INFO: Pod for on the node: dns-default-xvq2v, Cpu: 110, Mem: 283115520 Oct 13 22:56:01.043: INFO: Pod for on the node: image-registry-7d8ddf69f5-s48h6, Cpu: 100, Mem: 268435456 Oct 13 22:56:01.043: INFO: Pod for on the node: node-ca-2xl87, Cpu: 10, Mem: 10485760 Oct 13 22:56:01.043: INFO: Pod for on the node: router-default-7cf4f88fff-hfcjh, Cpu: 100, Mem: 268435456 Oct 13 22:56:01.043: INFO: Pod for on the node: machine-config-daemon-nzbx2, Cpu: 20, Mem: 52428800 Oct 13 22:56:01.043: INFO: Pod for on the node: certified-operators-84864f8bc-l7nk8, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.043: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Oct 13 22:56:01.043: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-4qkjl, Cpu: 300, Mem: 629145600 Oct 13 22:56:01.043: INFO: Pod for on the node: node-exporter-q79sr, Cpu: 110, Mem: 230686720 Oct 13 22:56:01.043: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Oct 13 22:56:01.043: INFO: Pod for on the node: multus-plc4l, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.043: INFO: Pod for on the node: ovs-m4gxq, Cpu: 200, Mem: 419430400 Oct 13 22:56:01.043: INFO: Pod for on the node: sdn-n8flh, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.043: INFO: Node: ip-10-0-141-37.us-east-2.compute.internal, totalRequestedCpuResource: 1160, cpuAllocatableMil: 3500, cpuFraction: 0.3314285714285714 Oct 13 22:56:01.043: INFO: Node: ip-10-0-141-37.us-east-2.compute.internal, totalRequestedMemResource: 2497708032, memAllocatableVal: 16181792768, memFraction: 0.1543529859645277 Oct 13 22:56:01.043: INFO: ComputeCpuMemFraction for node: ip-10-0-157-185.us-east-2.compute.internal Oct 13 22:56:01.096: INFO: Pod for on the node: tuned-wjtqw, Cpu: 10, Mem: 20971520 Oct 13 22:56:01.096: INFO: Pod for on the node: downloads-795f496c64-s4bjk, Cpu: 10, Mem: 52428800 Oct 13 22:56:01.096: INFO: Pod for on the node: dns-default-jlm7s, Cpu: 110, Mem: 283115520 Oct 13 22:56:01.096: INFO: Pod for on the node: node-ca-d7jqn, Cpu: 10, Mem: 10485760 Oct 13 22:56:01.096: INFO: Pod for on the node: router-default-7cf4f88fff-mqlsj, Cpu: 100, Mem: 268435456 Oct 13 22:56:01.096: INFO: Pod for on the node: machine-config-daemon-wcqlt, Cpu: 20, Mem: 52428800 Oct 13 22:56:01.096: INFO: Pod for on the node: community-operators-656f6798cf-4mvzl, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.096: INFO: Pod for on the node: redhat-operators-68dffc88bb-nzxj7, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.096: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Oct 13 22:56:01.096: INFO: Pod for on the node: node-exporter-62j7m, Cpu: 110, Mem: 230686720 Oct 13 22:56:01.097: INFO: Pod for on the node: prometheus-adapter-b6fb584c8-wgl8s, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.097: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Oct 13 22:56:01.097: INFO: Pod for on the node: prometheus-operator-5d4588dd6-klcm2, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.097: INFO: Pod for on the node: telemeter-client-78b5484ddf-lm764, Cpu: 210, Mem: 440401920 Oct 13 22:56:01.097: INFO: Pod for on the node: multus-n7zg8, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.097: INFO: Pod for on the node: ovs-5rj98, Cpu: 200, Mem: 419430400 Oct 13 22:56:01.097: INFO: Pod for on the node: sdn-hvg9q, Cpu: 100, Mem: 209715200 Oct 13 22:56:01.097: INFO: Node: ip-10-0-157-185.us-east-2.compute.internal, totalRequestedCpuResource: 1280, cpuAllocatableMil: 3500, cpuFraction: 0.3657142857142857 Oct 13 22:56:01.097: INFO: Node: ip-10-0-157-185.us-east-2.compute.internal, totalRequestedMemResource: 2722103296, memAllocatableVal: 16181800960, memFraction: 0.16822004563823284 Oct 13 22:56:01.143: INFO: Waiting for running... Oct 13 22:56:11.226: INFO: Waiting for running... Oct 13 22:56:21.309: INFO: Waiting for running... STEP: Compute Cpu, Mem Fraction after create balanced pods. Oct 13 22:56:31.360: INFO: ComputeCpuMemFraction for node: ip-10-0-131-112.us-east-2.compute.internal Oct 13 22:56:31.522: INFO: Pod for on the node: 43abe79a-0da7-11eb-af38-0a58ac107a4c-0, Cpu: 580, Mem: 5763061760 Oct 13 22:56:31.522: INFO: Pod for on the node: tuned-ttkbd, Cpu: 10, Mem: 20971520 Oct 13 22:56:31.522: INFO: Pod for on the node: downloads-795f496c64-w8fkj, Cpu: 10, Mem: 52428800 Oct 13 22:56:31.522: INFO: Pod for on the node: dns-default-kszjp, Cpu: 110, Mem: 283115520 Oct 13 22:56:31.522: INFO: Pod for on the node: node-ca-gdcq5, Cpu: 10, Mem: 10485760 Oct 13 22:56:31.522: INFO: Pod for on the node: machine-config-daemon-w5j8c, Cpu: 20, Mem: 52428800 Oct 13 22:56:31.522: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Oct 13 22:56:31.522: INFO: Pod for on the node: grafana-649f787944-8znfk, Cpu: 200, Mem: 314572800 Oct 13 22:56:31.522: INFO: Pod for on the node: node-exporter-h68hq, Cpu: 110, Mem: 230686720 Oct 13 22:56:31.522: INFO: Pod for on the node: prometheus-adapter-b6fb584c8-877np, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.522: INFO: Pod for on the node: multus-n7fm4, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.522: INFO: Pod for on the node: olm-operators-kt5zd, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.522: INFO: Pod for on the node: ovs-clnlv, Cpu: 200, Mem: 419430400 Oct 13 22:56:31.522: INFO: Pod for on the node: sdn-rgvn4, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.522: INFO: Node: ip-10-0-131-112.us-east-2.compute.internal, totalRequestedCpuResource: 1750, cpuAllocatableMil: 3500, cpuFraction: 0.5 Oct 13 22:56:31.522: INFO: Node: ip-10-0-131-112.us-east-2.compute.internal, totalRequestedMemResource: 8090900480, memAllocatableVal: 16181800960, memFraction: 0.5 STEP: Compute Cpu, Mem Fraction after create balanced pods. Oct 13 22:56:31.522: INFO: ComputeCpuMemFraction for node: ip-10-0-141-37.us-east-2.compute.internal Oct 13 22:56:31.589: INFO: Pod for on the node: 49b091f9-0da7-11eb-af38-0a58ac107a4c-0, Cpu: 590, Mem: 5593188352 Oct 13 22:56:31.589: INFO: Pod for on the node: tuned-bd4jd, Cpu: 10, Mem: 20971520 Oct 13 22:56:31.589: INFO: Pod for on the node: dns-default-xvq2v, Cpu: 110, Mem: 283115520 Oct 13 22:56:31.589: INFO: Pod for on the node: image-registry-7d8ddf69f5-s48h6, Cpu: 100, Mem: 268435456 Oct 13 22:56:31.589: INFO: Pod for on the node: node-ca-2xl87, Cpu: 10, Mem: 10485760 Oct 13 22:56:31.589: INFO: Pod for on the node: router-default-7cf4f88fff-hfcjh, Cpu: 100, Mem: 268435456 Oct 13 22:56:31.589: INFO: Pod for on the node: machine-config-daemon-nzbx2, Cpu: 20, Mem: 52428800 Oct 13 22:56:31.589: INFO: Pod for on the node: certified-operators-84864f8bc-l7nk8, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.589: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Oct 13 22:56:31.589: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-4qkjl, Cpu: 300, Mem: 629145600 Oct 13 22:56:31.589: INFO: Pod for on the node: node-exporter-q79sr, Cpu: 110, Mem: 230686720 Oct 13 22:56:31.589: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Oct 13 22:56:31.589: INFO: Pod for on the node: multus-plc4l, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.589: INFO: Pod for on the node: ovs-m4gxq, Cpu: 200, Mem: 419430400 Oct 13 22:56:31.589: INFO: Pod for on the node: sdn-n8flh, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.589: INFO: Node: ip-10-0-141-37.us-east-2.compute.internal, totalRequestedCpuResource: 1750, cpuAllocatableMil: 3500, cpuFraction: 0.5 Oct 13 22:56:31.589: INFO: Node: ip-10-0-141-37.us-east-2.compute.internal, totalRequestedMemResource: 8090896384, memAllocatableVal: 16181792768, memFraction: 0.5 STEP: Compute Cpu, Mem Fraction after create balanced pods. Oct 13 22:56:31.589: INFO: ComputeCpuMemFraction for node: ip-10-0-157-185.us-east-2.compute.internal Oct 13 22:56:31.635: INFO: Pod for on the node: 4fb330dc-0da7-11eb-af38-0a58ac107a4c-0, Cpu: 470, Mem: 5368797184 Oct 13 22:56:31.635: INFO: Pod for on the node: tuned-wjtqw, Cpu: 10, Mem: 20971520 Oct 13 22:56:31.635: INFO: Pod for on the node: downloads-795f496c64-s4bjk, Cpu: 10, Mem: 52428800 Oct 13 22:56:31.635: INFO: Pod for on the node: dns-default-jlm7s, Cpu: 110, Mem: 283115520 Oct 13 22:56:31.635: INFO: Pod for on the node: node-ca-d7jqn, Cpu: 10, Mem: 10485760 Oct 13 22:56:31.635: INFO: Pod for on the node: router-default-7cf4f88fff-mqlsj, Cpu: 100, Mem: 268435456 Oct 13 22:56:31.635: INFO: Pod for on the node: machine-config-daemon-wcqlt, Cpu: 20, Mem: 52428800 Oct 13 22:56:31.635: INFO: Pod for on the node: community-operators-656f6798cf-4mvzl, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.635: INFO: Pod for on the node: redhat-operators-68dffc88bb-nzxj7, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.635: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Oct 13 22:56:31.635: INFO: Pod for on the node: node-exporter-62j7m, Cpu: 110, Mem: 230686720 Oct 13 22:56:31.635: INFO: Pod for on the node: prometheus-adapter-b6fb584c8-wgl8s, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.635: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Oct 13 22:56:31.635: INFO: Pod for on the node: prometheus-operator-5d4588dd6-klcm2, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.635: INFO: Pod for on the node: telemeter-client-78b5484ddf-lm764, Cpu: 210, Mem: 440401920 Oct 13 22:56:31.635: INFO: Pod for on the node: multus-n7zg8, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.635: INFO: Pod for on the node: ovs-5rj98, Cpu: 200, Mem: 419430400 Oct 13 22:56:31.635: INFO: Pod for on the node: sdn-hvg9q, Cpu: 100, Mem: 209715200 Oct 13 22:56:31.635: INFO: Node: ip-10-0-157-185.us-east-2.compute.internal, totalRequestedCpuResource: 1750, cpuAllocatableMil: 3500, cpuFraction: 0.5 Oct 13 22:56:31.635: INFO: Node: ip-10-0-157-185.us-east-2.compute.internal, totalRequestedMemResource: 8090900480, memAllocatableVal: 16181800960, memFraction: 0.5 STEP: Trying to apply 10 taint on the nodes except first one. STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-55dfaa88-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-55dfaaa1-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-55ebd6cc-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-55ebd701-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-55f7c123-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-55f7c159-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5603d0a3-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5603d0ff-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-560fd502-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-560fd544-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-561bec54-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-561bec90-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56286285-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-562862b3-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56347606-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5634763f-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56408805-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56408839-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-564c8cea-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-564c8d25-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-565918ca-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-565918fb-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-566525d4-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5665260c-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56712fae-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56712fe8-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-567daa3d-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-567daa78-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5689b24f-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5689b287-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5695ac8e-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5695acc4-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56a2252d-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56a22561-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56ae8709-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56ae873c-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56baabbb-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56baabf2-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-56c6e82d-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56c6e865-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: Create a pod without any tolerations STEP: Pod should prefer scheduled to the node don\\'t have the taint. STEP: Trying to apply 10 taint on the first node. STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5cda1465-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5cda147f-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5ce60e2f-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5ce60e66-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5cf21790-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5cf217c0-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5cfe333f-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5cfe3413-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5d0a6153-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d0a618c-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5d1676d7-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d167709-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5d228235-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d228264-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5d2ea2cd-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d2ea2fa-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5d3aafcd-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d3aaff9-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node has the taint kubernetes.io/e2e-taint-key-5d46ca82-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d46cab3-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: Create a pod that tolerates all the taints of the first node. STEP: Pod should prefer scheduled to the node that pod can tolerate. STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-5d46ca82-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d46cab3-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-5d3aafcd-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d3aaff9-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-5d2ea2cd-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d2ea2fa-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-5d228235-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d228264-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-5d1676d7-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d167709-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-5d0a6153-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5d0a618c-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-5cfe333f-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5cfe3413-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-5cf21790-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5cf217c0-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-5ce60e2f-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5ce60e66-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-5cda1465-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5cda147f-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-56c6e82d-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56c6e865-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-56baabbb-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56baabf2-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-56ae8709-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56ae873c-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-56a2252d-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56a22561-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-5695ac8e-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5695acc4-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-5689b24f-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5689b287-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-567daa3d-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-567daa78-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-56712fae-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56712fe8-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-566525d4-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5665260c-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-565918ca-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-565918fb-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-564c8cea-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-564c8d25-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-56408805-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-56408839-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-56347606-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5634763f-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-56286285-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-562862b3-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-561bec54-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-561bec90-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-560fd502-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-560fd544-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-5603d0a3-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-5603d0ff-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-55f7c123-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-55f7c159-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-55ebd6cc-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-55ebd701-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule STEP: verifying the node doesn\\'t have the taint kubernetes.io/e2e-taint-key-55dfaa88-0da7-11eb-af38-0a58ac107a4c=testing-taint-value-55dfaaa1-0da7-11eb-af38-0a58ac107a4c:PreferNoSchedule [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:154 Oct 13 22:56:56.621: INFO: Waiting up to 3m0s for all (but 100) nodes to be ready STEP: Destroying namespace \"e2e-tests-sched-priority-6bh77\" for this suite. Oct 13 23:06:56.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Oct 13 23:07:57.862: INFO: Couldn\\'t delete ns: \"e2e-tests-sched-priority-6bh77\": the server was unable to return a response in the time allotted, but may still be processing the request (&errors.StatusError{ErrStatus:v1.Status{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ListMeta:v1.ListMeta{SelfLink:\"\", ResourceVersion:\"\", Continue:\"\"}, Status:\"Failure\", Message:\"the server was unable to return a response in the time allotted, but may still be processing the request\", Reason:\"Timeout\", Details:(*v1.StatusDetails)(0xc001efd020), Code:504}}) [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:68 Oct 13 23:07:57.867: INFO: Running AfterSuite actions on all nodes Oct 13 23:07:57.867: INFO: Running AfterSuite actions on node 1 fail [k8s.io/kubernetes/test/e2e/framework/framework.go:345]: Oct 13 23:07:57.862: Couldn\\'t delete ns: \"e2e-tests-sched-priority-6bh77\": the server was unable to return a response in the time allotted, but may still be processing the request (&errors.StatusError{ErrStatus:v1.Status{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ListMeta:v1.ListMeta{SelfLink:\"\", ResourceVersion:\"\", Continue:\"\"}, Status:\"Failure\", Message:\"the server was unable to return a response in the time allotted, but may still be processing the request\", Reason:\"Timeout\", Details:(*v1.StatusDetails)(0xc001efd020), Code:504}}) Oct 13 22:58:02.943 W clusteroperator/console changed Progressing to True: ProgressingSynchronizationError: Progressing: oauth client for console does not exist and cannot be created Oct 13 22:58:02.952 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"\" to \"Degraded: oauth client for console does not exist and cannot be created\",Progressing changed from False to True (\"Progressing: oauth client for console does not exist and cannot be created\") Oct 13 22:59:02.975 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"Degraded: oauth client for console does not exist and cannot be created\" to \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\",Progressing message changed from \"Progressing: oauth client for console does not exist and cannot be created\" to \"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\" Oct 13 22:59:40.168 W clusteroperator/openshift-apiserver changed Available to False: AvailableMultiple: Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.authorization.openshift.io is not ready: 0\\ Available: v1.oauth.openshift.io is not ready: 0\\ Available: v1.project.openshift.io is not ready: 0\\ Available: v1.security.openshift.io is not ready: 0 Oct 13 22:59:40.178 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available changed from True to False (\"Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.authorization.openshift.io is not ready: 0\\ Available: v1.oauth.openshift.io is not ready: 0\\ Available: v1.project.openshift.io is not ready: 0\\ Available: v1.security.openshift.io is not ready: 0\") Oct 13 23:00:01.369 E clusteroperator/monitoring changed Degraded to True: UpdatingconfigurationsharingFailed: Failed to rollout the stack. Error: running task Updating configuration sharing failed: failed to retrieve Grafana host: getting Route object failed: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io grafana) Oct 13 23:00:01.369 W clusteroperator/monitoring changed Available to False Oct 13 23:00:01.390 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:00:03.023 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\" to \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\",Progressing message changed from \"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\" to \"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\" Oct 13 23:00:03.097 W clusteroperator/console changed Progressing to False Oct 13 23:00:03.101 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\" to \"\",Progressing changed from True to False (\"\") Oct 13 23:00:20.262 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \"\" to \"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\" Oct 13 23:01:03.117 W clusteroperator/console changed Progressing to True: ProgressingSynchronizationError: Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console) Oct 13 23:01:03.125 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"\" to \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\",Progressing changed from False to True (\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\") Oct 13 23:01:09.039 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (9 times) Oct 13 23:01:10.012 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (6 times) Oct 13 23:01:11.011 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (6 times) Oct 13 23:01:12.176 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (6 times) Oct 13 23:01:12.369 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (9 times) Oct 13 23:01:12.508 W clusteroperator/monitoring changed Progressing to False Oct 13 23:01:12.531 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:01:12.574 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (9 times) Oct 13 23:01:21.492 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\" to \"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" Oct 13 23:02:03.170 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\" to \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\",Progressing message changed from \"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\" to \"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\" (2 times) Oct 13 23:02:18.622 W clusteroperator/monitoring changed Progressing to False Oct 13 23:02:18.642 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:02:21.528 E clusteroperator/authentication changed Degraded to True: MultipleConditionsMatching: RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client) Oct 13 23:02:21.533 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from False to True (\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\") Oct 13 23:02:52.456 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \"Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.authorization.openshift.io is not ready: 0\\ Available: v1.oauth.openshift.io is not ready: 0\\ Available: v1.project.openshift.io is not ready: 0\\ Available: v1.security.openshift.io is not ready: 0\" to \"Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.build.openshift.io is not ready: 0\\ Available: v1.quota.openshift.io is not ready: 0\\ Available: v1.route.openshift.io is not ready: 0\\ Available: v1.security.openshift.io is not ready: 0\\ Available: v1.template.openshift.io is not ready: 0\" Oct 13 23:03:03.212 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\" to \"Degraded: oauth client for console does not exist and cannot be created\",Progressing message changed from \"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\" to \"Progressing: oauth client for console does not exist and cannot be created\" Oct 13 23:03:22.744 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" to \"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" Oct 13 23:03:23.767 W clusteroperator/monitoring changed Progressing to False Oct 13 23:03:23.788 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:04:22.761 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" to \"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" Oct 13 23:04:44.727 W clusteroperator/monitoring changed Progressing to False Oct 13 23:04:44.747 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:05:23.991 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" to \"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" (2 times) Oct 13 23:05:25.224 W clusteroperator/authentication changed Degraded to False Oct 13 23:05:25.229 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from True to False (\"\") Oct 13 23:05:32.733 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \"Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.build.openshift.io is not ready: 0\\ Available: v1.quota.openshift.io is not ready: 0\\ Available: v1.route.openshift.io is not ready: 0\\ Available: v1.security.openshift.io is not ready: 0\\ Available: v1.template.openshift.io is not ready: 0\" to \"Available: v1.authorization.openshift.io is not ready: 0\\ Available: v1.build.openshift.io is not ready: 0\\ Available: v1.security.openshift.io is not ready: 0\\ Available: v1.template.openshift.io is not ready: 0\\ Available: v1.user.openshift.io is not ready: 0\" Oct 13 23:06:47.587 W clusteroperator/monitoring changed Progressing to False Oct 13 23:06:47.610 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:07:30.608 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \"\" to \"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" failed: (13m1s) 2020-10-13T23:07:57 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/9/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (4m49s) 2020-10-13T23:12:46 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/10/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (30.8s) 2020-10-13T23:13:17 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/11/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (12.8s) 2020-10-13T23:13:30 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/12/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m23s) 2020-10-13T23:14:52 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/13/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-10-13T23:15:21 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/14/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T23:15:50 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/15/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (31.3s) 2020-10-13T23:16:21 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/16/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (15.3s) 2020-10-13T23:16:36 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/17/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (49.3s) 2020-10-13T23:17:26 \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/18/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m43s) 2020-10-13T23:20:09 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/19/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m27s) 2020-10-13T23:22:36 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/20/79) \"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (58.1s) 2020-10-13T23:23:34 \"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/21/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (38.1s) 2020-10-13T23:24:12 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/22/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-10-13T23:24:41 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/23/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.1s) 2020-10-13T23:24:55 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/24/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.3s) 2020-10-13T23:25:09 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/25/79) \"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m47s) 2020-10-13T23:26:56 \"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/26/79) \"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m2s) 2020-10-13T23:27:58 \"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/27/79) \"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\" passed: (53.5s) 2020-10-13T23:28:52 \"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\" started: (1/28/79) \"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (11.5s) 2020-10-13T23:29:03 \"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/29/79) \"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m32s) 2020-10-13T23:30:35 \"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/30/79) \"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\" passed: (29.2s) 2020-10-13T23:31:04 \"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\" started: (1/31/79) \"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (11.9s) 2020-10-13T23:31:16 \"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/32/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-10-13T23:31:45 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/33/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m32s) 2020-10-13T23:33:17 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/34/79) \"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (54s) 2020-10-13T23:34:11 \"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/35/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m33s) 2020-10-13T23:36:44 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/36/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T23:37:13 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/37/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m53s) 2020-10-13T23:39:06 \"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/38/79) \"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:193]: Could not get controller-manager metrics - skipping skipped: (53.7s) 2020-10-13T23:40:00 \"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/39/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (32.1s) 2020-10-13T23:40:32 \"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/40/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.1s) 2020-10-13T23:40:46 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/41/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T23:41:15 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/42/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.3s) 2020-10-13T23:41:29 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/43/79) \"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m10s) 2020-10-13T23:42:39 \"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/44/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29s) 2020-10-13T23:43:08 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/45/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-10-13T23:43:36 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/46/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-10-13T23:44:05 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/47/79) \"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (11.7s) 2020-10-13T23:44:17 \"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/48/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m15s) 2020-10-13T23:46:32 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/49/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T23:47:01 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/50/79) \"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (11.5s) 2020-10-13T23:47:12 \"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/51/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.7s) 2020-10-13T23:47:41 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/52/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (26.6s) 2020-10-13T23:48:08 \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/53/79) \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (25.1s) 2020-10-13T23:48:33 \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/54/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (12.8s) 2020-10-13T23:48:46 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/55/79) \"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m47s) 2020-10-13T23:50:32 \"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/56/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m53s) 2020-10-13T23:52:25 \"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/57/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m4s) 2020-10-13T23:53:29 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/58/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (23.7s) 2020-10-13T23:53:52 \"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/59/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T23:54:21 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/60/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-13T23:54:50 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/61/79) \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m46s) 2020-10-13T23:56:36 \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/62/79) \"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m52s) 2020-10-13T23:58:28 \"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/63/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.1s) 2020-10-13T23:58:58 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/64/79) \"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:81]: Environment does not support getting controller-manager metrics - skipping skipped: (11.6s) 2020-10-13T23:59:09 \"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/65/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.2s) 2020-10-13T23:59:23 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/66/79) \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m35s) 2020-10-14T00:01:59 \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/67/79) \"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (27.1s) 2020-10-14T00:02:26 \"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/68/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.5s) 2020-10-14T00:02:40 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/69/79) \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (21.7s) 2020-10-14T00:03:02 \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/70/79) \"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/scheduling/taint_based_evictions.go:63]: Requires at least 2 nodes (not -1) skipped: (11.8s) 2020-10-14T00:03:14 \"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/71/79) \"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:247]: Could not get controller-manager metrics - skipping skipped: (19.7s) 2020-10-14T00:03:34 \"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/72/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.5s) 2020-10-14T00:03:48 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/73/79) \"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (11.8s) 2020-10-14T00:04:00 \"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/74/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-10-14T00:04:29 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/75/79) \"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (57.8s) 2020-10-14T00:05:26 \"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/76/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (31.2s) 2020-10-14T00:05:58 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/77/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m49s) 2020-10-14T00:07:47 \"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/78/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.2s) 2020-10-14T00:08:16 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/79/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.9s) 2020-10-14T00:08:45 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" Timeline: Oct 13 22:51:09.831 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (5 times) Oct 13 22:51:10.678 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (5 times) Oct 13 22:51:11.532 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (5 times) Oct 13 22:51:11.827 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (8 times) Oct 13 22:51:11.992 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (8 times) Oct 13 22:51:12.185 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (8 times) Oct 13 22:54:57.400 - 780s I test=\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" running Oct 13 22:58:02.943 W clusteroperator/console changed Progressing to True: ProgressingSynchronizationError: Progressing: oauth client for console does not exist and cannot be created Oct 13 22:58:02.952 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"\" to \"Degraded: oauth client for console does not exist and cannot be created\",Progressing changed from False to True (\"Progressing: oauth client for console does not exist and cannot be created\") Oct 13 22:59:02.975 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"Degraded: oauth client for console does not exist and cannot be created\" to \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\",Progressing message changed from \"Progressing: oauth client for console does not exist and cannot be created\" to \"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\" Oct 13 22:59:40.168 W clusteroperator/openshift-apiserver changed Available to False: AvailableMultiple: Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.authorization.openshift.io is not ready: 0\\ Available: v1.oauth.openshift.io is not ready: 0\\ Available: v1.project.openshift.io is not ready: 0\\ Available: v1.security.openshift.io is not ready: 0 Oct 13 22:59:40.178 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available changed from True to False (\"Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.authorization.openshift.io is not ready: 0\\ Available: v1.oauth.openshift.io is not ready: 0\\ Available: v1.project.openshift.io is not ready: 0\\ Available: v1.security.openshift.io is not ready: 0\") Oct 13 23:00:01.369 W clusteroperator/monitoring changed Available to False Oct 13 23:00:01.369 E clusteroperator/monitoring changed Degraded to True: UpdatingconfigurationsharingFailed: Failed to rollout the stack. Error: running task Updating configuration sharing failed: failed to retrieve Grafana host: getting Route object failed: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io grafana) Oct 13 23:00:01.390 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:00:03.023 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\" to \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\",Progressing message changed from \"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\" to \"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\" Oct 13 23:00:03.097 W clusteroperator/console changed Progressing to False Oct 13 23:00:03.101 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\" to \"\",Progressing changed from True to False (\"\") Oct 13 23:00:20.262 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \"\" to \"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\" Oct 13 23:01:03.117 W clusteroperator/console changed Progressing to True: ProgressingSynchronizationError: Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console) Oct 13 23:01:03.125 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"\" to \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\",Progressing changed from False to True (\"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\") Oct 13 23:01:09.039 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (9 times) Oct 13 23:01:10.012 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (6 times) Oct 13 23:01:11.011 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (6 times) Oct 13 23:01:12.176 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (6 times) Oct 13 23:01:12.369 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (9 times) Oct 13 23:01:12.508 W clusteroperator/monitoring changed Progressing to False Oct 13 23:01:12.531 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:01:12.574 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (9 times) Oct 13 23:01:21.492 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\" to \"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" Oct 13 23:02:03.170 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\" to \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\",Progressing message changed from \"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io console)\" to \"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\" (2 times) Oct 13 23:02:18.622 W clusteroperator/monitoring changed Progressing to False Oct 13 23:02:18.642 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:02:21.528 E clusteroperator/authentication changed Degraded to True: MultipleConditionsMatching: RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client) Oct 13 23:02:21.533 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from False to True (\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\") Oct 13 23:02:52.456 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \"Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.authorization.openshift.io is not ready: 0\\ Available: v1.oauth.openshift.io is not ready: 0\\ Available: v1.project.openshift.io is not ready: 0\\ Available: v1.security.openshift.io is not ready: 0\" to \"Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.build.openshift.io is not ready: 0\\ Available: v1.quota.openshift.io is not ready: 0\\ Available: v1.route.openshift.io is not ready: 0\\ Available: v1.security.openshift.io is not ready: 0\\ Available: v1.template.openshift.io is not ready: 0\" Oct 13 23:03:03.212 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\" to \"Degraded: oauth client for console does not exist and cannot be created\",Progressing message changed from \"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\" to \"Progressing: oauth client for console does not exist and cannot be created\" Oct 13 23:03:22.744 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" to \"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" Oct 13 23:03:23.767 W clusteroperator/monitoring changed Progressing to False Oct 13 23:03:23.788 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:04:22.761 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" to \"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" Oct 13 23:04:44.727 W clusteroperator/monitoring changed Progressing to False Oct 13 23:04:44.747 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:05:23.991 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" to \"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" (2 times) Oct 13 23:05:25.224 W clusteroperator/authentication changed Degraded to False Oct 13 23:05:25.229 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from True to False (\"\") Oct 13 23:05:32.733 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \"Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.build.openshift.io is not ready: 0\\ Available: v1.quota.openshift.io is not ready: 0\\ Available: v1.route.openshift.io is not ready: 0\\ Available: v1.security.openshift.io is not ready: 0\\ Available: v1.template.openshift.io is not ready: 0\" to \"Available: v1.authorization.openshift.io is not ready: 0\\ Available: v1.build.openshift.io is not ready: 0\\ Available: v1.security.openshift.io is not ready: 0\\ Available: v1.template.openshift.io is not ready: 0\\ Available: v1.user.openshift.io is not ready: 0\" Oct 13 23:06:47.587 W clusteroperator/monitoring changed Progressing to False Oct 13 23:06:47.610 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:07:30.608 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \"\" to \"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\" Oct 13 23:07:57.886 I test=\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" failed Oct 13 23:08:05.116 W clusteroperator/monitoring changed Progressing to False Oct 13 23:08:05.139 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:08:13.006 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \"Available: v1.authorization.openshift.io is not ready: 0\\ Available: v1.build.openshift.io is not ready: 0\\ Available: v1.security.openshift.io is not ready: 0\\ Available: v1.template.openshift.io is not ready: 0\\ Available: v1.user.openshift.io is not ready: 0\" to \"Available: v1.build.openshift.io is not ready: 0\\ Available: v1.image.openshift.io is not ready: 0\\ Available: v1.oauth.openshift.io is not ready: 0\\ Available: v1.quota.openshift.io is not ready: 0\\ Available: v1.route.openshift.io is not ready: 0\" Oct 13 23:08:30.618 E clusteroperator/authentication changed Degraded to True: MultipleConditionsMatching: RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client) Oct 13 23:08:30.626 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from False to True (\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-browser-client)\") (2 times) Oct 13 23:09:17.292 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \"Available: v1.build.openshift.io is not ready: 0\\ Available: v1.image.openshift.io is not ready: 0\\ Available: v1.oauth.openshift.io is not ready: 0\\ Available: v1.quota.openshift.io is not ready: 0\\ Available: v1.route.openshift.io is not ready: 0\" to \"Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.oauth.openshift.io is not ready: 0\" Oct 13 23:09:31.682 W clusteroperator/authentication changed Degraded to False Oct 13 23:09:31.690 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from True to False (\"\") (2 times) Oct 13 23:10:03.456 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"Degraded: oauth client for console does not exist and cannot be created\" to \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\",Progressing message changed from \"Progressing: oauth client for console does not exist and cannot be created\" to \"Progressing: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\" Oct 13 23:10:08.177 W clusteroperator/monitoring changed Progressing to False Oct 13 23:10:08.198 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:10:21.567 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \"Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.oauth.openshift.io is not ready: 0\" to \"Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.template.openshift.io is not ready: 0\" Oct 13 23:10:37.884 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded message changed from \"\" to \"OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-challenging-client)\" Oct 13 23:11:03.546 W clusteroperator/console changed Progressing to False Oct 13 23:11:03.555 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"Degraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io console)\" to \"\",Progressing changed from True to False (\"\") (2 times) Oct 13 23:11:09.005 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (10 times) Oct 13 23:11:09.363 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (10 times) Oct 13 23:11:10.443 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (7 times) Oct 13 23:11:11.408 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (7 times) Oct 13 23:11:12.478 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (7 times) Oct 13 23:11:12.656 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (10 times) Oct 13 23:11:19.322 W clusteroperator/monitoring changed Progressing to False Oct 13 23:11:19.344 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:11:37.891 E clusteroperator/authentication changed Degraded to True: MultipleConditionsMatching: RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-challenging-client) Oct 13 23:11:37.898 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from False to True (\"RouteStatusDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get routes.route.openshift.io oauth-openshift)\\ OAuthClientsDegraded: the server was unable to return a response in the time allotted, but may still be processing the request (get oauthclients.oauth.openshift.io openshift-challenging-client)\") Oct 13 23:12:03.713 W clusteroperator/console changed Progressing to True: ProgressingSynchronizationError: Progressing: oauth client for console does not exist and cannot be created Oct 13 23:12:03.723 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"\" to \"Degraded: oauth client for console does not exist and cannot be created\",Progressing changed from False to True (\"Progressing: oauth client for console does not exist and cannot be created\") (2 times) Oct 13 23:12:03.775 W clusteroperator/console changed Progressing to False Oct 13 23:12:03.781 I ns/openshift-console-operator deployment/console-operator Status for clusteroperator/console changed: Degraded message changed from \"Degraded: oauth client for console does not exist and cannot be created\" to \"\",Progressing changed from True to False (\"\") Oct 13 23:12:29.877 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available message changed from \"Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.template.openshift.io is not ready: 0\" to \"Available: v1.apps.openshift.io is not ready: 0\\ Available: v1.authorization.openshift.io is not ready: 0\\ Available: v1.project.openshift.io is not ready: 0\\ Available: v1.route.openshift.io is not ready: 0\" Oct 13 23:12:30.357 W clusteroperator/openshift-apiserver changed Available to True Oct 13 23:12:30.374 I ns/openshift-apiserver-operator deployment/openshift-apiserver-operator Status for clusteroperator/openshift-apiserver changed: Available changed from False to True (\"\") (3 times) Oct 13 23:12:30.773 W clusteroperator/authentication changed Degraded to False Oct 13 23:12:30.791 I ns/openshift-authentication-operator deployment/authentication-operator Status for clusteroperator/authentication changed: Degraded changed from True to False (\"\") (3 times) Oct 13 23:12:32.204 W ns/openshift-machine-config-operator pod/machine-config-daemon-w5j8c node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 13 23:12:32.216 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-w5j8c Oct 13 23:12:32.216 I ns/openshift-machine-config-operator pod/machine-config-daemon-w5j8c Stopping container machine-config-daemon Oct 13 23:12:32.216 W ns/openshift-image-registry pod/node-ca-gdcq5 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:12:32.216 I ns/openshift-image-registry pod/node-ca-gdcq5 Stopping container node-ca Oct 13 23:12:32.226 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-gdcq5 Oct 13 23:12:32.235 I ns/openshift-console pod/downloads-795f496c64-w8fkj Marking for deletion Pod openshift-console/downloads-795f496c64-w8fkj Oct 13 23:12:32.239 W ns/openshift-console pod/downloads-795f496c64-w8fkj node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:12:32.243 I ns/openshift-image-registry pod/node-ca-gdcq5 Marking for deletion Pod openshift-image-registry/node-ca-gdcq5 Oct 13 23:12:32.253 W ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:12:32.253 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np Marking for deletion Pod openshift-monitoring/prometheus-adapter-b6fb584c8-877np Oct 13 23:12:32.253 W ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:12:32.254 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 0s Oct 13 23:12:32.256 I ns/openshift-console pod/downloads-795f496c64-w8fkj Stopping container download-server Oct 13 23:12:32.263 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:12:32.274 I ns/openshift-monitoring pod/alertmanager-main-0 Marking for deletion Pod openshift-monitoring/alertmanager-main-0 Oct 13 23:12:32.274 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np Stopping container prometheus-adapter Oct 13 23:12:32.283 I ns/openshift-monitoring pod/grafana-649f787944-8znfk Stopping container grafana-proxy Oct 13 23:12:32.283 I ns/openshift-machine-config-operator pod/machine-config-daemon-w5j8c Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-w5j8c Oct 13 23:12:32.287 I ns/openshift-monitoring pod/grafana-649f787944-8znfk Stopping container grafana Oct 13 23:12:32.289 I ns/openshift-monitoring pod/grafana-649f787944-8znfk Marking for deletion Pod openshift-monitoring/grafana-649f787944-8znfk Oct 13 23:12:32.295 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy Oct 13 23:12:32.297 I ns/openshift-monitoring pod/alertmanager-main-0 Cancelling deletion of Pod openshift-monitoring/alertmanager-main-0 Oct 13 23:12:32.300 I ns/openshift-console pod/downloads-795f496c64-97mxr node/ created Oct 13 23:12:32.305 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader Oct 13 23:12:32.311 I ns/openshift-console replicaset/downloads-795f496c64 Created pod: downloads-795f496c64-97mxr Oct 13 23:12:32.315 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager Oct 13 23:12:32.318 I ns/openshift-console pod/downloads-795f496c64-97mxr Successfully assigned openshift-console/downloads-795f496c64-97mxr to ip-10-0-141-37.us-east-2.compute.internal Oct 13 23:12:32.319 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-94mld node/ created Oct 13 23:12:32.331 I ns/openshift-monitoring replicaset/prometheus-adapter-b6fb584c8 Created pod: prometheus-adapter-b6fb584c8-94mld Oct 13 23:12:32.333 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy (2 times) Oct 13 23:12:32.346 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-94mld Successfully assigned openshift-monitoring/prometheus-adapter-b6fb584c8-94mld to ip-10-0-141-37.us-east-2.compute.internal Oct 13 23:12:32.363 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 node/ created Oct 13 23:12:32.368 I ns/openshift-monitoring replicaset/grafana-649f787944 Created pod: grafana-649f787944-jm8n9 Oct 13 23:12:32.392 I ns/openshift-monitoring pod/alertmanager-main-0 node/ created Oct 13 23:12:32.397 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Successfully assigned openshift-monitoring/grafana-649f787944-jm8n9 to ip-10-0-141-37.us-east-2.compute.internal Oct 13 23:12:32.410 I ns/openshift-monitoring statefulset/alertmanager-main create Pod alertmanager-main-0 in StatefulSet alertmanager-main successful (2 times) Oct 13 23:12:32.410 I ns/openshift-monitoring pod/alertmanager-main-0 Successfully assigned openshift-monitoring/alertmanager-main-0 to ip-10-0-157-185.us-east-2.compute.internal Oct 13 23:12:32.431 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader (2 times) Oct 13 23:12:32.612 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager (2 times) Oct 13 23:12:34.015 W ns/openshift-image-registry pod/node-ca-gdcq5 node/ip-10-0-131-112.us-east-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 13 23:12:34.015 W ns/openshift-image-registry pod/node-ca-gdcq5 node/ip-10-0-131-112.us-east-2.compute.internal container=node-ca container stopped being ready Oct 13 23:12:34.329 W ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np node/ip-10-0-131-112.us-east-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 13 23:12:34.329 W ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np node/ip-10-0-131-112.us-east-2.compute.internal container=prometheus-adapter container stopped being ready Oct 13 23:12:35.130 W ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 13 23:12:35.130 W ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal container=grafana-proxy container stopped being ready Oct 13 23:12:35.130 W ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal container=grafana container stopped being ready Oct 13 23:12:35.532 E ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal container=grafana container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 13 23:12:35.532 E ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal container=grafana-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 13 23:12:36.533 W ns/openshift-image-registry pod/node-ca-gdcq5 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:12:37.026 W clusteroperator/monitoring changed Progressing to False Oct 13 23:12:37.048 W clusteroperator/monitoring changed Progressing to True: RollOutInProgress: Rolling out the stack. Oct 13 23:12:37.844 W ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np node/ip-10-0-131-112.us-east-2.compute.internal pod has been pending longer than a minute Oct 13 23:12:37.844 W ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal pod has been pending longer than a minute Oct 13 23:12:38.126 W ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-877np node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:12:38.225 I ns/openshift-image-registry pod/node-ca-gb5x2 node/ created Oct 13 23:12:38.232 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-gb5x2 Oct 13 23:12:38.235 I ns/openshift-image-registry pod/node-ca-gb5x2 Successfully assigned openshift-image-registry/node-ca-gb5x2 to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:12:38.540 W ns/openshift-machine-config-operator pod/machine-config-daemon-w5j8c node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:12:38.549 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 node/ created Oct 13 23:12:38.554 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-lghv9 Oct 13 23:12:38.560 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 Successfully assigned openshift-machine-config-operator/machine-config-daemon-lghv9 to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:12:39.851 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b9d07e8eed67a34b50a3afaa36cd4b9f052697894e64b5c203d0675d12ab5ec\" already present on machine Oct 13 23:12:40.031 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager Oct 13 23:12:40.062 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager Oct 13 23:12:40.067 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Oct 13 23:12:40.135 W ns/openshift-monitoring pod/grafana-649f787944-8znfk node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:12:40.253 I ns/openshift-monitoring pod/alertmanager-main-0 Created container config-reloader Oct 13 23:12:40.277 I ns/openshift-monitoring pod/alertmanager-main-0 Started container config-reloader Oct 13 23:12:40.285 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Oct 13 23:12:40.300 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 13 23:12:40.414 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 Created container machine-config-daemon Oct 13 23:12:40.438 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 Started container machine-config-daemon Oct 13 23:12:40.448 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager-proxy Oct 13 23:12:40.477 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager-proxy Oct 13 23:12:42.662 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-94mld Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\" Oct 13 23:12:43.396 I ns/openshift-console pod/downloads-795f496c64-97mxr Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e7c14a3e743a80bf16d36154e823343f7d50f54dbcb914daab6013381bad8fc5\" already present on machine Oct 13 23:12:43.543 I ns/openshift-console pod/downloads-795f496c64-97mxr Created container download-server Oct 13 23:12:43.568 I ns/openshift-console pod/downloads-795f496c64-97mxr Started container download-server Oct 13 23:12:43.771 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c229ac6674c217359c243e4621aae565c2af46caaf231a077b7f7e3d6f07ef5a\" Oct 13 23:12:45.079 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-94mld Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\" Oct 13 23:12:45.223 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-94mld Created container prometheus-adapter Oct 13 23:12:45.245 I ns/openshift-monitoring pod/prometheus-adapter-b6fb584c8-94mld Started container prometheus-adapter Oct 13 23:12:47.782 I ns/openshift-image-registry pod/node-ca-gb5x2 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 13 23:12:47.907 I ns/openshift-image-registry pod/node-ca-gb5x2 Created container node-ca Oct 13 23:12:47.931 I ns/openshift-image-registry pod/node-ca-gb5x2 Started container node-ca Oct 13 23:12:49.666 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c229ac6674c217359c243e4621aae565c2af46caaf231a077b7f7e3d6f07ef5a\" Oct 13 23:12:49.845 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Created container grafana Oct 13 23:12:49.868 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Started container grafana Oct 13 23:12:49.875 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Oct 13 23:12:50.018 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Created container grafana-proxy Oct 13 23:12:50.047 I ns/openshift-monitoring pod/grafana-649f787944-jm8n9 Started container grafana-proxy Oct 13 23:13:02.466 W clusteroperator/monitoring changed Available to True: RollOutDone: Successfully rolled out the stack. Oct 13 23:13:02.466 W clusteroperator/monitoring changed Degraded to False Oct 13 23:13:02.466 W clusteroperator/monitoring changed Progressing to False Oct 13 23:13:03.079 E ns/openshift-console pod/downloads-795f496c64-w8fkj node/ip-10-0-131-112.us-east-2.compute.internal container=download-server container exited with code 137 (Error): Oct 13 23:13:08.126 W ns/openshift-console pod/downloads-795f496c64-w8fkj node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:18:30.044 W ns/openshift-image-registry pod/node-ca-gb5x2 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:18:30.045 W ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 13 23:18:30.057 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-gb5x2 Oct 13 23:18:30.057 I ns/openshift-image-registry pod/node-ca-gb5x2 Stopping container node-ca Oct 13 23:18:30.057 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-lghv9 Oct 13 23:18:30.057 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 Stopping container machine-config-daemon Oct 13 23:18:30.063 I ns/openshift-image-registry pod/node-ca-gb5x2 Marking for deletion Pod openshift-image-registry/node-ca-gb5x2 Oct 13 23:18:30.067 I ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-lghv9 Oct 13 23:18:38.128 W ns/openshift-machine-config-operator pod/machine-config-daemon-lghv9 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:18:38.146 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-qwphm Oct 13 23:18:38.147 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm node/ created Oct 13 23:18:38.151 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm Successfully assigned openshift-machine-config-operator/machine-config-daemon-qwphm to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:18:38.151 W ns/openshift-image-registry pod/node-ca-gb5x2 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:18:38.172 I ns/openshift-image-registry pod/node-ca-hgsz8 node/ created Oct 13 23:18:38.176 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-hgsz8 Oct 13 23:18:38.183 I ns/openshift-image-registry pod/node-ca-hgsz8 Successfully assigned openshift-image-registry/node-ca-hgsz8 to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:18:40.236 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 13 23:18:40.362 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm Created container machine-config-daemon Oct 13 23:18:40.399 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm Started container machine-config-daemon Oct 13 23:18:48.482 I ns/openshift-image-registry pod/node-ca-hgsz8 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 13 23:18:48.611 I ns/openshift-image-registry pod/node-ca-hgsz8 Created container node-ca Oct 13 23:18:48.633 I ns/openshift-image-registry pod/node-ca-hgsz8 Started container node-ca Oct 13 23:21:09.970 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (8 times) Oct 13 23:21:11.024 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (8 times) Oct 13 23:21:11.888 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (8 times) Oct 13 23:21:12.114 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (11 times) Oct 13 23:21:12.329 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (11 times) Oct 13 23:21:12.531 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (11 times) Oct 13 23:21:13.104 W ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 13 23:21:13.105 W ns/openshift-image-registry pod/node-ca-hgsz8 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:21:13.115 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-qwphm Oct 13 23:21:13.115 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm Stopping container machine-config-daemon Oct 13 23:21:13.115 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-hgsz8 Oct 13 23:21:13.118 I ns/openshift-image-registry pod/node-ca-hgsz8 Stopping container node-ca Oct 13 23:21:13.121 I ns/openshift-image-registry pod/node-ca-hgsz8 Marking for deletion Pod openshift-image-registry/node-ca-hgsz8 Oct 13 23:21:13.124 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-qwphm Oct 13 23:21:14.929 W ns/openshift-image-registry pod/node-ca-hgsz8 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:21:15.031 W ns/openshift-machine-config-operator pod/machine-config-daemon-qwphm node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:22:28.192 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 node/ created Oct 13 23:22:28.201 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-4wlc2 Oct 13 23:22:28.205 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 Successfully assigned openshift-machine-config-operator/machine-config-daemon-4wlc2 to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:22:28.215 I ns/openshift-image-registry pod/node-ca-vghrz node/ created Oct 13 23:22:28.219 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-vghrz Oct 13 23:22:28.225 I ns/openshift-image-registry pod/node-ca-vghrz Successfully assigned openshift-image-registry/node-ca-vghrz to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:22:28.855 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 13 23:22:28.982 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 Created container machine-config-daemon Oct 13 23:22:29.016 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 Started container machine-config-daemon Oct 13 23:22:36.245 I ns/openshift-image-registry pod/node-ca-vghrz Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 13 23:22:36.364 I ns/openshift-image-registry pod/node-ca-vghrz Created container node-ca Oct 13 23:22:36.386 I ns/openshift-image-registry pod/node-ca-vghrz Started container node-ca Oct 13 23:31:09.762 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (9 times) Oct 13 23:31:10.585 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (9 times) Oct 13 23:31:11.373 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (9 times) Oct 13 23:31:11.572 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (12 times) Oct 13 23:31:11.742 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (12 times) Oct 13 23:31:11.917 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (12 times) Oct 13 23:35:15.472 W ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 13 23:35:15.474 W ns/openshift-image-registry pod/node-ca-vghrz node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:35:15.484 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 Stopping container machine-config-daemon Oct 13 23:35:15.484 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-4wlc2 Oct 13 23:35:15.487 I ns/openshift-image-registry pod/node-ca-vghrz Marking for deletion Pod openshift-image-registry/node-ca-vghrz Oct 13 23:35:15.487 I ns/openshift-image-registry pod/node-ca-vghrz Stopping container node-ca Oct 13 23:35:15.488 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-vghrz Oct 13 23:35:15.490 I ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-4wlc2 Oct 13 23:35:17.004 W ns/openshift-image-registry pod/node-ca-vghrz node/ip-10-0-131-112.us-east-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 13 23:35:17.004 W ns/openshift-image-registry pod/node-ca-vghrz node/ip-10-0-131-112.us-east-2.compute.internal container=node-ca container stopped being ready Oct 13 23:35:22.844 W ns/openshift-image-registry pod/node-ca-vghrz node/ip-10-0-131-112.us-east-2.compute.internal pod has been pending longer than a minute Oct 13 23:35:28.124 W ns/openshift-machine-config-operator pod/machine-config-daemon-4wlc2 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:35:28.145 W ns/openshift-image-registry pod/node-ca-vghrz node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:36:20.563 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 node/ created Oct 13 23:36:20.575 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-2mdk8 Oct 13 23:36:20.581 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 Successfully assigned openshift-machine-config-operator/machine-config-daemon-2mdk8 to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:36:20.588 I ns/openshift-image-registry pod/node-ca-4276f node/ created Oct 13 23:36:20.596 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4276f Oct 13 23:36:20.598 I ns/openshift-image-registry pod/node-ca-4276f Successfully assigned openshift-image-registry/node-ca-4276f to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:36:21.236 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 13 23:36:21.359 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 Created container machine-config-daemon Oct 13 23:36:21.384 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 Started container machine-config-daemon Oct 13 23:36:28.162 I ns/openshift-image-registry pod/node-ca-4276f Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 13 23:36:28.277 I ns/openshift-image-registry pod/node-ca-4276f Created container node-ca Oct 13 23:36:28.308 I ns/openshift-image-registry pod/node-ca-4276f Started container node-ca Oct 13 23:39:48.299 W persistentvolume/pvc-4a791b95-0dad-11eb-8499-0282592b2e9e Error deleting EBS volume \"vol-07767b7983fe690d7\" since volume is currently attached to \"i-04986068922e9122c\" Oct 13 23:41:09.815 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (10 times) Oct 13 23:41:10.006 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (13 times) Oct 13 23:41:10.171 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (13 times) Oct 13 23:41:10.429 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (13 times) Oct 13 23:41:11.261 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (10 times) Oct 13 23:41:12.100 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (10 times) Oct 13 23:44:08.883 I ns/kube-system pod/pod0-system-node-critical node/ created Oct 13 23:44:08.895 I ns/kube-system pod/pod0-system-node-critical Successfully assigned kube-system/pod0-system-node-critical to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:44:08.910 I ns/kube-system pod/pod1-system-cluster-critical node/ created Oct 13 23:44:08.918 I ns/kube-system pod/pod1-system-cluster-critical Successfully assigned kube-system/pod1-system-cluster-critical to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:44:08.939 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 0s Oct 13 23:44:08.942 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:44:08.970 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 0s Oct 13 23:44:08.974 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:46:11.911 W ns/kube-system pod/pod0-system-node-critical Unable to mount volumes for pod \"pod0-system-node-critical_kube-system(fcea83e2-0dad-11eb-b1a9-02de12e903fe)\": timeout expired waiting for volumes to attach or mount for pod \"kube-system\"/\"pod0-system-node-critical\". list of unmounted volumes=[default-token-wn8pl]. list of unattached volumes=[default-token-wn8pl] Oct 13 23:46:11.933 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \"pod1-system-cluster-critical_kube-system(fceef1c6-0dad-11eb-b1a9-02de12e903fe)\": timeout expired waiting for volumes to attach or mount for pod \"kube-system\"/\"pod1-system-cluster-critical\". list of unmounted volumes=[default-token-wn8pl]. list of unattached volumes=[default-token-wn8pl] Oct 13 23:48:23.491 I ns/openshift-image-registry pod/node-ca-4276f Stopping container node-ca Oct 13 23:48:23.491 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 Stopping container machine-config-daemon Oct 13 23:48:23.491 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4276f Oct 13 23:48:23.491 W ns/openshift-image-registry pod/node-ca-4276f node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:48:23.491 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-2mdk8 Oct 13 23:48:23.491 W ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 13 23:48:23.494 I ns/openshift-image-registry pod/node-ca-4276f Marking for deletion Pod openshift-image-registry/node-ca-4276f Oct 13 23:48:23.498 I ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-2mdk8 Oct 13 23:48:28.125 W ns/openshift-machine-config-operator pod/machine-config-daemon-2mdk8 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:48:28.144 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-7dqvc Oct 13 23:48:28.145 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc node/ created Oct 13 23:48:28.148 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc Successfully assigned openshift-machine-config-operator/machine-config-daemon-7dqvc to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:48:28.149 W ns/openshift-image-registry pod/node-ca-4276f node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:48:28.169 I ns/openshift-image-registry pod/node-ca-pbk85 node/ created Oct 13 23:48:28.173 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-pbk85 Oct 13 23:48:28.178 I ns/openshift-image-registry pod/node-ca-pbk85 Successfully assigned openshift-image-registry/node-ca-pbk85 to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:48:29.603 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 13 23:48:29.711 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc Created container machine-config-daemon Oct 13 23:48:29.733 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc Started container machine-config-daemon Oct 13 23:48:37.414 I ns/openshift-image-registry pod/node-ca-pbk85 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 13 23:48:37.540 I ns/openshift-image-registry pod/node-ca-pbk85 Created container node-ca Oct 13 23:48:37.565 I ns/openshift-image-registry pod/node-ca-pbk85 Started container node-ca Oct 13 23:51:09.730 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (11 times) Oct 13 23:51:10.542 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (11 times) Oct 13 23:51:11.549 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (11 times) Oct 13 23:51:11.737 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (14 times) Oct 13 23:51:11.900 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (14 times) Oct 13 23:51:12.091 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (14 times) Oct 13 23:51:46.921 I ns/kube-system pod/critical-pod node/ created Oct 13 23:51:46.929 W ns/kube-system pod/critical-pod 0/6 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 3 node(s) had taints that the pod didn\\'t tolerate. Oct 13 23:51:46.950 W ns/kube-system pod/critical-pod 0/6 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 3 node(s) had taints that the pod didn\\'t tolerate. (2 times) Oct 13 23:51:58.137 I ns/kube-system pod/critical-pod Successfully assigned kube-system/critical-pod to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:52:05.975 I ns/kube-system pod/critical-pod Container image \"k8s.gcr.io/pause:3.1\" already present on machine Oct 13 23:52:06.110 I ns/kube-system pod/critical-pod Created container critical-pod Oct 13 23:52:06.129 I ns/kube-system pod/critical-pod Started container critical-pod Oct 13 23:52:07.115 W ns/kube-system pod/critical-pod node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 0s Oct 13 23:52:07.118 W ns/kube-system pod/critical-pod node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:52:07.691 I ns/kube-system pod/critical-pod Pod sandbox changed, it will be killed and re-created. Oct 13 23:52:17.295 W ns/kube-system pod/critical-pod Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_critical-pod_kube-system_0dede5b8-0daf-11eb-8499-0282592b2e9e_1(ef346ef22eb280f6b4369c549be3d57fff13fb2d286c3e9fe48380dd01f49636): Multus: Err adding pod to network \"openshift-sdn\": cannot set \"openshift-sdn\" ifname to \"eth0\": no netns: failed to Statfs \"/proc/131341/ns/net\": no such file or directory Oct 13 23:55:54.651 W ns/openshift-image-registry pod/node-ca-pbk85 node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 13 23:55:54.654 W ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 13 23:55:54.667 I ns/openshift-image-registry pod/node-ca-pbk85 Stopping container node-ca Oct 13 23:55:54.667 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-7dqvc Oct 13 23:55:54.667 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc Stopping container machine-config-daemon Oct 13 23:55:54.669 I ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-7dqvc Oct 13 23:55:54.670 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-pbk85 Oct 13 23:55:54.674 I ns/openshift-image-registry pod/node-ca-pbk85 Marking for deletion Pod openshift-image-registry/node-ca-pbk85 Oct 13 23:55:56.213 W ns/openshift-machine-config-operator pod/machine-config-daemon-7dqvc node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:55:56.231 W ns/openshift-image-registry pod/node-ca-pbk85 node/ip-10-0-131-112.us-east-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 13 23:55:56.231 W ns/openshift-image-registry pod/node-ca-pbk85 node/ip-10-0-131-112.us-east-2.compute.internal container=node-ca container stopped being ready Oct 13 23:56:07.844 W ns/openshift-image-registry pod/node-ca-pbk85 node/ip-10-0-131-112.us-east-2.compute.internal pod has been pending longer than a minute Oct 13 23:56:08.145 W ns/openshift-image-registry pod/node-ca-pbk85 node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 13 23:56:28.198 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf node/ created Oct 13 23:56:28.204 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-rxxtf Oct 13 23:56:28.207 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf Successfully assigned openshift-machine-config-operator/machine-config-daemon-rxxtf to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:56:28.217 I ns/openshift-image-registry pod/node-ca-5w4xw node/ created Oct 13 23:56:28.222 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-5w4xw Oct 13 23:56:28.227 I ns/openshift-image-registry pod/node-ca-5w4xw Successfully assigned openshift-image-registry/node-ca-5w4xw to ip-10-0-131-112.us-east-2.compute.internal Oct 13 23:56:28.881 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 13 23:56:29.032 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf Created container machine-config-daemon Oct 13 23:56:29.064 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf Started container machine-config-daemon Oct 13 23:56:36.440 I ns/openshift-image-registry pod/node-ca-5w4xw Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 13 23:56:36.566 I ns/openshift-image-registry pod/node-ca-5w4xw Created container node-ca Oct 13 23:56:36.596 I ns/openshift-image-registry pod/node-ca-5w4xw Started container node-ca Oct 14 00:00:27.730 W ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 600s Oct 14 00:00:27.733 W ns/openshift-image-registry pod/node-ca-5w4xw node/ip-10-0-131-112.us-east-2.compute.internal graceful deletion within 30s Oct 14 00:00:27.738 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf Stopping container machine-config-daemon Oct 14 00:00:27.742 I ns/openshift-image-registry pod/node-ca-5w4xw Stopping container node-ca Oct 14 00:00:27.745 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-5w4xw Oct 14 00:00:27.750 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-rxxtf Oct 14 00:00:27.751 I ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-rxxtf Oct 14 00:00:27.755 I ns/openshift-image-registry pod/node-ca-5w4xw Marking for deletion Pod openshift-image-registry/node-ca-5w4xw Oct 14 00:00:38.126 W ns/openshift-machine-config-operator pod/machine-config-daemon-rxxtf node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 14 00:00:38.156 W ns/openshift-image-registry pod/node-ca-5w4xw node/ip-10-0-131-112.us-east-2.compute.internal deleted Oct 14 00:01:09.797 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-2 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-2 (12 times) Oct 14 00:01:10.722 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-0 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-0 (12 times) Oct 14 00:01:11.588 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-master-1 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-master-1 (12 times) Oct 14 00:01:11.803 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-7ld2c (15 times) Oct 14 00:01:11.991 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2a-l5ds7 (15 times) Oct 14 00:01:12.177 I ns/openshift-machine-api machine/ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 Updated machine ci-op-xh6icmvm-7bc5c-2vl8g-worker-us-east-2b-wpqc8 (15 times) Oct 14 00:01:32.826 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq node/ created Oct 14 00:01:32.837 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-2g2jq Oct 14 00:01:32.842 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Successfully assigned openshift-machine-config-operator/machine-config-daemon-2g2jq to ip-10-0-131-112.us-east-2.compute.internal Oct 14 00:01:32.845 I ns/openshift-image-registry pod/node-ca-gr4h4 node/ created Oct 14 00:01:32.851 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-gr4h4 Oct 14 00:01:32.855 I ns/openshift-image-registry pod/node-ca-gr4h4 Successfully assigned openshift-image-registry/node-ca-gr4h4 to ip-10-0-131-112.us-east-2.compute.internal Oct 14 00:01:33.502 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 14 00:01:33.622 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Created container machine-config-daemon Oct 14 00:01:33.645 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Started container machine-config-daemon Oct 14 00:01:40.522 I ns/openshift-image-registry pod/node-ca-gr4h4 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 14 00:01:40.641 I ns/openshift-image-registry pod/node-ca-gr4h4 Created container node-ca Oct 14 00:01:40.663 I ns/openshift-image-registry pod/node-ca-gr4h4 Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201014-000845.xml error: 1 fail, 39 pass, 39 skip (1h23m8s) 2020/10/14 00:08:46 Container test in pod e2e-aws-serial failed, exit code 1, reason Error 2020/10/14 00:14:25 Copied 120.93MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/14 00:14:25 Releasing lease for \"aws-quota-slice\" 2020/10/14 00:14:25 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/14 00:14:25 Ran for 2h4m17s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-xh6icmvm/e2e-aws-serial failed after 1h59m29s (failed containers: test): ContainerFailed one or more containers exited Container test exited with code 1, reason Error --- 5 times) Oct 14 00:01:32.826 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq node/ created Oct 14 00:01:32.837 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-2g2jq Oct 14 00:01:32.842 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Successfully assigned openshift-machine-config-operator/machine-config-daemon-2g2jq to ip-10-0-131-112.us-east-2.compute.internal Oct 14 00:01:32.845 I ns/openshift-image-registry pod/node-ca-gr4h4 node/ created Oct 14 00:01:32.851 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-gr4h4 Oct 14 00:01:32.855 I ns/openshift-image-registry pod/node-ca-gr4h4 Successfully assigned openshift-image-registry/node-ca-gr4h4 to ip-10-0-131-112.us-east-2.compute.internal Oct 14 00:01:33.502 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 14 00:01:33.622 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Created container machine-config-daemon Oct 14 00:01:33.645 I ns/openshift-machine-config-operator pod/machine-config-daemon-2g2jq Started container machine-config-daemon Oct 14 00:01:40.522 I ns/openshift-image-registry pod/node-ca-gr4h4 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 14 00:01:40.641 I ns/openshift-image-registry pod/node-ca-gr4h4 Created container node-ca Oct 14 00:01:40.663 I ns/openshift-image-registry pod/node-ca-gr4h4 Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201014-000845.xml error: 1 fail, 39 pass, 39 skip (1h23m8s) --- '\n",
            "ID=14    : size=11        : <*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \"release-images-latest-cli\" <*> <*> Executing pod \"release-images-latest\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \"aws-quota-slice\" <*> <*> Acquired lease <*> for \"aws-quota-slice\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \"aws-quota-slice\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\n",
            "ID=15    : size=1         : b'2020/10/21 22:15:26 ci-operator version v20201021-d45a03c 2020/10/21 22:15:26 No source defined 2020/10/21 22:15:26 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/21 22:15:26 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-vv1cddgl 2020/10/21 22:15:26 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/21 22:15:26 Creating namespace ci-op-vv1cddgl 2020/10/21 22:15:26 Setting up pipeline imagestream for the test 2020/10/21 22:15:26 Created secret e2e-aws-serial-cluster-profile 2020/10/21 22:15:26 Created secret pull-secret 2020/10/21 22:15:26 Created PDB for pods with openshift.io/build.name label 2020/10/21 22:15:26 Created PDB for pods with created-by-ci label 2020/10/21 22:15:26 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-vv1cddgl/stable:${component} 2020/10/21 22:15:29 Importing release image latest 2020/10/21 22:15:30 Executing pod \"release-images-latest-cli\" 2020/10/21 22:15:36 Executing pod \"release-images-latest\" 2020/10/21 22:16:23 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/21 22:16:23 Acquiring lease for \"aws-quota-slice\" 2020/10/21 22:16:23 Acquired lease \"6abd33c9-1f1d-4d0f-9faa-e6bbfeb06460\" for \"aws-quota-slice\" 2020/10/21 22:16:23 Executing template e2e-aws-serial 2020/10/21 22:16:23 Creating or restarting template instance 2020/10/21 22:16:23 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/21 22:16:23 Waiting for template instance to be ready 2020/10/21 22:16:25 Running pod e2e-aws-serial 2020/10/21 22:45:42 Container setup in pod e2e-aws-serial completed successfully secret/support created started: (0/1/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m47s) 2020-10-21T22:48:37 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/2/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.8s) 2020-10-21T22:49:18 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/3/79) \"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (39s) 2020-10-21T22:49:57 \"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/4/79) \"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\" skip [github.com/openshift/origin/test/extended/images/signatures.go:25]: disable because containers/image: https://github.com/containers/image/pull/570 skipped: (14.4s) 2020-10-21T22:50:12 \"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\" started: (0/5/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T22:50:52 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/6/79) \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (36.3s) 2020-10-21T22:51:28 \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/7/79) \"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (2m2s) 2020-10-21T22:53:30 \"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/8/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" Oct 21 22:53:31.532: INFO: >>> kubeConfig: /tmp/admin.kubeconfig Oct 21 22:53:31.534: INFO: Waiting up to 30m0s for all (but 100) nodes to be schedulable Oct 21 22:53:32.048: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace \\'kube-system\\' to be running and ready Oct 21 22:53:32.316: INFO: 0 / 0 pods in namespace \\'kube-system\\' are running and ready (0 seconds elapsed) Oct 21 22:53:32.316: INFO: expected 0 pod replicas in namespace \\'kube-system\\', 0 are Running and Ready. Oct 21 22:53:32.316: INFO: Waiting up to 5m0s for all daemonsets in namespace \\'kube-system\\' to start Oct 21 22:53:32.404: INFO: e2e test version: v1.13.4-138-g41dc99c Oct 21 22:53:32.486: INFO: kube-apiserver version: v1.13.4-138-g41dc99c [BeforeEach] [Top Level] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/test/extended/util/test.go:69 [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:153 STEP: Creating a kubernetes client Oct 21 22:53:32.488: INFO: >>> kubeConfig: /tmp/admin.kubeconfig STEP: Building a namespace api object, basename sched-priority Oct 21 22:53:37.542: INFO: About to run a Kube e2e test, ensuring namespace is privileged Oct 21 22:53:38.591: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled. STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:71 Oct 21 22:53:38.672: INFO: Waiting up to 1m0s for all nodes to be ready Oct 21 22:54:39.916: INFO: Waiting for terminating namespaces to be deleted... Oct 21 22:54:40.003: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace \\'kube-system\\' to be running and ready Oct 21 22:54:40.256: INFO: 0 / 0 pods in namespace \\'kube-system\\' are running and ready (0 seconds elapsed) Oct 21 22:54:40.256: INFO: expected 0 pod replicas in namespace \\'kube-system\\', 0 are Running and Ready. [It] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:146 Oct 21 22:54:40.256: INFO: ComputeCpuMemFraction for node: ip-10-0-142-54.us-west-1.compute.internal Oct 21 22:54:40.366: INFO: Pod for on the node: tuned-74x4c, Cpu: 10, Mem: 20971520 Oct 21 22:54:40.366: INFO: Pod for on the node: dns-default-7fm2d, Cpu: 110, Mem: 283115520 Oct 21 22:54:40.366: INFO: Pod for on the node: image-registry-7554c494f8-vn5kq, Cpu: 100, Mem: 268435456 Oct 21 22:54:40.366: INFO: Pod for on the node: node-ca-sqzxh, Cpu: 10, Mem: 10485760 Oct 21 22:54:40.366: INFO: Pod for on the node: router-default-76df456ddd-wnlxt, Cpu: 100, Mem: 268435456 Oct 21 22:54:40.366: INFO: Pod for on the node: machine-config-daemon-bhwc4, Cpu: 20, Mem: 52428800 Oct 21 22:54:40.366: INFO: Pod for on the node: certified-operators-758bf864b9-pmtnc, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.366: INFO: Pod for on the node: community-operators-6985ff7ccc-9jxc9, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.366: INFO: Pod for on the node: redhat-operators-6489d8696-7jqlz, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.366: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Oct 21 22:54:40.366: INFO: Pod for on the node: grafana-649f787944-ln2tc, Cpu: 200, Mem: 314572800 Oct 21 22:54:40.366: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-p69w5, Cpu: 300, Mem: 629145600 Oct 21 22:54:40.366: INFO: Pod for on the node: node-exporter-z499s, Cpu: 110, Mem: 230686720 Oct 21 22:54:40.366: INFO: Pod for on the node: prometheus-adapter-7b86577c4-mwcrw, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.366: INFO: Pod for on the node: telemeter-client-869d867dd8-wrt8b, Cpu: 210, Mem: 440401920 Oct 21 22:54:40.366: INFO: Pod for on the node: multus-jqjk6, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.366: INFO: Pod for on the node: ovs-wlc9p, Cpu: 200, Mem: 419430400 Oct 21 22:54:40.366: INFO: Pod for on the node: sdn-n2nlx, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.366: INFO: Node: ip-10-0-142-54.us-west-1.compute.internal, totalRequestedCpuResource: 1570, cpuAllocatableMil: 3500, cpuFraction: 0.44857142857142857 Oct 21 22:54:40.366: INFO: Node: ip-10-0-142-54.us-west-1.compute.internal, totalRequestedMemResource: 3252682752, memAllocatableVal: 16181800960, memFraction: 0.20100869860161721 Oct 21 22:54:40.366: INFO: ComputeCpuMemFraction for node: ip-10-0-143-110.us-west-1.compute.internal Oct 21 22:54:40.472: INFO: Pod for on the node: tuned-cnzrp, Cpu: 10, Mem: 20971520 Oct 21 22:54:40.472: INFO: Pod for on the node: dns-default-5mk9j, Cpu: 110, Mem: 283115520 Oct 21 22:54:40.472: INFO: Pod for on the node: node-ca-cxxsk, Cpu: 10, Mem: 10485760 Oct 21 22:54:40.472: INFO: Pod for on the node: machine-config-daemon-z9qzz, Cpu: 20, Mem: 52428800 Oct 21 22:54:40.472: INFO: Pod for on the node: node-exporter-fhpkb, Cpu: 110, Mem: 230686720 Oct 21 22:54:40.472: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Oct 21 22:54:40.472: INFO: Pod for on the node: multus-59h2z, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.472: INFO: Pod for on the node: ovs-58xn4, Cpu: 200, Mem: 419430400 Oct 21 22:54:40.472: INFO: Pod for on the node: sdn-h4mht, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.472: INFO: Node: ip-10-0-143-110.us-west-1.compute.internal, totalRequestedCpuResource: 660, cpuAllocatableMil: 3500, cpuFraction: 0.18857142857142858 Oct 21 22:54:40.472: INFO: Node: ip-10-0-143-110.us-west-1.compute.internal, totalRequestedMemResource: 1331691520, memAllocatableVal: 16181800960, memFraction: 0.08229563095552993 Oct 21 22:54:40.472: INFO: ComputeCpuMemFraction for node: ip-10-0-147-199.us-west-1.compute.internal Oct 21 22:54:40.575: INFO: Pod for on the node: tuned-dxcks, Cpu: 10, Mem: 20971520 Oct 21 22:54:40.575: INFO: Pod for on the node: dns-default-hlb8d, Cpu: 110, Mem: 283115520 Oct 21 22:54:40.575: INFO: Pod for on the node: node-ca-xzq8t, Cpu: 10, Mem: 10485760 Oct 21 22:54:40.575: INFO: Pod for on the node: router-default-76df456ddd-9f8l9, Cpu: 100, Mem: 268435456 Oct 21 22:54:40.575: INFO: Pod for on the node: machine-config-daemon-p7wps, Cpu: 20, Mem: 52428800 Oct 21 22:54:40.575: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Oct 21 22:54:40.575: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Oct 21 22:54:40.575: INFO: Pod for on the node: node-exporter-79lp8, Cpu: 110, Mem: 230686720 Oct 21 22:54:40.575: INFO: Pod for on the node: prometheus-adapter-7b86577c4-s57f8, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.575: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Oct 21 22:54:40.575: INFO: Pod for on the node: prometheus-operator-5d4588dd6-tzzgw, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.575: INFO: Pod for on the node: multus-dnd4k, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.575: INFO: Pod for on the node: olm-operators-66p8d, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.575: INFO: Pod for on the node: ovs-9dp2w, Cpu: 200, Mem: 419430400 Oct 21 22:54:40.575: INFO: Pod for on the node: sdn-tbbsf, Cpu: 100, Mem: 209715200 Oct 21 22:54:40.575: INFO: Node: ip-10-0-147-199.us-west-1.compute.internal, totalRequestedCpuResource: 1360, cpuAllocatableMil: 3500, cpuFraction: 0.38857142857142857 Oct 21 22:54:40.575: INFO: Node: ip-10-0-147-199.us-west-1.compute.internal, totalRequestedMemResource: 2858418176, memAllocatableVal: 16181792768, memFraction: 0.17664409728770047 Oct 21 22:54:40.680: INFO: Waiting for running... Oct 21 22:54:50.877: INFO: Waiting for running... Oct 21 22:55:01.075: INFO: Waiting for running... STEP: Compute Cpu, Mem Fraction after create balanced pods. Oct 21 22:55:11.176: INFO: ComputeCpuMemFraction for node: ip-10-0-142-54.us-west-1.compute.internal Oct 21 22:55:11.680: INFO: Pod for on the node: 66fb5002-13f0-11eb-ab84-0a58ac10517a-0, Cpu: 180, Mem: 4838217728 Oct 21 22:55:11.680: INFO: Pod for on the node: tuned-74x4c, Cpu: 10, Mem: 20971520 Oct 21 22:55:11.680: INFO: Pod for on the node: dns-default-7fm2d, Cpu: 110, Mem: 283115520 Oct 21 22:55:11.680: INFO: Pod for on the node: image-registry-7554c494f8-vn5kq, Cpu: 100, Mem: 268435456 Oct 21 22:55:11.680: INFO: Pod for on the node: node-ca-sqzxh, Cpu: 10, Mem: 10485760 Oct 21 22:55:11.680: INFO: Pod for on the node: router-default-76df456ddd-wnlxt, Cpu: 100, Mem: 268435456 Oct 21 22:55:11.680: INFO: Pod for on the node: machine-config-daemon-bhwc4, Cpu: 20, Mem: 52428800 Oct 21 22:55:11.680: INFO: Pod for on the node: certified-operators-758bf864b9-pmtnc, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.680: INFO: Pod for on the node: community-operators-6985ff7ccc-9jxc9, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.680: INFO: Pod for on the node: redhat-operators-6489d8696-7jqlz, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.680: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Oct 21 22:55:11.680: INFO: Pod for on the node: grafana-649f787944-ln2tc, Cpu: 200, Mem: 314572800 Oct 21 22:55:11.680: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-p69w5, Cpu: 300, Mem: 629145600 Oct 21 22:55:11.680: INFO: Pod for on the node: node-exporter-z499s, Cpu: 110, Mem: 230686720 Oct 21 22:55:11.680: INFO: Pod for on the node: prometheus-adapter-7b86577c4-mwcrw, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.680: INFO: Pod for on the node: telemeter-client-869d867dd8-wrt8b, Cpu: 210, Mem: 440401920 Oct 21 22:55:11.680: INFO: Pod for on the node: multus-jqjk6, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.680: INFO: Pod for on the node: ovs-wlc9p, Cpu: 200, Mem: 419430400 Oct 21 22:55:11.680: INFO: Pod for on the node: sdn-n2nlx, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.680: INFO: Node: ip-10-0-142-54.us-west-1.compute.internal, totalRequestedCpuResource: 1750, cpuAllocatableMil: 3500, cpuFraction: 0.5 Oct 21 22:55:11.680: INFO: Node: ip-10-0-142-54.us-west-1.compute.internal, totalRequestedMemResource: 8090900480, memAllocatableVal: 16181800960, memFraction: 0.5 STEP: Compute Cpu, Mem Fraction after create balanced pods. Oct 21 22:55:11.680: INFO: ComputeCpuMemFraction for node: ip-10-0-143-110.us-west-1.compute.internal Oct 21 22:55:11.863: INFO: Pod for on the node: 6d10a1ae-13f0-11eb-ab84-0a58ac10517a-0, Cpu: 1089, Mem: 6759208960 Oct 21 22:55:11.863: INFO: Pod for on the node: tuned-cnzrp, Cpu: 10, Mem: 20971520 Oct 21 22:55:11.863: INFO: Pod for on the node: dns-default-5mk9j, Cpu: 110, Mem: 283115520 Oct 21 22:55:11.863: INFO: Pod for on the node: node-ca-cxxsk, Cpu: 10, Mem: 10485760 Oct 21 22:55:11.863: INFO: Pod for on the node: machine-config-daemon-z9qzz, Cpu: 20, Mem: 52428800 Oct 21 22:55:11.863: INFO: Pod for on the node: node-exporter-fhpkb, Cpu: 110, Mem: 230686720 Oct 21 22:55:11.863: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Oct 21 22:55:11.863: INFO: Pod for on the node: multus-59h2z, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.863: INFO: Pod for on the node: ovs-58xn4, Cpu: 200, Mem: 419430400 Oct 21 22:55:11.863: INFO: Pod for on the node: sdn-h4mht, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.863: INFO: Node: ip-10-0-143-110.us-west-1.compute.internal, totalRequestedCpuResource: 1749, cpuAllocatableMil: 3500, cpuFraction: 0.4997142857142857 Oct 21 22:55:11.863: INFO: Node: ip-10-0-143-110.us-west-1.compute.internal, totalRequestedMemResource: 8090900480, memAllocatableVal: 16181800960, memFraction: 0.5 STEP: Compute Cpu, Mem Fraction after create balanced pods. Oct 21 22:55:11.863: INFO: ComputeCpuMemFraction for node: ip-10-0-147-199.us-west-1.compute.internal Oct 21 22:55:11.975: INFO: Pod for on the node: 73248907-13f0-11eb-ab84-0a58ac10517a-0, Cpu: 390, Mem: 5232478208 Oct 21 22:55:11.975: INFO: Pod for on the node: tuned-dxcks, Cpu: 10, Mem: 20971520 Oct 21 22:55:11.975: INFO: Pod for on the node: dns-default-hlb8d, Cpu: 110, Mem: 283115520 Oct 21 22:55:11.975: INFO: Pod for on the node: node-ca-xzq8t, Cpu: 10, Mem: 10485760 Oct 21 22:55:11.975: INFO: Pod for on the node: router-default-76df456ddd-9f8l9, Cpu: 100, Mem: 268435456 Oct 21 22:55:11.975: INFO: Pod for on the node: machine-config-daemon-p7wps, Cpu: 20, Mem: 52428800 Oct 21 22:55:11.975: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Oct 21 22:55:11.975: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Oct 21 22:55:11.975: INFO: Pod for on the node: node-exporter-79lp8, Cpu: 110, Mem: 230686720 Oct 21 22:55:11.975: INFO: Pod for on the node: prometheus-adapter-7b86577c4-s57f8, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.975: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Oct 21 22:55:11.975: INFO: Pod for on the node: prometheus-operator-5d4588dd6-tzzgw, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.975: INFO: Pod for on the node: multus-dnd4k, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.975: INFO: Pod for on the node: olm-operators-66p8d, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.975: INFO: Pod for on the node: ovs-9dp2w, Cpu: 200, Mem: 419430400 Oct 21 22:55:11.975: INFO: Pod for on the node: sdn-tbbsf, Cpu: 100, Mem: 209715200 Oct 21 22:55:11.975: INFO: Node: ip-10-0-147-199.us-west-1.compute.internal, totalRequestedCpuResource: 1750, cpuAllocatableMil: 3500, cpuFraction: 0.5 Oct 21 22:55:11.975: INFO: Node: ip-10-0-147-199.us-west-1.compute.internal, totalRequestedMemResource: 8090896384, memAllocatableVal: 16181792768, memFraction: 0.5 STEP: Create a RC, with 0 replicas STEP: Trying to apply avoidPod annotations on the first node. Oct 21 22:55:12.399: INFO: Conflict when trying to add/update avoidPonds {[{{&OwnerReference{Kind:ReplicationController,Name:scheduler-priority-avoid-pod,UID:79b978a3-13f0-11eb-9b44-06dc8e64439d,APIVersion:v1,Controller:*true,BlockOwnerDeletion:nil,}} 0001-01-01 00:00:00 +0000 UTC some reson some message}]} to ip-10-0-142-54.us-west-1.compute.internal STEP: deleting ReplicationController scheduler-priority-avoid-pod in namespace e2e-tests-sched-priority-757tb, will wait for the garbage collector to delete the pods Oct 21 22:57:12.678: INFO: Deleting ReplicationController scheduler-priority-avoid-pod took: 92.052032ms Oct 21 22:57:12.678: INFO: Terminating ReplicationController scheduler-priority-avoid-pod pods took: 93.442\\xc2\\xb5s [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:154 STEP: Collecting events from namespace \"e2e-tests-sched-priority-757tb\". STEP: Found 9 events. Oct 21 22:57:12.764: INFO: At 2020-10-21 22:54:49 +0000 UTC - event for 66fb5002-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-142-54.us-west-1.compute.internal} Pulled: Container image \"k8s.gcr.io/pause:3.1\" already present on machine Oct 21 22:57:12.764: INFO: At 2020-10-21 22:54:49 +0000 UTC - event for 66fb5002-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-142-54.us-west-1.compute.internal} Created: Created container 66fb5002-13f0-11eb-ab84-0a58ac10517a-0 Oct 21 22:57:12.764: INFO: At 2020-10-21 22:54:49 +0000 UTC - event for 66fb5002-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-142-54.us-west-1.compute.internal} Started: Started container 66fb5002-13f0-11eb-ab84-0a58ac10517a-0 Oct 21 22:57:12.764: INFO: At 2020-10-21 22:54:58 +0000 UTC - event for 6d10a1ae-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-143-110.us-west-1.compute.internal} Pulled: Container image \"k8s.gcr.io/pause:3.1\" already present on machine Oct 21 22:57:12.764: INFO: At 2020-10-21 22:54:58 +0000 UTC - event for 6d10a1ae-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-143-110.us-west-1.compute.internal} Created: Created container 6d10a1ae-13f0-11eb-ab84-0a58ac10517a-0 Oct 21 22:57:12.764: INFO: At 2020-10-21 22:54:58 +0000 UTC - event for 6d10a1ae-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-143-110.us-west-1.compute.internal} Started: Started container 6d10a1ae-13f0-11eb-ab84-0a58ac10517a-0 Oct 21 22:57:12.764: INFO: At 2020-10-21 22:55:08 +0000 UTC - event for 73248907-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-147-199.us-west-1.compute.internal} Pulled: Container image \"k8s.gcr.io/pause:3.1\" already present on machine Oct 21 22:57:12.764: INFO: At 2020-10-21 22:55:08 +0000 UTC - event for 73248907-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-147-199.us-west-1.compute.internal} Created: Created container 73248907-13f0-11eb-ab84-0a58ac10517a-0 Oct 21 22:57:12.764: INFO: At 2020-10-21 22:55:08 +0000 UTC - event for 73248907-13f0-11eb-ab84-0a58ac10517a-0: {kubelet ip-10-0-147-199.us-west-1.compute.internal} Started: Started container 73248907-13f0-11eb-ab84-0a58ac10517a-0 Oct 21 22:57:12.929: INFO: skipping dumping cluster info - cluster too large Oct 21 22:57:12.929: INFO: Waiting up to 3m0s for all (but 100) nodes to be ready STEP: Destroying namespace \"e2e-tests-sched-priority-757tb\" for this suite. Oct 21 22:57:37.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Oct 21 22:57:43.272: INFO: namespace e2e-tests-sched-priority-757tb deletion completed in 30.256522442s [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:68 Oct 21 22:57:43.275: INFO: Running AfterSuite actions on all nodes Oct 21 22:57:43.275: INFO: Running AfterSuite actions on node 1 fail [k8s.io/kubernetes/test/e2e/scheduling/priorities.go:191]: Expected error: <*errors.errorString | 0xc0002b0400>: { s: \"timed out waiting for the condition\", } timed out waiting for the condition not to have occurred Oct 21 22:55:31.482 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (9 times) Oct 21 22:55:32.491 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (5 times) Oct 21 22:55:33.359 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (5 times) Oct 21 22:55:34.374 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (6 times) Oct 21 22:55:34.527 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (8 times) Oct 21 22:55:34.641 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (9 times) failed: (4m13s) 2020-10-21T22:57:43 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/9/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-21T22:58:23 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/10/79) \"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:247]: Could not get controller-manager metrics - skipping skipped: (29.5s) 2020-10-21T22:58:53 \"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/11/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m38s) 2020-10-21T23:00:31 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/12/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.5s) 2020-10-21T23:00:54 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/13/79) \"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m15s) 2020-10-21T23:02:09 \"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/14/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:02:50 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/15/79) \"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\" passed: (58.8s) 2020-10-21T23:03:49 \"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\" started: (1/16/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:04:29 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/17/79) \"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m4s) 2020-10-21T23:05:34 \"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/18/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:06:14 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/19/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:06:55 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/20/79) \"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:193]: Could not get controller-manager metrics - skipping skipped: (1m2s) 2020-10-21T23:07:57 \"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/21/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-21T23:08:37 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/22/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m54s) 2020-10-21T23:11:31 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/23/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m57s) 2020-10-21T23:13:29 \"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/24/79) \"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (19.1s) 2020-10-21T23:13:48 \"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/25/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:14:29 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/26/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m34s) 2020-10-21T23:17:03 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/27/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-21T23:17:43 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/28/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:18:24 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/29/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-10-21T23:19:05 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/30/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.4s) 2020-10-21T23:19:27 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/31/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (50.7s) 2020-10-21T23:20:18 \"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/32/79) \"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (3m15s) 2020-10-21T23:23:33 \"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/33/79) \"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m20s) 2020-10-21T23:24:53 \"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/34/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.6s) 2020-10-21T23:25:16 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/35/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-10-21T23:25:56 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/36/79) \"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/scheduling/taint_based_evictions.go:63]: Requires at least 2 nodes (not -1) skipped: (19s) 2020-10-21T23:26:15 \"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/37/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m21s) 2020-10-21T23:27:36 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/38/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m52s) 2020-10-21T23:29:28 \"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/39/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m32s) 2020-10-21T23:31:59 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/40/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m24s) 2020-10-21T23:33:23 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/41/79) \"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m54s) 2020-10-21T23:35:17 \"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/42/79) \"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m50s) 2020-10-21T23:37:07 \"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/43/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.7s) 2020-10-21T23:37:30 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/44/79) \"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:81]: Environment does not support getting controller-manager metrics - skipping skipped: (19.2s) 2020-10-21T23:37:49 \"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/45/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.7s) 2020-10-21T23:38:11 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/46/79) \"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (19.5s) 2020-10-21T23:38:31 \"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/47/79) \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (30.9s) 2020-10-21T23:39:02 \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/48/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-21T23:39:43 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/49/79) \"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m11s) 2020-10-21T23:40:53 \"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/50/79) \"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m3s) 2020-10-21T23:41:56 \"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/51/79) \"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (19.3s) 2020-10-21T23:42:15 \"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/52/79) \"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m18s) 2020-10-21T23:43:33 \"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/53/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (41.7s) 2020-10-21T23:44:15 \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/54/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m51s) 2020-10-21T23:46:06 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/55/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-10-21T23:46:46 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/56/79) \"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\" passed: (40.7s) 2020-10-21T23:47:27 \"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\" started: (1/57/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-21T23:48:07 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/58/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.6s) 2020-10-21T23:48:30 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/59/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (34.7s) 2020-10-21T23:49:05 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/60/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-21T23:49:45 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/61/79) \"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m39s) 2020-10-21T23:51:24 \"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/62/79) \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m40s) 2020-10-21T23:54:04 \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/63/79) \"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (19.3s) 2020-10-21T23:54:23 \"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/64/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (44.7s) 2020-10-21T23:55:08 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/65/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (21s) 2020-10-21T23:55:29 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/66/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (51.3s) 2020-10-21T23:56:20 \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/67/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m10s) 2020-10-21T23:58:31 \"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/68/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (32.6s) 2020-10-21T23:59:03 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/69/79) \"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m58s) 2020-10-22T00:01:01 \"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/70/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.8s) 2020-10-22T00:01:24 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/71/79) \"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (19.2s) 2020-10-22T00:01:43 \"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/72/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m44s) 2020-10-22T00:04:27 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/73/79) \"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/apps/daemon_set.go:378]: Requires at least 2 nodes (not -1) skipped: (20.2s) 2020-10-22T00:04:48 \"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/74/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (33s) 2020-10-22T00:05:21 \"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/75/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (21.2s) 2020-10-22T00:05:42 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/76/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (22.6s) 2020-10-22T00:06:04 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/77/79) \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m59s) 2020-10-22T00:08:03 \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/78/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-10-22T00:08:44 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/79/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-10-22T00:09:24 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" Timeline: Oct 21 22:45:52.682 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-129-97.us-west-1.compute.internal node/ip-10-0-129-97.us-west-1.compute.internal created Oct 21 22:45:52.684 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Created Pod/revision-pruner-6-ip-10-0-129-97.us-west-1.compute.internal -n openshift-kube-apiserver because it was missing Oct 21 22:46:01.067 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-129-97.us-west-1.compute.internal Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c18ce2736a4fed8091de16f868bc46e6a9a03b7956c2418bad42fbb041ffbafb\" already present on machine Oct 21 22:46:01.255 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-129-97.us-west-1.compute.internal Created container pruner Oct 21 22:46:01.294 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-129-97.us-west-1.compute.internal Started container pruner Oct 21 22:51:13.982 W ns/openshift-machine-config-operator pod/machine-config-daemon-nkwv7 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 22:51:14.037 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-kmjst Oct 21 22:51:14.037 I ns/openshift-machine-config-operator pod/machine-config-daemon-nkwv7 Stopping container machine-config-daemon Oct 21 22:51:14.038 I ns/openshift-image-registry pod/node-ca-kmjst Marking for deletion Pod openshift-image-registry/node-ca-kmjst Oct 21 22:51:14.038 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-nkwv7 Oct 21 22:51:14.038 W ns/openshift-image-registry pod/node-ca-kmjst node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 22:51:14.053 I ns/openshift-monitoring pod/alertmanager-main-1 Marking for deletion Pod openshift-monitoring/alertmanager-main-1 Oct 21 22:51:14.053 I ns/openshift-image-registry pod/node-ca-kmjst Stopping container node-ca Oct 21 22:51:14.053 I ns/openshift-monitoring pod/grafana-649f787944-bd9j2 Marking for deletion Pod openshift-monitoring/grafana-649f787944-bd9j2 Oct 21 22:51:14.054 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 22:51:14.109 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus Oct 21 22:51:14.109 W ns/openshift-monitoring pod/grafana-649f787944-bd9j2 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 22:51:14.110 W ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 22:51:14.110 W ns/openshift-monitoring pod/alertmanager-main-1 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 0s Oct 21 22:51:14.111 W ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-bsgkm node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 22:51:14.126 I ns/openshift-monitoring pod/prometheus-k8s-1 Marking for deletion Pod openshift-monitoring/prometheus-k8s-1 Oct 21 22:51:14.126 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container rules-configmap-reloader Oct 21 22:51:14.126 I ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 Marking for deletion Pod openshift-ingress/router-default-76df456ddd-9gdw7 Oct 21 22:51:14.126 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prom-label-proxy Oct 21 22:51:14.126 I ns/openshift-monitoring pod/grafana-649f787944-bd9j2 Stopping container grafana Oct 21 22:51:14.126 I ns/openshift-machine-config-operator pod/machine-config-daemon-nkwv7 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-nkwv7 Oct 21 22:51:14.126 W ns/openshift-monitoring pod/alertmanager-main-1 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:14.126 I ns/openshift-ingress pod/router-default-76df456ddd-9f8l9 node/ created Oct 21 22:51:14.126 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-mwcrw node/ created Oct 21 22:51:14.127 I ns/openshift-monitoring pod/grafana-649f787944-bd9j2 Stopping container grafana-proxy Oct 21 22:51:14.127 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-bsgkm Marking for deletion Pod openshift-monitoring/prometheus-adapter-7b86577c4-bsgkm Oct 21 22:51:14.127 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager Oct 21 22:51:14.127 I ns/openshift-monitoring pod/alertmanager-main-1 Cancelling deletion of Pod openshift-monitoring/alertmanager-main-1 Oct 21 22:51:14.127 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-bsgkm Stopping container prometheus-adapter Oct 21 22:51:14.180 I ns/openshift-ingress replicaset/router-default-76df456ddd Created pod: router-default-76df456ddd-9f8l9 Oct 21 22:51:14.180 I ns/openshift-ingress pod/router-default-76df456ddd-9f8l9 Successfully assigned openshift-ingress/router-default-76df456ddd-9f8l9 to ip-10-0-147-199.us-west-1.compute.internal Oct 21 22:51:14.201 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager-proxy Oct 21 22:51:14.202 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-mwcrw Successfully assigned openshift-monitoring/prometheus-adapter-7b86577c4-mwcrw to ip-10-0-142-54.us-west-1.compute.internal Oct 21 22:51:14.202 I ns/openshift-monitoring replicaset/prometheus-adapter-7b86577c4 Created pod: prometheus-adapter-7b86577c4-mwcrw Oct 21 22:51:14.219 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container config-reloader Oct 21 22:51:14.223 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc node/ created Oct 21 22:51:14.251 I ns/openshift-monitoring replicaset/grafana-649f787944 Created pod: grafana-649f787944-ln2tc Oct 21 22:51:14.265 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Successfully assigned openshift-monitoring/grafana-649f787944-ln2tc to ip-10-0-142-54.us-west-1.compute.internal Oct 21 22:51:14.313 I ns/openshift-monitoring pod/alertmanager-main-1 node/ created Oct 21 22:51:14.318 I ns/openshift-monitoring statefulset/alertmanager-main create Pod alertmanager-main-1 in StatefulSet alertmanager-main successful Oct 21 22:51:14.390 I ns/openshift-monitoring pod/alertmanager-main-1 Successfully assigned openshift-monitoring/alertmanager-main-1 to ip-10-0-147-199.us-west-1.compute.internal Oct 21 22:51:14.404 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager-proxy (2 times) Oct 21 22:51:14.597 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container config-reloader (2 times) Oct 21 22:51:14.798 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager (2 times) Oct 21 22:51:14.993 I ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 Stopping container router Oct 21 22:51:15.195 W ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 Liveness probe failed: Get http://10.129.2.3:1936/healthz: dial tcp 10.129.2.3:1936: connect: connection refused Oct 21 22:51:16.032 W ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-bsgkm node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 22:51:16.032 W ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-bsgkm node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-adapter container stopped being ready Oct 21 22:51:16.048 W ns/openshift-image-registry pod/node-ca-kmjst node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 22:51:16.048 W ns/openshift-image-registry pod/node-ca-kmjst node/ip-10-0-143-110.us-west-1.compute.internal container=node-ca container stopped being ready Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=rules-configmap-reloader container stopped being ready Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-config-reloader container stopped being ready Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-proxy container stopped being ready Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=kube-rbac-proxy container stopped being ready Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prom-label-proxy container stopped being ready Oct 21 22:51:16.067 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container stopped being ready Oct 21 22:51:16.195 W ns/openshift-machine-config-operator pod/machine-config-daemon-nkwv7 node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 22:51:16.195 W ns/openshift-machine-config-operator pod/machine-config-daemon-nkwv7 node/ip-10-0-143-110.us-west-1.compute.internal container=machine-config-daemon container stopped being ready Oct 21 22:51:16.596 W ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 22:51:16.596 W ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 node/ip-10-0-143-110.us-west-1.compute.internal container=router container stopped being ready Oct 21 22:51:16.996 E ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 node/ip-10-0-143-110.us-west-1.compute.internal container=router container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 22:51:17.396 W ns/openshift-monitoring pod/grafana-649f787944-bd9j2 node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 22:51:17.396 W ns/openshift-monitoring pod/grafana-649f787944-bd9j2 node/ip-10-0-143-110.us-west-1.compute.internal container=grafana container stopped being ready Oct 21 22:51:17.396 W ns/openshift-monitoring pod/grafana-649f787944-bd9j2 node/ip-10-0-143-110.us-west-1.compute.internal container=grafana-proxy container stopped being ready Oct 21 22:51:17.606 W ns/openshift-monitoring pod/grafana-649f787944-bd9j2 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:18.407 W ns/openshift-machine-config-operator pod/machine-config-daemon-nkwv7 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:18.418 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz node/ created Oct 21 22:51:18.422 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-z9qzz Oct 21 22:51:18.469 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz Successfully assigned openshift-machine-config-operator/machine-config-daemon-z9qzz to ip-10-0-143-110.us-west-1.compute.internal Oct 21 22:51:19.205 W ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-bsgkm node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:19.687 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 21 22:51:19.816 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz Created container machine-config-daemon Oct 21 22:51:19.848 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz Started container machine-config-daemon Oct 21 22:51:20.007 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:20.099 I ns/openshift-monitoring pod/prometheus-k8s-1 node/ created Oct 21 22:51:20.104 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-1 in StatefulSet prometheus-k8s successful Oct 21 22:51:20.122 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully assigned openshift-monitoring/prometheus-k8s-1 to ip-10-0-143-110.us-west-1.compute.internal Oct 21 22:51:21.145 W ns/openshift-image-registry pod/node-ca-kmjst node/ip-10-0-143-110.us-west-1.compute.internal pod has been pending longer than a minute Oct 21 22:51:21.145 W ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 node/ip-10-0-143-110.us-west-1.compute.internal pod has been pending longer than a minute Oct 21 22:51:22.334 I ns/openshift-ingress pod/router-default-76df456ddd-9f8l9 Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" Oct 21 22:51:22.606 W ns/openshift-image-registry pod/node-ca-kmjst node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:22.646 I ns/openshift-image-registry pod/node-ca-cxxsk node/ created Oct 21 22:51:22.653 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-cxxsk Oct 21 22:51:22.659 I ns/openshift-image-registry pod/node-ca-cxxsk Successfully assigned openshift-image-registry/node-ca-cxxsk to ip-10-0-143-110.us-west-1.compute.internal Oct 21 22:51:23.637 I ns/openshift-monitoring pod/alertmanager-main-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b9d07e8eed67a34b50a3afaa36cd4b9f052697894e64b5c203d0675d12ab5ec\" already present on machine Oct 21 22:51:23.797 I ns/openshift-monitoring pod/alertmanager-main-1 Created container alertmanager Oct 21 22:51:23.822 I ns/openshift-monitoring pod/alertmanager-main-1 Started container alertmanager Oct 21 22:51:23.829 I ns/openshift-monitoring pod/alertmanager-main-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Oct 21 22:51:24.001 I ns/openshift-monitoring pod/alertmanager-main-1 Created container config-reloader Oct 21 22:51:24.023 I ns/openshift-monitoring pod/alertmanager-main-1 Started container config-reloader Oct 21 22:51:24.030 I ns/openshift-monitoring pod/alertmanager-main-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Oct 21 22:51:24.177 I ns/openshift-monitoring pod/alertmanager-main-1 Created container alertmanager-proxy Oct 21 22:51:24.202 I ns/openshift-monitoring pod/alertmanager-main-1 Started container alertmanager-proxy Oct 21 22:51:24.648 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-mwcrw Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\" Oct 21 22:51:24.809 W ns/openshift-ingress pod/router-default-76df456ddd-9gdw7 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 22:51:25.454 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c229ac6674c217359c243e4621aae565c2af46caaf231a077b7f7e3d6f07ef5a\" Oct 21 22:51:26.938 I ns/openshift-ingress pod/router-default-76df456ddd-9f8l9 Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" Oct 21 22:51:27.092 I ns/openshift-ingress pod/router-default-76df456ddd-9f8l9 Created container router Oct 21 22:51:27.114 I ns/openshift-ingress pod/router-default-76df456ddd-9f8l9 Started container router Oct 21 22:51:29.407 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-mwcrw Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\" Oct 21 22:51:29.608 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-mwcrw Created container prometheus-adapter Oct 21 22:51:29.642 I ns/openshift-monitoring pod/prometheus-adapter-7b86577c4-mwcrw Started container prometheus-adapter Oct 21 22:51:29.927 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine Oct 21 22:51:30.097 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus Oct 21 22:51:30.125 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus Oct 21 22:51:30.132 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\" already present on machine Oct 21 22:51:30.284 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-config-reloader Oct 21 22:51:30.307 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-config-reloader Oct 21 22:51:30.315 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Oct 21 22:51:30.461 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-proxy Oct 21 22:51:30.488 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-proxy Oct 21 22:51:30.495 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Oct 21 22:51:30.661 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container kube-rbac-proxy Oct 21 22:51:30.686 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container kube-rbac-proxy Oct 21 22:51:30.693 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\" already present on machine Oct 21 22:51:30.840 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prom-label-proxy Oct 21 22:51:30.926 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prom-label-proxy Oct 21 22:51:31.107 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container exited with code 1 (Error): Oct 21 22:51:31.127 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Oct 21 22:51:31.326 I ns/openshift-image-registry pod/node-ca-cxxsk Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 21 22:51:31.527 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container rules-configmap-reloader Oct 21 22:51:31.726 I ns/openshift-image-registry pod/node-ca-cxxsk Created container node-ca Oct 21 22:51:31.927 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container rules-configmap-reloader Oct 21 22:51:32.126 I ns/openshift-image-registry pod/node-ca-cxxsk Started container node-ca Oct 21 22:51:32.330 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine (2 times) Oct 21 22:51:32.530 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus (2 times) Oct 21 22:51:32.730 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus (2 times) Oct 21 22:51:33.211 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c229ac6674c217359c243e4621aae565c2af46caaf231a077b7f7e3d6f07ef5a\" Oct 21 22:51:33.389 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Created container grafana Oct 21 22:51:33.413 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Started container grafana Oct 21 22:51:33.419 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Oct 21 22:51:33.563 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Created container grafana-proxy Oct 21 22:51:33.588 I ns/openshift-monitoring pod/grafana-649f787944-ln2tc Started container grafana-proxy Oct 21 22:51:33.946 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container restarted Oct 21 22:53:30.690 - 252s I test=\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" running Oct 21 22:55:31.482 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (9 times) Oct 21 22:55:32.491 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (5 times) Oct 21 22:55:33.359 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (5 times) Oct 21 22:55:34.374 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (6 times) Oct 21 22:55:34.527 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (8 times) Oct 21 22:55:34.641 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (9 times) Oct 21 22:57:43.292 I test=\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" failed Oct 21 23:05:31.509 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (10 times) Oct 21 23:05:32.527 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (6 times) Oct 21 23:05:33.484 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (6 times) Oct 21 23:05:34.311 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (7 times) Oct 21 23:05:34.457 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (9 times) Oct 21 23:05:34.580 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (10 times) Oct 21 23:07:41.413 W persistentvolume/pvc-2166e4e6-13f2-11eb-9b44-06dc8e64439d Error deleting EBS volume \"vol-069a135c4da9eb540\" since volume is currently attached to \"i-0681d725f9f4ff5a1\" Oct 21 23:09:46.745 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz Stopping container machine-config-daemon Oct 21 23:09:46.746 W ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 23:09:46.746 W ns/openshift-image-registry pod/node-ca-cxxsk node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 23:09:46.750 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-z9qzz Oct 21 23:09:46.750 I ns/openshift-monitoring pod/prometheus-k8s-1 Marking for deletion Pod openshift-monitoring/prometheus-k8s-1 (2 times) Oct 21 23:09:46.755 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 23:09:46.817 I ns/openshift-image-registry pod/node-ca-cxxsk Stopping container node-ca Oct 21 23:09:46.817 I ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-z9qzz Oct 21 23:09:46.817 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-cxxsk Oct 21 23:09:46.817 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus Oct 21 23:09:46.817 I ns/openshift-image-registry pod/node-ca-cxxsk Marking for deletion Pod openshift-image-registry/node-ca-cxxsk Oct 21 23:09:46.817 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus-config-reloader Oct 21 23:09:46.817 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus-proxy Oct 21 23:09:46.818 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prom-label-proxy Oct 21 23:09:46.818 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container rules-configmap-reloader Oct 21 23:09:46.824 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container kube-rbac-proxy Oct 21 23:09:47.797 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-config-reloader container exited with code 2 (Error): Oct 21 23:09:47.797 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-proxy container exited with code 2 (Error): Oct 21 23:09:47.797 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=rules-configmap-reloader container exited with code 2 (Error): Oct 21 23:09:49.568 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:09:49.638 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-1 in StatefulSet prometheus-k8s successful (2 times) Oct 21 23:09:49.638 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully assigned openshift-monitoring/prometheus-k8s-1 to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:09:49.639 I ns/openshift-monitoring pod/prometheus-k8s-1 node/ created Oct 21 23:09:52.108 W ns/openshift-image-registry pod/node-ca-cxxsk node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:09:52.168 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-wbk6b Oct 21 23:09:52.168 I ns/openshift-image-registry pod/node-ca-wbk6b Successfully assigned openshift-image-registry/node-ca-wbk6b to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:09:52.169 I ns/openshift-image-registry pod/node-ca-wbk6b node/ created Oct 21 23:09:59.932 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine Oct 21 23:10:00.024 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus Oct 21 23:10:00.050 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus Oct 21 23:10:00.058 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\" already present on machine Oct 21 23:10:00.205 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-config-reloader Oct 21 23:10:00.227 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-config-reloader Oct 21 23:10:00.235 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Oct 21 23:10:00.376 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-proxy Oct 21 23:10:00.406 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-proxy Oct 21 23:10:00.414 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Oct 21 23:10:00.591 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container kube-rbac-proxy Oct 21 23:10:00.614 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container kube-rbac-proxy Oct 21 23:10:00.622 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\" already present on machine Oct 21 23:10:00.755 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prom-label-proxy Oct 21 23:10:00.876 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prom-label-proxy Oct 21 23:10:01.076 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Oct 21 23:10:01.275 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container rules-configmap-reloader Oct 21 23:10:01.477 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container rules-configmap-reloader Oct 21 23:10:01.675 I ns/openshift-image-registry pod/node-ca-wbk6b Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 21 23:10:01.831 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container exited with code 1 (Error): Oct 21 23:10:01.875 I ns/openshift-image-registry pod/node-ca-wbk6b Created container node-ca Oct 21 23:10:02.093 I ns/openshift-image-registry pod/node-ca-wbk6b Started container node-ca Oct 21 23:10:02.278 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine (2 times) Oct 21 23:10:02.478 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus (2 times) Oct 21 23:10:02.678 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus (2 times) Oct 21 23:10:04.494 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container restarted Oct 21 23:10:41.293 W ns/openshift-machine-config-operator pod/machine-config-daemon-z9qzz node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:10:41.293 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 node/ created Oct 21 23:10:41.293 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-f9w59 Oct 21 23:10:41.295 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 Successfully assigned openshift-machine-config-operator/machine-config-daemon-f9w59 to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:10:41.922 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 21 23:10:42.038 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 Created container machine-config-daemon Oct 21 23:10:42.063 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 Started container machine-config-daemon Oct 21 23:15:32.427 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (7 times) Oct 21 23:15:33.353 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (8 times) Oct 21 23:15:33.508 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (10 times) Oct 21 23:15:33.633 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (11 times) Oct 21 23:15:33.772 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (11 times) Oct 21 23:15:34.721 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (7 times) Oct 21 23:15:37.573 W ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 23:15:37.575 W ns/openshift-image-registry pod/node-ca-wbk6b node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 23:15:37.588 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 Stopping container machine-config-daemon Oct 21 23:15:37.596 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-wbk6b Oct 21 23:15:37.596 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 23:15:37.598 I ns/openshift-image-registry pod/node-ca-wbk6b Stopping container node-ca Oct 21 23:15:37.626 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-f9w59 Oct 21 23:15:37.626 I ns/openshift-monitoring pod/prometheus-k8s-1 Marking for deletion Pod openshift-monitoring/prometheus-k8s-1 (3 times) Oct 21 23:15:37.626 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus Oct 21 23:15:37.626 I ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-f9w59 Oct 21 23:15:37.634 I ns/openshift-image-registry pod/node-ca-wbk6b Marking for deletion Pod openshift-image-registry/node-ca-wbk6b Oct 21 23:15:37.634 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container rules-configmap-reloader Oct 21 23:15:37.642 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prom-label-proxy Oct 21 23:15:37.651 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container kube-rbac-proxy Oct 21 23:15:37.658 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus-proxy Oct 21 23:15:39.561 W ns/openshift-machine-config-operator pod/machine-config-daemon-f9w59 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prom-label-proxy container stopped being ready Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container stopped being ready Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-config-reloader container stopped being ready Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-proxy container stopped being ready Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=rules-configmap-reloader container stopped being ready Oct 21 23:15:40.154 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=kube-rbac-proxy container stopped being ready Oct 21 23:15:40.827 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=kube-rbac-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 23:15:40.827 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prom-label-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 23:15:40.827 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 23:15:40.827 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 23:15:40.827 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=rules-configmap-reloader container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 23:15:40.827 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal container=prometheus-config-reloader container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Oct 21 23:15:51.145 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal pod has been pending longer than a minute Oct 21 23:15:51.331 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:15:51.364 W ns/openshift-image-registry pod/node-ca-wbk6b node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:15:51.378 I ns/openshift-monitoring pod/prometheus-k8s-1 node/ created Oct 21 23:15:51.386 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-1 in StatefulSet prometheus-k8s successful (3 times) Oct 21 23:15:51.401 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully assigned openshift-monitoring/prometheus-k8s-1 to ip-10-0-142-54.us-west-1.compute.internal Oct 21 23:15:59.368 I ns/openshift-monitoring pod/prometheus-k8s-1 Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" Oct 21 23:16:04.787 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" Oct 21 23:16:04.961 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus Oct 21 23:16:04.988 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus Oct 21 23:16:04.993 I ns/openshift-monitoring pod/prometheus-k8s-1 Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\" Oct 21 23:16:08.618 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\" Oct 21 23:16:08.790 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-config-reloader Oct 21 23:16:08.813 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-config-reloader Oct 21 23:16:08.818 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Oct 21 23:16:08.990 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-proxy Oct 21 23:16:09.012 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-proxy Oct 21 23:16:09.017 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Oct 21 23:16:09.163 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container kube-rbac-proxy Oct 21 23:16:09.185 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container kube-rbac-proxy Oct 21 23:16:09.191 I ns/openshift-monitoring pod/prometheus-k8s-1 Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\" Oct 21 23:16:12.833 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\" Oct 21 23:16:12.988 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prom-label-proxy Oct 21 23:16:13.011 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prom-label-proxy Oct 21 23:16:13.016 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Oct 21 23:16:13.157 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container rules-configmap-reloader Oct 21 23:16:13.179 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container rules-configmap-reloader Oct 21 23:16:13.989 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine Oct 21 23:16:14.067 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-142-54.us-west-1.compute.internal container=prometheus container exited with code 1 (Error): Oct 21 23:16:14.171 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus (2 times) Oct 21 23:16:14.281 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus (2 times) Oct 21 23:16:15.257 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-142-54.us-west-1.compute.internal container=prometheus container restarted Oct 21 23:16:51.441 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm node/ created Oct 21 23:16:51.447 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-crnnm Oct 21 23:16:51.456 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm Successfully assigned openshift-machine-config-operator/machine-config-daemon-crnnm to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:16:51.467 I ns/openshift-image-registry pod/node-ca-qg5rv node/ created Oct 21 23:16:51.473 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-qg5rv Oct 21 23:16:51.487 I ns/openshift-image-registry pod/node-ca-qg5rv Successfully assigned openshift-image-registry/node-ca-qg5rv to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:16:52.129 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 21 23:16:52.252 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm Created container machine-config-daemon Oct 21 23:16:52.278 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm Started container machine-config-daemon Oct 21 23:16:59.478 I ns/openshift-image-registry pod/node-ca-qg5rv Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 21 23:16:59.611 I ns/openshift-image-registry pod/node-ca-qg5rv Created container node-ca Oct 21 23:16:59.636 I ns/openshift-image-registry pod/node-ca-qg5rv Started container node-ca Oct 21 23:25:32.363 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (8 times) Oct 21 23:25:33.237 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (8 times) Oct 21 23:25:34.103 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (9 times) Oct 21 23:25:34.257 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (11 times) Oct 21 23:25:34.392 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (12 times) Oct 21 23:25:34.520 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (12 times) Oct 21 23:33:08.448 W ns/openshift-image-registry pod/node-ca-qg5rv node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 23:33:08.454 W ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 23:33:08.460 I ns/openshift-image-registry pod/node-ca-qg5rv Stopping container node-ca Oct 21 23:33:08.462 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-qg5rv Oct 21 23:33:08.467 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-crnnm Oct 21 23:33:08.468 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm Stopping container machine-config-daemon Oct 21 23:33:08.504 I ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-crnnm Oct 21 23:33:08.504 I ns/openshift-image-registry pod/node-ca-qg5rv Marking for deletion Pod openshift-image-registry/node-ca-qg5rv Oct 21 23:33:21.293 W ns/openshift-machine-config-operator pod/machine-config-daemon-crnnm node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:33:21.293 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd node/ created Oct 21 23:33:21.294 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-4tfsd Oct 21 23:33:21.294 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd Successfully assigned openshift-machine-config-operator/machine-config-daemon-4tfsd to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:33:21.329 W ns/openshift-image-registry pod/node-ca-qg5rv node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:33:21.365 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4brks Oct 21 23:33:21.365 I ns/openshift-image-registry pod/node-ca-4brks Successfully assigned openshift-image-registry/node-ca-4brks to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:33:21.365 I ns/openshift-image-registry pod/node-ca-4brks node/ created Oct 21 23:33:22.757 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 21 23:33:22.886 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd Created container machine-config-daemon Oct 21 23:33:22.915 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd Started container machine-config-daemon Oct 21 23:33:30.322 I ns/openshift-image-registry pod/node-ca-4brks Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 21 23:33:30.451 I ns/openshift-image-registry pod/node-ca-4brks Created container node-ca Oct 21 23:33:30.479 I ns/openshift-image-registry pod/node-ca-4brks Started container node-ca Oct 21 23:35:31.506 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (13 times) Oct 21 23:35:31.644 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (13 times) Oct 21 23:35:32.647 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (9 times) Oct 21 23:35:33.468 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (9 times) Oct 21 23:35:34.344 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (10 times) Oct 21 23:35:34.486 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (12 times) Oct 21 23:38:19.359 I ns/kube-system pod/pod0-system-node-critical node/ created Oct 21 23:38:19.376 I ns/kube-system pod/pod0-system-node-critical Successfully assigned kube-system/pod0-system-node-critical to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:38:19.435 I ns/kube-system pod/pod1-system-cluster-critical node/ created Oct 21 23:38:19.448 I ns/kube-system pod/pod1-system-cluster-critical Successfully assigned kube-system/pod1-system-cluster-critical to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:38:19.515 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 0s Oct 21 23:38:19.522 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:38:19.600 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 0s Oct 21 23:38:19.605 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:38:29.442 W ns/kube-system pod/pod0-system-node-critical Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_pod0-system-node-critical_kube-system_7fdfd1e7-13f6-11eb-be53-026094549e09_0(d55c5fa2b9920e466c16dfc42455b2d6954e838274f66236d03b7a461a5fa191): Multus: Err adding pod to network \"openshift-sdn\": cannot set \"openshift-sdn\" ifname to \"eth0\": no netns: failed to Statfs \"/proc/76949/ns/net\": no such file or directory Oct 21 23:40:22.469 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \"pod1-system-cluster-critical_kube-system(7feb8674-13f6-11eb-be53-026094549e09)\": timeout expired waiting for volumes to attach or mount for pod \"kube-system\"/\"pod1-system-cluster-critical\". list of unmounted volumes=[default-token-z8dnz]. list of unattached volumes=[default-token-z8dnz] Oct 21 23:45:31.493 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (14 times) Oct 21 23:45:32.666 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (10 times) Oct 21 23:45:33.542 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (10 times) Oct 21 23:45:34.428 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (11 times) Oct 21 23:45:34.565 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (13 times) Oct 21 23:45:34.690 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (14 times) Oct 21 23:52:33.361 W ns/openshift-image-registry pod/node-ca-4brks node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 21 23:52:33.417 I ns/openshift-image-registry pod/node-ca-4brks Stopping container node-ca Oct 21 23:52:33.417 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-4tfsd Oct 21 23:52:33.417 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-4tfsd Oct 21 23:52:33.417 I ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd Stopping container machine-config-daemon Oct 21 23:52:33.417 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4brks Oct 21 23:52:33.417 I ns/openshift-image-registry pod/node-ca-4brks Marking for deletion Pod openshift-image-registry/node-ca-4brks Oct 21 23:52:33.417 W ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 21 23:52:41.267 W ns/openshift-image-registry pod/node-ca-4brks node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:52:41.341 W ns/openshift-machine-config-operator pod/machine-config-daemon-4tfsd node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 21 23:53:38.596 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 node/ created Oct 21 23:53:38.654 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-5vxz9 Oct 21 23:53:38.655 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 Successfully assigned openshift-machine-config-operator/machine-config-daemon-5vxz9 to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:53:38.655 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-srv2w Oct 21 23:53:38.655 I ns/openshift-image-registry pod/node-ca-srv2w Successfully assigned openshift-image-registry/node-ca-srv2w to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:53:38.655 I ns/openshift-image-registry pod/node-ca-srv2w node/ created Oct 21 23:53:39.538 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 21 23:53:39.699 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 Created container machine-config-daemon Oct 21 23:53:39.745 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 Started container machine-config-daemon Oct 21 23:53:46.745 I ns/openshift-image-registry pod/node-ca-srv2w Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 21 23:53:46.874 I ns/openshift-image-registry pod/node-ca-srv2w Created container node-ca Oct 21 23:53:46.898 I ns/openshift-image-registry pod/node-ca-srv2w Started container node-ca Oct 21 23:55:32.455 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (11 times) Oct 21 23:55:33.436 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (11 times) Oct 21 23:55:34.378 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (12 times) Oct 21 23:55:34.524 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (14 times) Oct 21 23:55:34.703 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (15 times) Oct 21 23:55:34.817 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (15 times) Oct 21 23:57:40.624 I ns/kube-system pod/critical-pod node/ created Oct 21 23:57:40.630 W ns/kube-system pod/critical-pod 0/6 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 3 node(s) had taints that the pod didn\\'t tolerate. Oct 21 23:57:40.696 W ns/kube-system pod/critical-pod 0/6 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 3 node(s) had taints that the pod didn\\'t tolerate. (2 times) Oct 21 23:57:51.293 I ns/kube-system pod/critical-pod Successfully assigned kube-system/critical-pod to ip-10-0-143-110.us-west-1.compute.internal Oct 21 23:57:59.097 I ns/kube-system pod/critical-pod Container image \"k8s.gcr.io/pause:3.1\" already present on machine Oct 21 23:57:59.236 I ns/kube-system pod/critical-pod Created container critical-pod Oct 21 23:57:59.293 I ns/kube-system pod/critical-pod Started container critical-pod Oct 21 23:58:01.177 W ns/kube-system pod/critical-pod node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 0s Oct 21 23:58:01.182 W ns/kube-system pod/critical-pod node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 22 00:02:52.678 W ns/openshift-image-registry pod/node-ca-srv2w node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 22 00:02:52.680 W ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 22 00:02:52.692 I ns/openshift-image-registry pod/node-ca-srv2w Stopping container node-ca Oct 22 00:02:52.692 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-srv2w Oct 22 00:02:52.705 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 Stopping container machine-config-daemon Oct 22 00:02:52.705 I ns/openshift-image-registry pod/node-ca-srv2w Marking for deletion Pod openshift-image-registry/node-ca-srv2w Oct 22 00:02:52.705 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-5vxz9 Oct 22 00:02:52.735 I ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-5vxz9 Oct 22 00:02:54.231 W ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 22 00:02:54.231 W ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 node/ip-10-0-143-110.us-west-1.compute.internal container=machine-config-daemon container stopped being ready Oct 22 00:02:54.294 W ns/openshift-image-registry pod/node-ca-srv2w node/ip-10-0-143-110.us-west-1.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Oct 22 00:02:54.294 W ns/openshift-image-registry pod/node-ca-srv2w node/ip-10-0-143-110.us-west-1.compute.internal container=node-ca container stopped being ready Oct 22 00:02:55.293 W ns/openshift-image-registry pod/node-ca-srv2w node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 22 00:03:01.294 W ns/openshift-machine-config-operator pod/machine-config-daemon-5vxz9 node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 22 00:03:57.916 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b node/ created Oct 22 00:03:57.926 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-2th9b Oct 22 00:03:57.937 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b Successfully assigned openshift-machine-config-operator/machine-config-daemon-2th9b to ip-10-0-143-110.us-west-1.compute.internal Oct 22 00:03:57.970 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-56gsn Oct 22 00:03:57.970 I ns/openshift-image-registry pod/node-ca-56gsn Successfully assigned openshift-image-registry/node-ca-56gsn to ip-10-0-143-110.us-west-1.compute.internal Oct 22 00:03:57.971 I ns/openshift-image-registry pod/node-ca-56gsn node/ created Oct 22 00:03:58.593 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 22 00:03:58.719 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b Created container machine-config-daemon Oct 22 00:03:58.745 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b Started container machine-config-daemon Oct 22 00:04:05.572 I ns/openshift-image-registry pod/node-ca-56gsn Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 22 00:04:05.696 I ns/openshift-image-registry pod/node-ca-56gsn Created container node-ca Oct 22 00:04:05.723 I ns/openshift-image-registry pod/node-ca-56gsn Started container node-ca Oct 22 00:05:32.333 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-0 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-0 (12 times) Oct 22 00:05:33.197 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-1 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-1 (12 times) Oct 22 00:05:34.049 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-master-2 Updated machine ci-op-vv1cddgl-7bc5c-wghkj-master-2 (13 times) Oct 22 00:05:34.207 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-5xgjp (15 times) Oct 22 00:05:34.348 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1a-6qz5r (16 times) Oct 22 00:05:34.471 I ns/openshift-machine-api machine/ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw Updated machine ci-op-vv1cddgl-7bc5c-wghkj-worker-us-west-1b-zkvlw (16 times) Oct 22 00:07:13.781 W ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 600s Oct 22 00:07:13.840 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b Stopping container machine-config-daemon Oct 22 00:07:13.840 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-2th9b Oct 22 00:07:13.840 I ns/openshift-image-registry pod/node-ca-56gsn Stopping container node-ca Oct 22 00:07:13.841 I ns/openshift-image-registry pod/node-ca-56gsn Marking for deletion Pod openshift-image-registry/node-ca-56gsn Oct 22 00:07:13.841 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-56gsn Oct 22 00:07:13.841 I ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-2th9b Oct 22 00:07:13.841 W ns/openshift-image-registry pod/node-ca-56gsn node/ip-10-0-143-110.us-west-1.compute.internal graceful deletion within 30s Oct 22 00:07:15.648 W ns/openshift-image-registry pod/node-ca-56gsn node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 22 00:07:15.709 W ns/openshift-machine-config-operator pod/machine-config-daemon-2th9b node/ip-10-0-143-110.us-west-1.compute.internal deleted Oct 22 00:07:51.434 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf node/ created Oct 22 00:07:51.439 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-qwgwf Oct 22 00:07:51.453 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Successfully assigned openshift-machine-config-operator/machine-config-daemon-qwgwf to ip-10-0-143-110.us-west-1.compute.internal Oct 22 00:07:51.487 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-7qmdn Oct 22 00:07:51.487 I ns/openshift-image-registry pod/node-ca-7qmdn node/ created Oct 22 00:07:51.492 I ns/openshift-image-registry pod/node-ca-7qmdn Successfully assigned openshift-image-registry/node-ca-7qmdn to ip-10-0-143-110.us-west-1.compute.internal Oct 22 00:07:52.365 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 22 00:07:52.506 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Created container machine-config-daemon Oct 22 00:07:52.533 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Started container machine-config-daemon Oct 22 00:07:59.615 I ns/openshift-image-registry pod/node-ca-7qmdn Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 22 00:07:59.741 I ns/openshift-image-registry pod/node-ca-7qmdn Created container node-ca Oct 22 00:07:59.767 I ns/openshift-image-registry pod/node-ca-7qmdn Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201022-000924.xml error: 1 fail, 39 pass, 39 skip (1h23m33s) 2020/10/22 00:09:24 Container test in pod e2e-aws-serial failed, exit code 1, reason Error 2020/10/22 00:15:50 Copied 125.68MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/22 00:15:50 Releasing lease for \"aws-quota-slice\" 2020/10/22 00:15:50 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/22 00:15:51 Ran for 2h0m24s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-vv1cddgl/e2e-aws-serial failed after 1h59m16s (failed containers: test): ContainerFailed one or more containers exited Container test exited with code 1, reason Error --- nternal deleted Oct 22 00:07:51.434 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf node/ created Oct 22 00:07:51.439 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-qwgwf Oct 22 00:07:51.453 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Successfully assigned openshift-machine-config-operator/machine-config-daemon-qwgwf to ip-10-0-143-110.us-west-1.compute.internal Oct 22 00:07:51.487 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-7qmdn Oct 22 00:07:51.487 I ns/openshift-image-registry pod/node-ca-7qmdn node/ created Oct 22 00:07:51.492 I ns/openshift-image-registry pod/node-ca-7qmdn Successfully assigned openshift-image-registry/node-ca-7qmdn to ip-10-0-143-110.us-west-1.compute.internal Oct 22 00:07:52.365 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Oct 22 00:07:52.506 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Created container machine-config-daemon Oct 22 00:07:52.533 I ns/openshift-machine-config-operator pod/machine-config-daemon-qwgwf Started container machine-config-daemon Oct 22 00:07:59.615 I ns/openshift-image-registry pod/node-ca-7qmdn Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Oct 22 00:07:59.741 I ns/openshift-image-registry pod/node-ca-7qmdn Created container node-ca Oct 22 00:07:59.767 I ns/openshift-image-registry pod/node-ca-7qmdn Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201022-000924.xml error: 1 fail, 39 pass, 39 skip (1h23m33s) --- '\n",
            "ID=16    : size=1         : b'2020/10/22 22:16:14 ci-operator version v20201022-5a97895 2020/10/22 22:16:14 No source defined 2020/10/22 22:16:14 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/10/22 22:16:14 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-h6z8vdkq 2020/10/22 22:16:14 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/10/22 22:16:14 Creating namespace ci-op-h6z8vdkq 2020/10/22 22:16:14 Setting up pipeline imagestream for the test 2020/10/22 22:16:14 Created secret e2e-aws-serial-cluster-profile 2020/10/22 22:16:14 Created secret pull-secret 2020/10/22 22:16:14 Created PDB for pods with openshift.io/build.name label 2020/10/22 22:16:14 Created PDB for pods with created-by-ci label 2020/10/22 22:16:14 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-h6z8vdkq/stable:${component} 2020/10/22 22:16:16 Importing release image latest 2020/10/22 22:16:17 Executing pod \"release-images-latest-cli\" 2020/10/22 22:16:29 Executing pod \"release-images-latest\" 2020/10/22 22:17:22 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/10/22 22:17:22 Acquiring lease for \"aws-quota-slice\" 2020/10/22 22:17:22 Acquired lease \"ea3120df-7633-4f58-9a56-701fcac10196\" for \"aws-quota-slice\" 2020/10/22 22:17:22 Executing template e2e-aws-serial 2020/10/22 22:17:22 Creating or restarting template instance 2020/10/22 22:17:22 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/10/22 22:17:22 Waiting for template instance to be ready 2020/10/22 22:17:24 Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-1 (zones: us-east-1b us-east-1c) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=warning msg=\"Found override for ReleaseImage. Please be warned, this is not advised\" level=info msg=\"Consuming \\\\\"Master Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Worker Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Openshift Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Common Manifests\\\\\" from target directory\" level=info msg=\"Creating infrastructure resources...\" level=error level=error msg=\"Error: Error applying plan:\" level=error level=error msg=\"1 error occurred:\" level=error msg=\"\\\\t* module.masters.aws_instance.master[2]: 1 error occurred:\" level=error msg=\"\\\\t* aws_instance.master.2: Error waiting for instance (i-08d1bca630d4287a5) to become ready: Failed to reach target state. Reason: Server.InternalError: Internal error on launch\" level=error level=error level=error level=error level=error level=error msg=\"Terraform does not automatically rollback in the face of errors.\" level=error msg=\"Instead, your Terraform state file has been partially updated with\" level=error msg=\"any resources that successfully completed. Please address the error\" level=error msg=\"above and apply again to incrementally change your infrastructure.\" level=error level=error level=fatal msg=\"failed to fetch Cluster: failed to generate asset \\\\\"Cluster\\\\\": failed to create cluster: failed to apply using Terraform\" 2020/10/22 22:24:37 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2020/10/22 22:29:33 Copied 6.97MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/10/22 22:29:33 Releasing lease for \"aws-quota-slice\" 2020/10/22 22:29:33 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/10/22 22:29:33 Ran for 13m19s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-h6z8vdkq/e2e-aws-serial failed after 12m8s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-1 (zones: us-east-1b us-east-1c) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=warning msg=\"Found override for ReleaseImage. Please be warned, this is not advised\" level=info msg=\"Consuming \\\\\"Master Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Worker Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Openshift Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Common Manifests\\\\\" from target directory\" level=info msg=\"Creating infrastructure resources...\" level=error level=error msg=\"Error: Error applying plan:\" level=error level=error msg=\"1 error occurred:\" level=error msg=\"\\\\t* module.masters.aws_instance.master[2]: 1 error occurred:\" level=error msg=\"\\\\t* aws_instance.master.2: Error waiting for instance (i-08d1bca630d4287a5) to become ready: Failed to reach target state. Reason: Server.InternalError: Internal error on launch\" level=error level=error level=error level=error level=error level=error msg=\"Terraform does not automatically rollback in the face of errors.\" level=error msg=\"Instead, your Terraform state file has been partially updated with\" level=error msg=\"any resources that successfully completed. Please address the error\" level=error msg=\"above and apply again to incrementally change your infrastructure.\" level=error level=error level=fatal msg=\"failed to fetch Cluster: failed to generate asset \\\\\"Cluster\\\\\": failed to create cluster: failed to apply using Terraform\" --- '\n",
            "ID=17    : size=2         : <*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \"release-images-latest-cli\" <*> <*> Executing pod \"release-images-latest\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring lease for \"aws-quota-slice\" <*> <*> Acquired lease <*> for \"aws-quota-slice\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing lease for \"aws-quota-slice\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\n",
            "ID=18    : size=8         : <*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \"release-images-latest-cli\" <*> <*> Executing pod \"release-images-latest\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \"e2e-aws-serial\" <*> <*> Acquiring lease for \"aws-quota-slice\" <*> <*> Acquired lease <*> for \"aws-quota-slice\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \"e2e-aws-serial\" <*> <*> Releasing lease <*> for \"aws-quota-slice\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\n",
            "ID=19    : size=6         : <*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \"release-images-latest-cli\" <*> <*> Executing pod \"release-images-latest\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \"e2e-aws-serial\" <*> <*> Acquiring lease for \"aws-quota-slice\" <*> <*> Acquired lease <*> for \"aws-quota-slice\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \"e2e-aws-serial\" <*> <*> Releasing lease <*> for \"aws-quota-slice\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\n",
            "ID=20    : size=5         : <*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> warning: overriding parameter \"LEASED_RESOURCE\" <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \"release-images-latest-cli\" <*> <*> Executing pod \"release-images-latest\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \"e2e-aws-serial\" <*> <*> Acquiring lease for \"aws-quota-slice\" <*> <*> Acquired lease <*> for \"aws-quota-slice\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \"e2e-aws-serial\" <*> <*> Releasing lease <*> for \"aws-quota-slice\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\n",
            "ID=21    : size=1         : b'2020/11/14 22:34:12 ci-operator version v20201113-e46b6a4 2020/11/14 22:34:12 No source defined 2020/11/14 22:34:12 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/11/14 22:34:12 warning: overriding parameter \"LEASED_RESOURCE\" 2020/11/14 22:34:12 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-i12kcdn7 2020/11/14 22:34:12 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/11/14 22:34:12 Creating namespace ci-op-i12kcdn7 2020/11/14 22:34:12 Setting up pipeline imagestream for the test 2020/11/14 22:34:12 Created secret e2e-aws-serial-cluster-profile 2020/11/14 22:34:12 Created secret pull-secret 2020/11/14 22:34:12 Created PDB for pods with openshift.io/build.name label 2020/11/14 22:34:12 Created PDB for pods with created-by-ci label 2020/11/14 22:34:12 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-i12kcdn7/stable:${component} 2020/11/14 22:34:16 Importing release image latest 2020/11/14 22:34:17 Executing pod \"release-images-latest-cli\" 2020/11/14 22:34:30 Executing pod \"release-images-latest\" 2020/11/14 22:35:24 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/11/14 22:35:24 Acquiring leases for \"e2e-aws-serial\" 2020/11/14 22:35:24 Acquiring lease for \"aws-quota-slice\" 2020/11/14 22:35:24 Acquired lease \"fd5aff4f-18ec-4542-a726-2510deb7a53d\" for \"aws-quota-slice\" 2020/11/14 22:35:24 Executing template e2e-aws-serial 2020/11/14 22:35:24 Creating or restarting template instance 2020/11/14 22:35:24 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/11/14 22:35:24 Waiting for template instance to be ready 2020/11/14 22:35:26 Running pod e2e-aws-serial 2020/11/14 23:03:09 Container setup in pod e2e-aws-serial completed successfully 2020/11/15 00:12:58 Container test in pod e2e-aws-serial completed successfully 2020/11/15 00:19:23 Copied 108.07MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/11/15 00:19:24 Releasing leases for \"e2e-aws-serial\" 2020/11/15 00:19:24 Releasing lease \"fd5aff4f-18ec-4542-a726-2510deb7a53d\" for \"aws-quota-slice\" 2020/11/15 00:19:24 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/11/15 00:19:24 Ran for 1h45m12s '\n",
            "ID=22    : size=2         : <*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \"release-images-latest-cli\" <*> <*> Executing pod \"release-images-latest\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \"e2e-aws-serial\" <*> <*> Acquiring lease for \"aws-quota-slice\" <*> <*> Acquired lease <*> for \"aws-quota-slice\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \"e2e-aws-serial\" <*> <*> Releasing lease <*> for \"aws-quota-slice\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\n",
            "ID=23    : size=1         : b'2020/11/19 22:39:26 ci-operator version v20201119-ff03225 2020/11/19 22:39:26 No source defined 2020/11/19 22:39:26 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/11/19 22:39:26 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-bxig87c1 2020/11/19 22:39:26 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/11/19 22:39:26 Creating namespace ci-op-bxig87c1 2020/11/19 22:39:26 Setting up pipeline imagestream for the test 2020/11/19 22:39:26 Created secret e2e-aws-serial-cluster-profile 2020/11/19 22:39:26 Created secret pull-secret 2020/11/19 22:39:26 Created PDB for pods with openshift.io/build.name label 2020/11/19 22:39:26 Created PDB for pods with created-by-ci label 2020/11/19 22:39:26 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-bxig87c1/stable:${component} 2020/11/19 22:39:28 Importing release image latest 2020/11/19 22:39:29 Executing pod \"release-images-latest-cli\" 2020/11/19 22:39:34 Executing pod \"release-images-latest\" 2020/11/19 22:41:00 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/11/19 22:41:00 Acquiring leases for \"e2e-aws-serial\" 2020/11/19 22:41:00 Acquiring lease for \"aws-quota-slice\" 2020/11/19 22:41:00 Acquired lease \"d65cd353-d217-4fbd-ba5b-aa4a522fb068\" for \"aws-quota-slice\" 2020/11/19 22:41:00 Executing template e2e-aws-serial 2020/11/19 22:41:00 Creating or restarting template instance 2020/11/19 22:41:00 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/11/19 22:41:00 Waiting for template instance to be ready 2020/11/19 22:41:02 Running pod e2e-aws-serial 2020/11/19 23:16:21 Container setup in pod e2e-aws-serial completed successfully secret/support created started: (0/1/79) \"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\" passed: (1m2s) 2020-11-19T23:17:30 \"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\" started: (0/2/79) \"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m4s) 2020-11-19T23:19:34 \"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/3/79) \"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/scheduling/taint_based_evictions.go:63]: Requires at least 2 nodes (not -1) skipped: (20.4s) 2020-11-19T23:19:54 \"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/4/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (26.4s) 2020-11-19T23:20:21 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/5/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (23s) 2020-11-19T23:20:44 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/6/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-19T23:21:24 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/7/79) \"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m2s) 2020-11-19T23:23:26 \"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/8/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m54s) 2020-11-19T23:26:20 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/9/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (22.6s) 2020-11-19T23:26:42 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/10/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40s) 2020-11-19T23:27:22 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/11/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (50.6s) 2020-11-19T23:28:13 \"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/12/79) \"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m28s) 2020-11-19T23:29:41 \"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/13/79) \"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/apps/daemon_set.go:378]: Requires at least 2 nodes (not -1) skipped: (21.5s) 2020-11-19T23:30:02 \"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/14/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-19T23:30:42 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/15/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-19T23:31:23 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/16/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24s) 2020-11-19T23:31:47 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/17/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m49s) 2020-11-19T23:34:36 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/18/79) \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (37.9s) 2020-11-19T23:35:14 \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/19/79) \"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m5s) 2020-11-19T23:36:18 \"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/20/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.2s) 2020-11-19T23:36:59 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/21/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m30s) 2020-11-19T23:38:28 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/22/79) \"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.8s) 2020-11-19T23:38:49 \"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/23/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-19T23:39:30 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/24/79) \"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (41.7s) 2020-11-19T23:40:11 \"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/25/79) \"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:193]: Could not get controller-manager metrics - skipping skipped: (1m9s) 2020-11-19T23:41:21 \"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/26/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m37s) 2020-11-19T23:43:58 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/27/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m5s) 2020-11-19T23:46:03 \"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/28/79) \"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\" passed: (37.1s) 2020-11-19T23:46:40 \"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\" started: (0/29/79) \"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m48s) 2020-11-19T23:48:28 \"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/30/79) \"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\" skip [github.com/openshift/origin/test/extended/images/signatures.go:25]: disable because containers/image: https://github.com/containers/image/pull/570 skipped: (15.2s) 2020-11-19T23:48:44 \"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\" started: (0/31/79) \"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:247]: Could not get controller-manager metrics - skipping skipped: (28.6s) 2020-11-19T23:49:12 \"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/32/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.1s) 2020-11-19T23:49:36 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/33/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.3s) 2020-11-19T23:50:01 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/34/79) \"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m36s) 2020-11-19T23:51:37 \"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/35/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24s) 2020-11-19T23:52:01 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/36/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m44s) 2020-11-19T23:54:44 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/37/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m0s) 2020-11-19T23:56:45 \"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/38/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.5s) 2020-11-19T23:57:09 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/39/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-11-19T23:57:49 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/40/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.2s) 2020-11-19T23:58:30 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/41/79) \"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:81]: Environment does not support getting controller-manager metrics - skipping skipped: (20.7s) 2020-11-19T23:58:50 \"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/42/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m56s) 2020-11-20T00:00:46 \"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/43/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-20T00:01:27 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/44/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (43.6s) 2020-11-20T00:02:10 \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/45/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-20T00:02:51 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/46/79) \"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (2m10s) 2020-11-20T00:05:01 \"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/47/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-11-20T00:05:41 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/48/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m33s) 2020-11-20T00:08:14 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/49/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-20T00:08:54 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/50/79) \"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.8s) 2020-11-20T00:09:15 \"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/51/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" Nov 20 00:09:16.498: INFO: >>> kubeConfig: /tmp/admin.kubeconfig Nov 20 00:09:16.500: INFO: Waiting up to 30m0s for all (but 100) nodes to be schedulable Nov 20 00:09:17.021: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace \\'kube-system\\' to be running and ready Nov 20 00:09:17.286: INFO: 0 / 0 pods in namespace \\'kube-system\\' are running and ready (0 seconds elapsed) Nov 20 00:09:17.286: INFO: expected 0 pod replicas in namespace \\'kube-system\\', 0 are Running and Ready. Nov 20 00:09:17.286: INFO: Waiting up to 5m0s for all daemonsets in namespace \\'kube-system\\' to start Nov 20 00:09:17.379: INFO: e2e test version: v1.13.4-138-g41dc99c Nov 20 00:09:17.461: INFO: kube-apiserver version: v1.13.4-138-g41dc99c [BeforeEach] [Top Level] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/test/extended/util/test.go:69 [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:153 STEP: Creating a kubernetes client Nov 20 00:09:17.463: INFO: >>> kubeConfig: /tmp/admin.kubeconfig STEP: Building a namespace api object, basename sched-priority Nov 20 00:09:22.594: INFO: About to run a Kube e2e test, ensuring namespace is privileged Nov 20 00:09:23.627: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled. STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:71 Nov 20 00:09:23.710: INFO: Waiting up to 1m0s for all nodes to be ready Nov 20 00:10:24.824: INFO: Waiting for terminating namespaces to be deleted... Nov 20 00:10:24.909: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace \\'kube-system\\' to be running and ready Nov 20 00:10:25.163: INFO: 0 / 0 pods in namespace \\'kube-system\\' are running and ready (0 seconds elapsed) Nov 20 00:10:25.163: INFO: expected 0 pod replicas in namespace \\'kube-system\\', 0 are Running and Ready. [It] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:146 Nov 20 00:10:25.163: INFO: ComputeCpuMemFraction for node: ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:10:25.264: INFO: Pod for on the node: tuned-jfbcn, Cpu: 10, Mem: 20971520 Nov 20 00:10:25.264: INFO: Pod for on the node: dns-default-9bb6j, Cpu: 110, Mem: 283115520 Nov 20 00:10:25.264: INFO: Pod for on the node: node-ca-rdx58, Cpu: 10, Mem: 10485760 Nov 20 00:10:25.264: INFO: Pod for on the node: machine-config-daemon-2nwhn, Cpu: 20, Mem: 52428800 Nov 20 00:10:25.264: INFO: Pod for on the node: node-exporter-dxfx8, Cpu: 110, Mem: 230686720 Nov 20 00:10:25.264: INFO: Pod for on the node: multus-z4jlm, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.264: INFO: Pod for on the node: ovs-9dthp, Cpu: 200, Mem: 419430400 Nov 20 00:10:25.264: INFO: Pod for on the node: sdn-dfk44, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.264: INFO: Node: ip-10-0-132-239.us-west-2.compute.internal, totalRequestedCpuResource: 660, cpuAllocatableMil: 3500, cpuFraction: 0.18857142857142858 Nov 20 00:10:25.264: INFO: Node: ip-10-0-132-239.us-west-2.compute.internal, totalRequestedMemResource: 1331691520, memAllocatableVal: 16181792768, memFraction: 0.08229567261752736 Nov 20 00:10:25.264: INFO: ComputeCpuMemFraction for node: ip-10-0-134-44.us-west-2.compute.internal Nov 20 00:10:25.371: INFO: Pod for on the node: tuned-wgwsl, Cpu: 10, Mem: 20971520 Nov 20 00:10:25.371: INFO: Pod for on the node: dns-default-p6jkj, Cpu: 110, Mem: 283115520 Nov 20 00:10:25.371: INFO: Pod for on the node: image-registry-54fd967995-sjxmw, Cpu: 100, Mem: 268435456 Nov 20 00:10:25.371: INFO: Pod for on the node: node-ca-xkjnr, Cpu: 10, Mem: 10485760 Nov 20 00:10:25.371: INFO: Pod for on the node: router-default-756b45d66d-dtcjj, Cpu: 100, Mem: 268435456 Nov 20 00:10:25.371: INFO: Pod for on the node: machine-config-daemon-9cdtt, Cpu: 20, Mem: 52428800 Nov 20 00:10:25.371: INFO: Pod for on the node: certified-operators-74db69bc55-vjs88, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.371: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Nov 20 00:10:25.371: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Nov 20 00:10:25.371: INFO: Pod for on the node: grafana-649f787944-7r225, Cpu: 200, Mem: 314572800 Nov 20 00:10:25.371: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-kt9sp, Cpu: 300, Mem: 629145600 Nov 20 00:10:25.371: INFO: Pod for on the node: node-exporter-9xlm7, Cpu: 110, Mem: 230686720 Nov 20 00:10:25.371: INFO: Pod for on the node: prometheus-adapter-6b8bf7fcb6-c9ccp, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.371: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Nov 20 00:10:25.372: INFO: Pod for on the node: multus-8sxsv, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.372: INFO: Pod for on the node: ovs-cxp2j, Cpu: 200, Mem: 419430400 Nov 20 00:10:25.372: INFO: Pod for on the node: sdn-b6gvc, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.372: INFO: Node: ip-10-0-134-44.us-west-2.compute.internal, totalRequestedCpuResource: 1660, cpuAllocatableMil: 3500, cpuFraction: 0.4742857142857143 Nov 20 00:10:25.372: INFO: Node: ip-10-0-134-44.us-west-2.compute.internal, totalRequestedMemResource: 3441426432, memAllocatableVal: 16181800960, memFraction: 0.21267264629610177 Nov 20 00:10:25.372: INFO: ComputeCpuMemFraction for node: ip-10-0-146-237.us-west-2.compute.internal Nov 20 00:10:25.473: INFO: Pod for on the node: tuned-69g6d, Cpu: 10, Mem: 20971520 Nov 20 00:10:25.473: INFO: Pod for on the node: dns-default-9tv4w, Cpu: 110, Mem: 283115520 Nov 20 00:10:25.473: INFO: Pod for on the node: node-ca-fl47z, Cpu: 10, Mem: 10485760 Nov 20 00:10:25.473: INFO: Pod for on the node: router-default-756b45d66d-dc2ds, Cpu: 100, Mem: 268435456 Nov 20 00:10:25.473: INFO: Pod for on the node: machine-config-daemon-rtlm7, Cpu: 20, Mem: 52428800 Nov 20 00:10:25.473: INFO: Pod for on the node: community-operators-65d8c88b5c-lrgl8, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.473: INFO: Pod for on the node: redhat-operators-598f74f9b-h42zk, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.473: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Nov 20 00:10:25.473: INFO: Pod for on the node: node-exporter-tjp24, Cpu: 110, Mem: 230686720 Nov 20 00:10:25.473: INFO: Pod for on the node: prometheus-adapter-6b8bf7fcb6-mwh87, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.473: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Nov 20 00:10:25.473: INFO: Pod for on the node: prometheus-operator-5d4588dd6-d6zf5, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.473: INFO: Pod for on the node: telemeter-client-5d87684b5-lzp2x, Cpu: 210, Mem: 440401920 Nov 20 00:10:25.473: INFO: Pod for on the node: multus-l7gw6, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.473: INFO: Pod for on the node: ovs-rkp57, Cpu: 200, Mem: 419430400 Nov 20 00:10:25.473: INFO: Pod for on the node: sdn-jvhnl, Cpu: 100, Mem: 209715200 Nov 20 00:10:25.473: INFO: Node: ip-10-0-146-237.us-west-2.compute.internal, totalRequestedCpuResource: 1270, cpuAllocatableMil: 3500, cpuFraction: 0.3628571428571429 Nov 20 00:10:25.473: INFO: Node: ip-10-0-146-237.us-west-2.compute.internal, totalRequestedMemResource: 2669674496, memAllocatableVal: 16181800960, memFraction: 0.16498006016754269 Nov 20 00:10:25.570: INFO: Waiting for running... Nov 20 00:10:35.761: INFO: Waiting for running... Nov 20 00:10:50.953: INFO: Waiting for running... STEP: Compute Cpu, Mem Fraction after create balanced pods. Nov 20 00:11:01.055: INFO: ComputeCpuMemFraction for node: ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:11:01.571: INFO: Pod for on the node: c9ee7b37-2ac4-11eb-9365-0a58ac104a46-0, Cpu: 1089, Mem: 6759204864 Nov 20 00:11:01.571: INFO: Pod for on the node: tuned-jfbcn, Cpu: 10, Mem: 20971520 Nov 20 00:11:01.571: INFO: Pod for on the node: dns-default-9bb6j, Cpu: 110, Mem: 283115520 Nov 20 00:11:01.571: INFO: Pod for on the node: node-ca-rdx58, Cpu: 10, Mem: 10485760 Nov 20 00:11:01.571: INFO: Pod for on the node: machine-config-daemon-2nwhn, Cpu: 20, Mem: 52428800 Nov 20 00:11:01.571: INFO: Pod for on the node: node-exporter-dxfx8, Cpu: 110, Mem: 230686720 Nov 20 00:11:01.571: INFO: Pod for on the node: multus-z4jlm, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.571: INFO: Pod for on the node: ovs-9dthp, Cpu: 200, Mem: 419430400 Nov 20 00:11:01.571: INFO: Pod for on the node: sdn-dfk44, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.571: INFO: Node: ip-10-0-132-239.us-west-2.compute.internal, totalRequestedCpuResource: 1749, cpuAllocatableMil: 3500, cpuFraction: 0.4997142857142857 Nov 20 00:11:01.571: INFO: Node: ip-10-0-132-239.us-west-2.compute.internal, totalRequestedMemResource: 8090896384, memAllocatableVal: 16181792768, memFraction: 0.5 STEP: Compute Cpu, Mem Fraction after create balanced pods. Nov 20 00:11:01.571: INFO: ComputeCpuMemFraction for node: ip-10-0-134-44.us-west-2.compute.internal Nov 20 00:11:01.752: INFO: Pod for on the node: d00269c8-2ac4-11eb-9365-0a58ac104a46-0, Cpu: 89, Mem: 4649474048 Nov 20 00:11:01.752: INFO: Pod for on the node: tuned-wgwsl, Cpu: 10, Mem: 20971520 Nov 20 00:11:01.752: INFO: Pod for on the node: dns-default-p6jkj, Cpu: 110, Mem: 283115520 Nov 20 00:11:01.752: INFO: Pod for on the node: image-registry-54fd967995-sjxmw, Cpu: 100, Mem: 268435456 Nov 20 00:11:01.752: INFO: Pod for on the node: node-ca-xkjnr, Cpu: 10, Mem: 10485760 Nov 20 00:11:01.752: INFO: Pod for on the node: router-default-756b45d66d-dtcjj, Cpu: 100, Mem: 268435456 Nov 20 00:11:01.752: INFO: Pod for on the node: machine-config-daemon-9cdtt, Cpu: 20, Mem: 52428800 Nov 20 00:11:01.752: INFO: Pod for on the node: certified-operators-74db69bc55-vjs88, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.752: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Nov 20 00:11:01.752: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Nov 20 00:11:01.752: INFO: Pod for on the node: grafana-649f787944-7r225, Cpu: 200, Mem: 314572800 Nov 20 00:11:01.752: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-kt9sp, Cpu: 300, Mem: 629145600 Nov 20 00:11:01.752: INFO: Pod for on the node: node-exporter-9xlm7, Cpu: 110, Mem: 230686720 Nov 20 00:11:01.752: INFO: Pod for on the node: prometheus-adapter-6b8bf7fcb6-c9ccp, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.752: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Nov 20 00:11:01.752: INFO: Pod for on the node: multus-8sxsv, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.752: INFO: Pod for on the node: ovs-cxp2j, Cpu: 200, Mem: 419430400 Nov 20 00:11:01.752: INFO: Pod for on the node: sdn-b6gvc, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.752: INFO: Node: ip-10-0-134-44.us-west-2.compute.internal, totalRequestedCpuResource: 1749, cpuAllocatableMil: 3500, cpuFraction: 0.4997142857142857 Nov 20 00:11:01.752: INFO: Node: ip-10-0-134-44.us-west-2.compute.internal, totalRequestedMemResource: 8090900480, memAllocatableVal: 16181800960, memFraction: 0.5 STEP: Compute Cpu, Mem Fraction after create balanced pods. Nov 20 00:11:01.752: INFO: ComputeCpuMemFraction for node: ip-10-0-146-237.us-west-2.compute.internal Nov 20 00:11:01.853: INFO: Pod for on the node: d910877e-2ac4-11eb-9365-0a58ac104a46-0, Cpu: 479, Mem: 5421225984 Nov 20 00:11:01.854: INFO: Pod for on the node: tuned-69g6d, Cpu: 10, Mem: 20971520 Nov 20 00:11:01.854: INFO: Pod for on the node: dns-default-9tv4w, Cpu: 110, Mem: 283115520 Nov 20 00:11:01.854: INFO: Pod for on the node: node-ca-fl47z, Cpu: 10, Mem: 10485760 Nov 20 00:11:01.854: INFO: Pod for on the node: router-default-756b45d66d-dc2ds, Cpu: 100, Mem: 268435456 Nov 20 00:11:01.854: INFO: Pod for on the node: machine-config-daemon-rtlm7, Cpu: 20, Mem: 52428800 Nov 20 00:11:01.854: INFO: Pod for on the node: community-operators-65d8c88b5c-lrgl8, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.854: INFO: Pod for on the node: redhat-operators-598f74f9b-h42zk, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.854: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Nov 20 00:11:01.854: INFO: Pod for on the node: node-exporter-tjp24, Cpu: 110, Mem: 230686720 Nov 20 00:11:01.854: INFO: Pod for on the node: prometheus-adapter-6b8bf7fcb6-mwh87, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.854: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Nov 20 00:11:01.854: INFO: Pod for on the node: prometheus-operator-5d4588dd6-d6zf5, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.854: INFO: Pod for on the node: telemeter-client-5d87684b5-lzp2x, Cpu: 210, Mem: 440401920 Nov 20 00:11:01.854: INFO: Pod for on the node: multus-l7gw6, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.854: INFO: Pod for on the node: ovs-rkp57, Cpu: 200, Mem: 419430400 Nov 20 00:11:01.854: INFO: Pod for on the node: sdn-jvhnl, Cpu: 100, Mem: 209715200 Nov 20 00:11:01.854: INFO: Node: ip-10-0-146-237.us-west-2.compute.internal, totalRequestedCpuResource: 1749, cpuAllocatableMil: 3500, cpuFraction: 0.4997142857142857 Nov 20 00:11:01.854: INFO: Node: ip-10-0-146-237.us-west-2.compute.internal, totalRequestedMemResource: 8090900480, memAllocatableVal: 16181800960, memFraction: 0.5 STEP: Create a RC, with 0 replicas STEP: Trying to apply avoidPod annotations on the first node. Nov 20 00:11:02.278: INFO: Conflict when trying to add/update avoidPonds {[{{&OwnerReference{Kind:ReplicationController,Name:scheduler-priority-avoid-pod,UID:dfa4592a-2ac4-11eb-aac2-069d8b705ae3,APIVersion:v1,Controller:*true,BlockOwnerDeletion:nil,}} 0001-01-01 00:00:00 +0000 UTC some reson some message}]} to ip-10-0-132-239.us-west-2.compute.internal STEP: deleting ReplicationController scheduler-priority-avoid-pod in namespace e2e-tests-sched-priority-9txg9, will wait for the garbage collector to delete the pods Nov 20 00:13:02.555: INFO: Deleting ReplicationController scheduler-priority-avoid-pod took: 89.171437ms Nov 20 00:13:02.555: INFO: Terminating ReplicationController scheduler-priority-avoid-pod pods took: 68.656\\xc2\\xb5s [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:154 STEP: Collecting events from namespace \"e2e-tests-sched-priority-9txg9\". STEP: Found 9 events. Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:33 +0000 UTC - event for c9ee7b37-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-132-239.us-west-2.compute.internal} Pulled: Container image \"k8s.gcr.io/pause:3.1\" already present on machine Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:33 +0000 UTC - event for c9ee7b37-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-132-239.us-west-2.compute.internal} Created: Created container c9ee7b37-2ac4-11eb-9365-0a58ac104a46-0 Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:33 +0000 UTC - event for c9ee7b37-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-132-239.us-west-2.compute.internal} Started: Started container c9ee7b37-2ac4-11eb-9365-0a58ac104a46-0 Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:43 +0000 UTC - event for d00269c8-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-134-44.us-west-2.compute.internal} Pulled: Container image \"k8s.gcr.io/pause:3.1\" already present on machine Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:44 +0000 UTC - event for d00269c8-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-134-44.us-west-2.compute.internal} Created: Created container d00269c8-2ac4-11eb-9365-0a58ac104a46-0 Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:45 +0000 UTC - event for d00269c8-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-134-44.us-west-2.compute.internal} Started: Started container d00269c8-2ac4-11eb-9365-0a58ac104a46-0 Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:58 +0000 UTC - event for d910877e-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-146-237.us-west-2.compute.internal} Pulled: Container image \"k8s.gcr.io/pause:3.1\" already present on machine Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:58 +0000 UTC - event for d910877e-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-146-237.us-west-2.compute.internal} Created: Created container d910877e-2ac4-11eb-9365-0a58ac104a46-0 Nov 20 00:13:02.640: INFO: At 2020-11-20 00:10:58 +0000 UTC - event for d910877e-2ac4-11eb-9365-0a58ac104a46-0: {kubelet ip-10-0-146-237.us-west-2.compute.internal} Started: Started container d910877e-2ac4-11eb-9365-0a58ac104a46-0 Nov 20 00:13:02.806: INFO: skipping dumping cluster info - cluster too large Nov 20 00:13:02.807: INFO: Waiting up to 3m0s for all (but 100) nodes to be ready STEP: Destroying namespace \"e2e-tests-sched-priority-9txg9\" for this suite. Nov 20 00:13:17.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Nov 20 00:13:23.195: INFO: namespace e2e-tests-sched-priority-9txg9 deletion completed in 20.301575405s [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:68 Nov 20 00:13:23.198: INFO: Running AfterSuite actions on all nodes Nov 20 00:13:23.198: INFO: Running AfterSuite actions on node 1 fail [k8s.io/kubernetes/test/e2e/scheduling/priorities.go:191]: Expected error: <*errors.errorString | 0xc0002d83f0>: { s: \"timed out waiting for the condition\", } timed out waiting for the condition not to have occurred failed: (4m8s) 2020-11-20T00:13:23 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/52/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.2s) 2020-11-20T00:14:03 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/53/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.5s) 2020-11-20T00:14:27 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/54/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (42.3s) 2020-11-20T00:15:10 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/55/79) \"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.9s) 2020-11-20T00:15:31 \"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/56/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-11-20T00:16:11 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/57/79) \"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m8s) 2020-11-20T00:17:19 \"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/58/79) \"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m15s) 2020-11-20T00:18:34 \"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/59/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m55s) 2020-11-20T00:20:28 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/60/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (35.2s) 2020-11-20T00:21:04 \"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/61/79) \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m50s) 2020-11-20T00:22:54 \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/62/79) \"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (2m55s) 2020-11-20T00:25:48 \"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/63/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-20T00:26:29 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/64/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.9s) 2020-11-20T00:27:10 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/65/79) \"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m9s) 2020-11-20T00:28:19 \"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/66/79) \"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (20.9s) 2020-11-20T00:28:39 \"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/67/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-20T00:29:20 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/68/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (34.3s) 2020-11-20T00:29:54 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/69/79) \"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.7s) 2020-11-20T00:30:15 \"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/70/79) \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (35s) 2020-11-20T00:30:50 \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/71/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-20T00:31:31 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/72/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-11-20T00:32:11 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/73/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m33s) 2020-11-20T00:33:44 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/74/79) \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m44s) 2020-11-20T00:36:28 \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/75/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (57.1s) 2020-11-20T00:37:25 \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/76/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-20T00:38:06 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/77/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m22s) 2020-11-20T00:39:28 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/78/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.1s) 2020-11-20T00:39:52 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/79/79) \"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m25s) 2020-11-20T00:41:17 \"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" Timeline: Nov 19 23:24:36.397 W ns/openshift-machine-config-operator pod/machine-config-daemon-lsnww node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:24:36.398 W ns/openshift-image-registry pod/node-ca-w7mtc node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:24:36.465 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:24:36.466 W ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:24:36.479 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-lsnww Nov 19 23:24:36.479 I ns/openshift-image-registry pod/node-ca-w7mtc Marking for deletion Pod openshift-image-registry/node-ca-w7mtc Nov 19 23:24:36.479 I ns/openshift-machine-config-operator pod/machine-config-daemon-lsnww Stopping container machine-config-daemon Nov 19 23:24:36.479 W ns/openshift-ingress pod/router-default-756b45d66d-nphxt node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:24:36.480 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 0s Nov 19 23:24:36.480 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:24:36.487 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-w7mtc Nov 19 23:24:36.487 I ns/openshift-image-registry pod/node-ca-w7mtc Stopping container node-ca Nov 19 23:24:36.487 I ns/openshift-monitoring pod/prometheus-k8s-1 Marking for deletion Pod openshift-monitoring/prometheus-k8s-1 Nov 19 23:24:36.487 I ns/openshift-machine-config-operator pod/machine-config-daemon-lsnww Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-lsnww Nov 19 23:24:36.487 I ns/openshift-monitoring pod/alertmanager-main-0 Marking for deletion Pod openshift-monitoring/alertmanager-main-0 Nov 19 23:24:36.487 I ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 node/ created Nov 19 23:24:36.488 I ns/openshift-ingress pod/router-default-756b45d66d-nphxt Marking for deletion Pod openshift-ingress/router-default-756b45d66d-nphxt Nov 19 23:24:36.488 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus Nov 19 23:24:36.488 I ns/openshift-ingress pod/router-default-756b45d66d-dtcjj node/ created Nov 19 23:24:36.488 I ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh Marking for deletion Pod openshift-marketplace/certified-operators-74db69bc55-9wggh Nov 19 23:24:36.488 I ns/openshift-monitoring pod/alertmanager-main-0 Cancelling deletion of Pod openshift-monitoring/alertmanager-main-0 Nov 19 23:24:36.488 I ns/openshift-marketplace replicaset/certified-operators-74db69bc55 Created pod: certified-operators-74db69bc55-vjs88 Nov 19 23:24:36.547 I ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Successfully assigned openshift-marketplace/certified-operators-74db69bc55-vjs88 to ip-10-0-134-44.us-west-2.compute.internal Nov 19 23:24:36.547 I ns/openshift-ingress replicaset/router-default-756b45d66d Created pod: router-default-756b45d66d-dtcjj Nov 19 23:24:36.547 I ns/openshift-ingress pod/router-default-756b45d66d-dtcjj Successfully assigned openshift-ingress/router-default-756b45d66d-dtcjj to ip-10-0-134-44.us-west-2.compute.internal Nov 19 23:24:36.547 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container rules-configmap-reloader Nov 19 23:24:36.547 I ns/openshift-monitoring statefulset/alertmanager-main create Pod alertmanager-main-0 in StatefulSet alertmanager-main successful Nov 19 23:24:36.548 I ns/openshift-monitoring pod/alertmanager-main-0 node/ created Nov 19 23:24:36.561 I ns/openshift-monitoring pod/alertmanager-main-0 Successfully assigned openshift-monitoring/alertmanager-main-0 to ip-10-0-134-44.us-west-2.compute.internal Nov 19 23:24:36.561 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container kube-rbac-proxy Nov 19 23:24:36.561 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus-proxy Nov 19 23:24:36.561 I ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh Stopping container certified-operators Nov 19 23:24:36.566 I ns/openshift-ingress pod/router-default-756b45d66d-nphxt Stopping container router Nov 19 23:24:36.571 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager Nov 19 23:24:36.575 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy Nov 19 23:24:36.610 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader Nov 19 23:24:36.808 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager (2 times) Nov 19 23:24:37.007 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy (2 times) Nov 19 23:24:37.207 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader (2 times) Nov 19 23:24:38.381 W ns/openshift-ingress pod/router-default-756b45d66d-nphxt node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:24:38.381 W ns/openshift-ingress pod/router-default-756b45d66d-nphxt node/ip-10-0-132-239.us-west-2.compute.internal container=router container stopped being ready Nov 19 23:24:38.732 W ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:24:38.732 W ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh node/ip-10-0-132-239.us-west-2.compute.internal container=certified-operators container stopped being ready Nov 19 23:24:39.131 E ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh node/ip-10-0-132-239.us-west-2.compute.internal container=certified-operators container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prom-label-proxy container stopped being ready Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-config-reloader container stopped being ready Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=kube-rbac-proxy container stopped being ready Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=rules-configmap-reloader container stopped being ready Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container stopped being ready Nov 19 23:24:39.532 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-proxy container stopped being ready Nov 19 23:24:39.931 W ns/openshift-image-registry pod/node-ca-w7mtc node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:24:39.931 W ns/openshift-image-registry pod/node-ca-w7mtc node/ip-10-0-132-239.us-west-2.compute.internal container=node-ca container stopped being ready Nov 19 23:24:40.134 W ns/openshift-image-registry pod/node-ca-w7mtc node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:24:40.155 I ns/openshift-image-registry pod/node-ca-q8n5q node/ created Nov 19 23:24:40.158 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-q8n5q Nov 19 23:24:40.165 I ns/openshift-image-registry pod/node-ca-q8n5q Successfully assigned openshift-image-registry/node-ca-q8n5q to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:24:40.345 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (8 times) Nov 19 23:24:40.493 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (8 times) Nov 19 23:24:40.622 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (8 times) Nov 19 23:24:41.133 W ns/openshift-ingress pod/router-default-756b45d66d-nphxt node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:24:41.655 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (5 times) Nov 19 23:24:42.109 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-1 in StatefulSet prometheus-k8s successful Nov 19 23:24:42.109 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:24:42.110 I ns/openshift-monitoring pod/prometheus-k8s-1 node/ created Nov 19 23:24:42.119 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully assigned openshift-monitoring/prometheus-k8s-1 to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:24:42.587 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (7 times) Nov 19 23:24:43.084 W ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh node/ip-10-0-132-239.us-west-2.compute.internal pod has been pending longer than a minute Nov 19 23:24:43.604 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (5 times) Nov 19 23:24:45.138 I ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:69e43ce2394b34a984aca4c30461087dcb4cef9eb6c9a61467ac192d11782ca6\" Nov 19 23:24:45.245 I ns/openshift-ingress pod/router-default-756b45d66d-dtcjj Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" Nov 19 23:24:45.425 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b9d07e8eed67a34b50a3afaa36cd4b9f052697894e64b5c203d0675d12ab5ec\" already present on machine Nov 19 23:24:45.576 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager Nov 19 23:24:45.601 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager Nov 19 23:24:45.603 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Nov 19 23:24:45.800 I ns/openshift-monitoring pod/alertmanager-main-0 Created container config-reloader Nov 19 23:24:45.830 I ns/openshift-monitoring pod/alertmanager-main-0 Started container config-reloader Nov 19 23:24:45.832 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Nov 19 23:24:45.975 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager-proxy Nov 19 23:24:46.001 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager-proxy Nov 19 23:24:47.136 W ns/openshift-machine-config-operator pod/machine-config-daemon-lsnww node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:24:47.215 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-vjpg6 Nov 19 23:24:47.215 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 Successfully assigned openshift-machine-config-operator/machine-config-daemon-vjpg6 to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:24:47.215 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 node/ created Nov 19 23:24:47.933 W ns/openshift-marketplace pod/certified-operators-74db69bc55-9wggh node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:24:48.425 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 19 23:24:48.545 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 Created container machine-config-daemon Nov 19 23:24:48.567 I ns/openshift-image-registry pod/node-ca-q8n5q Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 19 23:24:48.578 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 Started container machine-config-daemon Nov 19 23:24:48.732 I ns/openshift-image-registry pod/node-ca-q8n5q Created container node-ca Nov 19 23:24:48.763 I ns/openshift-image-registry pod/node-ca-q8n5q Started container node-ca Nov 19 23:24:50.428 I ns/openshift-ingress pod/router-default-756b45d66d-dtcjj Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" Nov 19 23:24:50.596 I ns/openshift-ingress pod/router-default-756b45d66d-dtcjj Created container router Nov 19 23:24:50.626 I ns/openshift-ingress pod/router-default-756b45d66d-dtcjj Started container router Nov 19 23:24:52.082 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine Nov 19 23:24:52.238 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus Nov 19 23:24:52.263 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus Nov 19 23:24:52.268 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\" already present on machine Nov 19 23:24:52.450 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-config-reloader Nov 19 23:24:52.473 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-config-reloader Nov 19 23:24:52.475 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Nov 19 23:24:52.622 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-proxy Nov 19 23:24:52.648 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-proxy Nov 19 23:24:52.651 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Nov 19 23:24:52.804 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container kube-rbac-proxy Nov 19 23:24:52.832 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container kube-rbac-proxy Nov 19 23:24:52.835 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\" already present on machine Nov 19 23:24:52.966 I ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:69e43ce2394b34a984aca4c30461087dcb4cef9eb6c9a61467ac192d11782ca6\" Nov 19 23:24:52.982 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prom-label-proxy Nov 19 23:24:53.083 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prom-label-proxy Nov 19 23:24:53.102 I ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Created container certified-operators Nov 19 23:24:53.127 I ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Started container certified-operators Nov 19 23:24:53.284 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Nov 19 23:24:53.433 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container exited with code 1 (Error): Nov 19 23:24:53.484 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container rules-configmap-reloader Nov 19 23:24:53.693 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container rules-configmap-reloader Nov 19 23:24:53.884 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine (2 times) Nov 19 23:24:54.085 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus (2 times) Nov 19 23:24:54.284 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus (2 times) Nov 19 23:24:54.733 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container restarted Nov 19 23:25:01.657 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Readiness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ Nov 19 23:25:03.735 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Liveness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ Nov 19 23:25:11.656 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Readiness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ (2 times) Nov 19 23:25:13.732 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Liveness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ (2 times) Nov 19 23:25:21.655 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Readiness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ (3 times) Nov 19 23:25:23.733 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Liveness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ (3 times) Nov 19 23:25:31.662 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Readiness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ (4 times) Nov 19 23:25:33.744 W ns/openshift-marketplace pod/certified-operators-74db69bc55-vjs88 Liveness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ (4 times) Nov 19 23:34:40.339 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (9 times) Nov 19 23:34:41.327 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (6 times) Nov 19 23:34:42.405 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (8 times) Nov 19 23:34:43.546 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (6 times) Nov 19 23:34:43.704 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (9 times) Nov 19 23:34:43.845 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (9 times) Nov 19 23:34:58.648 W ns/openshift-image-registry pod/node-ca-q8n5q node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:34:58.723 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-vjpg6 Nov 19 23:34:58.723 I ns/openshift-image-registry pod/node-ca-q8n5q Stopping container node-ca Nov 19 23:34:58.723 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-q8n5q Nov 19 23:34:58.723 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 Stopping container machine-config-daemon Nov 19 23:34:58.724 W ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:34:58.731 I ns/openshift-monitoring pod/prometheus-k8s-1 Marking for deletion Pod openshift-monitoring/prometheus-k8s-1 (2 times) Nov 19 23:34:58.731 I ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-vjpg6 Nov 19 23:34:58.731 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus Nov 19 23:34:58.731 I ns/openshift-image-registry pod/node-ca-q8n5q Marking for deletion Pod openshift-image-registry/node-ca-q8n5q Nov 19 23:34:58.731 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container rules-configmap-reloader Nov 19 23:34:58.732 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:34:58.806 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prom-label-proxy Nov 19 23:34:58.806 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container kube-rbac-proxy Nov 19 23:34:58.806 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus-proxy Nov 19 23:34:58.806 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus-config-reloader Nov 19 23:34:59.702 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-proxy container exited with code 2 (Error): Nov 19 23:34:59.702 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=rules-configmap-reloader container exited with code 2 (Error): Nov 19 23:34:59.702 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-config-reloader container exited with code 2 (Error): Nov 19 23:35:02.344 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:35:02.425 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully assigned openshift-monitoring/prometheus-k8s-1 to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:35:02.426 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-1 in StatefulSet prometheus-k8s successful (2 times) Nov 19 23:35:02.426 I ns/openshift-monitoring pod/prometheus-k8s-1 node/ created Nov 19 23:35:05.261 W ns/openshift-image-registry pod/node-ca-q8n5q node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:35:05.340 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-ncvkl Nov 19 23:35:05.340 I ns/openshift-image-registry pod/node-ca-ncvkl Successfully assigned openshift-image-registry/node-ca-ncvkl to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:35:05.341 I ns/openshift-image-registry pod/node-ca-ncvkl node/ created Nov 19 23:35:06.261 W ns/openshift-machine-config-operator pod/machine-config-daemon-vjpg6 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:35:06.340 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-dwwsn Nov 19 23:35:06.340 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn Successfully assigned openshift-machine-config-operator/machine-config-daemon-dwwsn to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:35:06.340 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn node/ created Nov 19 23:35:07.624 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 19 23:35:07.686 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn Created container machine-config-daemon Nov 19 23:35:07.711 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn Started container machine-config-daemon Nov 19 23:35:12.443 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine Nov 19 23:35:12.550 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus Nov 19 23:35:12.583 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus Nov 19 23:35:12.586 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\" already present on machine Nov 19 23:35:12.745 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-config-reloader Nov 19 23:35:12.770 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-config-reloader Nov 19 23:35:12.773 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Nov 19 23:35:12.919 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-proxy Nov 19 23:35:12.954 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-proxy Nov 19 23:35:12.957 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Nov 19 23:35:13.114 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container kube-rbac-proxy Nov 19 23:35:13.139 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container kube-rbac-proxy Nov 19 23:35:13.142 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\" already present on machine Nov 19 23:35:13.290 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prom-label-proxy Nov 19 23:35:13.371 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prom-label-proxy Nov 19 23:35:13.571 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Nov 19 23:35:13.731 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container exited with code 1 (Error): Nov 19 23:35:13.771 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container rules-configmap-reloader Nov 19 23:35:13.977 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container rules-configmap-reloader Nov 19 23:35:14.172 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine (2 times) Nov 19 23:35:14.372 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus (2 times) Nov 19 23:35:14.572 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus (2 times) Nov 19 23:35:14.771 I ns/openshift-image-registry pod/node-ca-ncvkl Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 19 23:35:14.971 I ns/openshift-image-registry pod/node-ca-ncvkl Created container node-ca Nov 19 23:35:15.128 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container restarted Nov 19 23:35:15.171 I ns/openshift-image-registry pod/node-ca-ncvkl Started container node-ca Nov 19 23:38:08.760 W ns/openshift-image-registry pod/node-ca-ncvkl node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:38:08.760 W ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:38:08.768 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-dwwsn Nov 19 23:38:08.769 I ns/openshift-image-registry pod/node-ca-ncvkl Stopping container node-ca Nov 19 23:38:08.773 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn Stopping container machine-config-daemon Nov 19 23:38:08.776 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-ncvkl Nov 19 23:38:08.779 I ns/openshift-image-registry pod/node-ca-ncvkl Marking for deletion Pod openshift-image-registry/node-ca-ncvkl Nov 19 23:38:08.779 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:38:08.783 I ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-dwwsn Nov 19 23:38:08.833 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prometheus Nov 19 23:38:08.833 I ns/openshift-monitoring pod/prometheus-k8s-1 Marking for deletion Pod openshift-monitoring/prometheus-k8s-1 (3 times) Nov 19 23:38:08.833 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container rules-configmap-reloader Nov 19 23:38:08.833 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container prom-label-proxy Nov 19 23:38:08.833 I ns/openshift-monitoring pod/prometheus-k8s-1 Stopping container kube-rbac-proxy Nov 19 23:38:10.194 W ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:38:10.194 W ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn node/ip-10-0-132-239.us-west-2.compute.internal container=machine-config-daemon container stopped being ready Nov 19 23:38:10.252 W ns/openshift-image-registry pod/node-ca-ncvkl node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:38:10.252 W ns/openshift-image-registry pod/node-ca-ncvkl node/ip-10-0-132-239.us-west-2.compute.internal container=node-ca container stopped being ready Nov 19 23:38:10.254 E ns/openshift-image-registry pod/node-ca-ncvkl node/ip-10-0-132-239.us-west-2.compute.internal container=node-ca container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-config-reloader container stopped being ready Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container stopped being ready Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=rules-configmap-reloader container stopped being ready Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=kube-rbac-proxy container stopped being ready Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prom-label-proxy container stopped being ready Nov 19 23:38:10.522 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-proxy container stopped being ready Nov 19 23:38:10.922 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=rules-configmap-reloader container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:10.922 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:10.922 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=kube-rbac-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:10.922 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prom-label-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:10.922 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:10.922 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal container=prometheus-config-reloader container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Nov 19 23:38:13.084 W ns/openshift-image-registry pod/node-ca-ncvkl node/ip-10-0-132-239.us-west-2.compute.internal pod has been pending longer than a minute Nov 19 23:38:13.084 W ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn node/ip-10-0-132-239.us-west-2.compute.internal pod has been pending longer than a minute Nov 19 23:38:13.084 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal pod has been pending longer than a minute Nov 19 23:38:15.715 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:38:15.715 W ns/openshift-image-registry pod/node-ca-ncvkl node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:38:15.797 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-1 in StatefulSet prometheus-k8s successful (3 times) Nov 19 23:38:15.797 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully assigned openshift-monitoring/prometheus-k8s-1 to ip-10-0-134-44.us-west-2.compute.internal Nov 19 23:38:15.798 W ns/openshift-machine-config-operator pod/machine-config-daemon-dwwsn node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:38:15.798 I ns/openshift-monitoring pod/prometheus-k8s-1 node/ created Nov 19 23:38:16.206 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq node/ created Nov 19 23:38:16.211 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-skgfq Nov 19 23:38:16.215 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq Successfully assigned openshift-machine-config-operator/machine-config-daemon-skgfq to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:38:16.222 I ns/openshift-image-registry pod/node-ca-jmwlw node/ created Nov 19 23:38:16.225 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-jmwlw Nov 19 23:38:16.231 I ns/openshift-image-registry pod/node-ca-jmwlw Successfully assigned openshift-image-registry/node-ca-jmwlw to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:38:17.916 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 19 23:38:18.046 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq Created container machine-config-daemon Nov 19 23:38:18.084 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq Started container machine-config-daemon Nov 19 23:38:24.012 I ns/openshift-monitoring pod/prometheus-k8s-1 Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" Nov 19 23:38:25.295 I ns/openshift-image-registry pod/node-ca-jmwlw Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 19 23:38:25.423 I ns/openshift-image-registry pod/node-ca-jmwlw Created container node-ca Nov 19 23:38:25.448 I ns/openshift-image-registry pod/node-ca-jmwlw Started container node-ca Nov 19 23:38:29.620 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" Nov 19 23:38:29.783 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus Nov 19 23:38:29.820 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus Nov 19 23:38:29.822 I ns/openshift-monitoring pod/prometheus-k8s-1 Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\" Nov 19 23:38:34.292 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\" Nov 19 23:38:34.440 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-config-reloader Nov 19 23:38:34.464 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-config-reloader Nov 19 23:38:34.467 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Nov 19 23:38:34.646 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus-proxy Nov 19 23:38:34.672 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus-proxy Nov 19 23:38:34.676 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Nov 19 23:38:34.830 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container kube-rbac-proxy Nov 19 23:38:34.853 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container kube-rbac-proxy Nov 19 23:38:34.856 I ns/openshift-monitoring pod/prometheus-k8s-1 Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\" Nov 19 23:38:38.790 I ns/openshift-monitoring pod/prometheus-k8s-1 Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\" Nov 19 23:38:38.958 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prom-label-proxy Nov 19 23:38:38.981 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prom-label-proxy Nov 19 23:38:38.984 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Nov 19 23:38:39.141 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container rules-configmap-reloader Nov 19 23:38:39.169 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container rules-configmap-reloader Nov 19 23:38:40.088 I ns/openshift-monitoring pod/prometheus-k8s-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine Nov 19 23:38:40.088 E ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-134-44.us-west-2.compute.internal container=prometheus container exited with code 1 (Error): Nov 19 23:38:40.178 I ns/openshift-monitoring pod/prometheus-k8s-1 Created container prometheus (2 times) Nov 19 23:38:40.204 I ns/openshift-monitoring pod/prometheus-k8s-1 Started container prometheus (2 times) Nov 19 23:38:41.626 W ns/openshift-monitoring pod/prometheus-k8s-1 node/ip-10-0-134-44.us-west-2.compute.internal container=prometheus container restarted Nov 19 23:41:05.840 W persistentvolume/pvc-95db1176-2ac0-11eb-b0cc-02f1b89323e7 Error deleting EBS volume \"vol-0133054b3faf7a686\" since volume is currently attached to \"i-03abce89e1e637147\" Nov 19 23:42:30.700 W ns/openshift-image-registry pod/node-ca-jmwlw node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:42:30.700 W ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:42:30.710 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq Stopping container machine-config-daemon Nov 19 23:42:30.712 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-jmwlw Nov 19 23:42:30.714 I ns/openshift-image-registry pod/node-ca-jmwlw Marking for deletion Pod openshift-image-registry/node-ca-jmwlw Nov 19 23:42:30.715 I ns/openshift-image-registry pod/node-ca-jmwlw Stopping container node-ca Nov 19 23:42:30.717 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-skgfq Nov 19 23:42:30.717 I ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-skgfq Nov 19 23:42:35.631 W ns/openshift-machine-config-operator pod/machine-config-daemon-skgfq node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:42:35.712 W ns/openshift-image-registry pod/node-ca-jmwlw node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:43:45.817 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 node/ created Nov 19 23:43:45.821 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-n8nd4 Nov 19 23:43:45.890 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 Successfully assigned openshift-machine-config-operator/machine-config-daemon-n8nd4 to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:43:45.890 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-6tkxb Nov 19 23:43:45.890 I ns/openshift-image-registry pod/node-ca-6tkxb Successfully assigned openshift-image-registry/node-ca-6tkxb to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:43:45.890 I ns/openshift-image-registry pod/node-ca-6tkxb node/ created Nov 19 23:43:46.499 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 19 23:43:46.637 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 Created container machine-config-daemon Nov 19 23:43:46.665 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 Started container machine-config-daemon Nov 19 23:43:54.262 I ns/openshift-image-registry pod/node-ca-6tkxb Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 19 23:43:54.382 I ns/openshift-image-registry pod/node-ca-6tkxb Created container node-ca Nov 19 23:43:54.408 I ns/openshift-image-registry pod/node-ca-6tkxb Started container node-ca Nov 19 23:44:41.153 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (9 times) Nov 19 23:44:42.162 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (7 times) Nov 19 23:44:42.314 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (10 times) Nov 19 23:44:42.454 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (10 times) Nov 19 23:44:42.598 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (10 times) Nov 19 23:44:43.484 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (7 times) Nov 19 23:53:10.940 W ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 19 23:53:10.940 W ns/openshift-image-registry pod/node-ca-6tkxb node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 19 23:53:11.013 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 Stopping container machine-config-daemon Nov 19 23:53:11.013 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-n8nd4 Nov 19 23:53:11.013 I ns/openshift-image-registry pod/node-ca-6tkxb Marking for deletion Pod openshift-image-registry/node-ca-6tkxb Nov 19 23:53:11.013 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-6tkxb Nov 19 23:53:11.013 I ns/openshift-image-registry pod/node-ca-6tkxb Stopping container node-ca Nov 19 23:53:11.013 I ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-n8nd4 Nov 19 23:53:15.632 W ns/openshift-machine-config-operator pod/machine-config-daemon-n8nd4 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:53:15.711 W ns/openshift-image-registry pod/node-ca-6tkxb node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 19 23:54:16.249 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn node/ created Nov 19 23:54:16.251 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-2nwhn Nov 19 23:54:16.251 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn Successfully assigned openshift-machine-config-operator/machine-config-daemon-2nwhn to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:54:16.272 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-rdx58 Nov 19 23:54:16.272 I ns/openshift-image-registry pod/node-ca-rdx58 Successfully assigned openshift-image-registry/node-ca-rdx58 to ip-10-0-132-239.us-west-2.compute.internal Nov 19 23:54:16.272 I ns/openshift-image-registry pod/node-ca-rdx58 node/ created Nov 19 23:54:16.883 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 19 23:54:17.003 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn Created container machine-config-daemon Nov 19 23:54:17.029 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn Started container machine-config-daemon Nov 19 23:54:23.970 I ns/openshift-image-registry pod/node-ca-rdx58 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 19 23:54:24.097 I ns/openshift-image-registry pod/node-ca-rdx58 Created container node-ca Nov 19 23:54:24.122 I ns/openshift-image-registry pod/node-ca-rdx58 Started container node-ca Nov 19 23:54:40.345 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (11 times) Nov 19 23:54:41.387 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (8 times) Nov 19 23:54:42.188 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (10 times) Nov 19 23:54:43.143 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (8 times) Nov 19 23:54:43.309 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (11 times) Nov 19 23:54:43.469 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (11 times) Nov 20 00:00:11.459 I ns/kube-system pod/critical-pod node/ created Nov 20 00:00:11.464 W ns/kube-system pod/critical-pod 0/6 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 3 node(s) had taints that the pod didn\\'t tolerate. Nov 20 00:00:11.541 W ns/kube-system pod/critical-pod 0/6 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 3 node(s) had taints that the pod didn\\'t tolerate. (2 times) Nov 20 00:00:15.638 I ns/kube-system pod/critical-pod Successfully assigned kube-system/critical-pod to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:00:23.496 I ns/kube-system pod/critical-pod Container image \"k8s.gcr.io/pause:3.1\" already present on machine Nov 20 00:00:23.631 I ns/kube-system pod/critical-pod Created container critical-pod Nov 20 00:00:23.653 I ns/kube-system pod/critical-pod Started container critical-pod Nov 20 00:00:26.057 W ns/kube-system pod/critical-pod node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 0s Nov 20 00:00:26.061 W ns/kube-system pod/critical-pod node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:00:26.065 I ns/kube-system pod/critical-pod Stopping container critical-pod Nov 20 00:04:41.182 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (9 times) Nov 20 00:04:42.012 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (11 times) Nov 20 00:04:42.926 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (9 times) Nov 20 00:04:43.090 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (12 times) Nov 20 00:04:43.227 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (12 times) Nov 20 00:04:43.355 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (12 times) Nov 20 00:09:15.607 - 247s I test=\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" running Nov 20 00:13:23.214 I test=\"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" failed Nov 20 00:14:40.295 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (13 times) Nov 20 00:14:41.352 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (10 times) Nov 20 00:14:42.589 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (12 times) Nov 20 00:14:43.487 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (10 times) Nov 20 00:14:43.660 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (13 times) Nov 20 00:14:43.787 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (13 times) Nov 20 00:22:14.224 W ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 20 00:22:14.224 W ns/openshift-image-registry pod/node-ca-rdx58 node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 20 00:22:14.233 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn Stopping container machine-config-daemon Nov 20 00:22:14.235 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-2nwhn Nov 20 00:22:14.236 I ns/openshift-image-registry pod/node-ca-rdx58 Marking for deletion Pod openshift-image-registry/node-ca-rdx58 Nov 20 00:22:14.236 I ns/openshift-image-registry pod/node-ca-rdx58 Stopping container node-ca Nov 20 00:22:14.252 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-rdx58 Nov 20 00:22:14.252 I ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-2nwhn Nov 20 00:22:15.687 W ns/openshift-image-registry pod/node-ca-rdx58 node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 20 00:22:15.687 W ns/openshift-image-registry pod/node-ca-rdx58 node/ip-10-0-132-239.us-west-2.compute.internal container=node-ca container stopped being ready Nov 20 00:22:15.695 W ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn node/ip-10-0-132-239.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 20 00:22:15.695 W ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn node/ip-10-0-132-239.us-west-2.compute.internal container=machine-config-daemon container stopped being ready Nov 20 00:22:25.632 W ns/openshift-image-registry pod/node-ca-rdx58 node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:22:25.712 W ns/openshift-machine-config-operator pod/machine-config-daemon-2nwhn node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:22:41.883 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh node/ created Nov 20 00:22:41.887 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-fl6xh Nov 20 00:22:41.956 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh Successfully assigned openshift-machine-config-operator/machine-config-daemon-fl6xh to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:22:41.956 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-cbcsc Nov 20 00:22:41.956 I ns/openshift-image-registry pod/node-ca-cbcsc Successfully assigned openshift-image-registry/node-ca-cbcsc to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:22:41.956 I ns/openshift-image-registry pod/node-ca-cbcsc node/ created Nov 20 00:22:42.553 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 20 00:22:42.685 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh Created container machine-config-daemon Nov 20 00:22:42.710 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh Started container machine-config-daemon Nov 20 00:22:49.460 I ns/openshift-image-registry pod/node-ca-cbcsc Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 20 00:22:49.597 I ns/openshift-image-registry pod/node-ca-cbcsc Created container node-ca Nov 20 00:22:49.629 I ns/openshift-image-registry pod/node-ca-cbcsc Started container node-ca Nov 20 00:24:40.289 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (14 times) Nov 20 00:24:40.427 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (14 times) Nov 20 00:24:41.377 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (11 times) Nov 20 00:24:42.345 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (13 times) Nov 20 00:24:43.256 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (11 times) Nov 20 00:24:43.414 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (14 times) Nov 20 00:28:27.423 I ns/kube-system pod/pod0-system-node-critical node/ created Nov 20 00:28:27.435 I ns/kube-system pod/pod0-system-node-critical Successfully assigned kube-system/pod0-system-node-critical to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:28:27.507 I ns/kube-system pod/pod1-system-cluster-critical node/ created Nov 20 00:28:27.516 I ns/kube-system pod/pod1-system-cluster-critical Successfully assigned kube-system/pod1-system-cluster-critical to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:28:27.594 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 0s Nov 20 00:28:27.598 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:28:27.686 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 0s Nov 20 00:28:27.689 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:28:37.371 W ns/kube-system pod/pod0-system-node-critical Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_pod0-system-node-critical_kube-system_4ecc5780-2ac7-11eb-b0cc-02f1b89323e7_0(202a5d3a2e97a97c0b20a31661acbc8902e197de448ab8b381953044a6b18748): Multus: Err adding pod to network \"openshift-sdn\": cannot set \"openshift-sdn\" ifname to \"eth0\": no netns: failed to Statfs \"/proc/110393/ns/net\": no such file or directory Nov 20 00:30:30.532 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \"pod1-system-cluster-critical_kube-system(4ed93b88-2ac7-11eb-b0cc-02f1b89323e7)\": timeout expired waiting for volumes to attach or mount for pod \"kube-system\"/\"pod1-system-cluster-critical\". list of unmounted volumes=[default-token-pvg6z]. list of unattached volumes=[default-token-pvg6z] Nov 20 00:34:41.162 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-1 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-1 (12 times) Nov 20 00:34:41.996 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-2 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-2 (14 times) Nov 20 00:34:42.953 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-master-0 Updated machine ci-op-bxig87c1-7bc5c-dbdks-master-0 (12 times) Nov 20 00:34:43.118 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2b-v8rcs (15 times) Nov 20 00:34:43.276 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-tkhs6 (15 times) Nov 20 00:34:43.437 I ns/openshift-machine-api machine/ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb Updated machine ci-op-bxig87c1-7bc5c-dbdks-worker-us-west-2a-d8ncb (15 times) Nov 20 00:34:54.805 W ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 600s Nov 20 00:34:54.805 W ns/openshift-image-registry pod/node-ca-cbcsc node/ip-10-0-132-239.us-west-2.compute.internal graceful deletion within 30s Nov 20 00:34:54.813 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh Stopping container machine-config-daemon Nov 20 00:34:54.880 I ns/openshift-image-registry pod/node-ca-cbcsc Stopping container node-ca Nov 20 00:34:54.880 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-fl6xh Nov 20 00:34:54.880 I ns/openshift-image-registry pod/node-ca-cbcsc Marking for deletion Pod openshift-image-registry/node-ca-cbcsc Nov 20 00:34:54.880 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-cbcsc Nov 20 00:34:54.880 I ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-fl6xh Nov 20 00:35:05.631 W ns/openshift-image-registry pod/node-ca-cbcsc node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:35:05.709 W ns/openshift-machine-config-operator pod/machine-config-daemon-fl6xh node/ip-10-0-132-239.us-west-2.compute.internal deleted Nov 20 00:36:00.068 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g node/ created Nov 20 00:36:00.138 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-dgg4g Nov 20 00:36:00.139 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Successfully assigned openshift-machine-config-operator/machine-config-daemon-dgg4g to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:36:00.139 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-b5q5l Nov 20 00:36:00.139 I ns/openshift-image-registry pod/node-ca-b5q5l node/ created Nov 20 00:36:00.149 I ns/openshift-image-registry pod/node-ca-b5q5l Successfully assigned openshift-image-registry/node-ca-b5q5l to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:36:00.766 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 20 00:36:00.890 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Created container machine-config-daemon Nov 20 00:36:00.917 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Started container machine-config-daemon Nov 20 00:36:08.327 I ns/openshift-image-registry pod/node-ca-b5q5l Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 20 00:36:08.464 I ns/openshift-image-registry pod/node-ca-b5q5l Created container node-ca Nov 20 00:36:08.491 I ns/openshift-image-registry pod/node-ca-b5q5l Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201120-004117.xml error: 1 fail, 39 pass, 39 skip (1h24m49s) 2020/11/20 00:41:18 Container test in pod e2e-aws-serial failed, exit code 1, reason Error 2020/11/20 00:48:50 Copied 127.96MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/11/20 00:48:50 Releasing leases for \"e2e-aws-serial\" 2020/11/20 00:48:50 Releasing lease \"d65cd353-d217-4fbd-ba5b-aa4a522fb068\" for \"aws-quota-slice\" 2020/11/20 00:48:50 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/11/20 00:48:50 Ran for 2h9m24s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-bxig87c1/e2e-aws-serial failed after 2h7m36s (failed containers: test): ContainerFailed one or more containers exited Container test exited with code 1, reason Error --- nternal deleted Nov 20 00:36:00.068 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g node/ created Nov 20 00:36:00.138 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-dgg4g Nov 20 00:36:00.139 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Successfully assigned openshift-machine-config-operator/machine-config-daemon-dgg4g to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:36:00.139 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-b5q5l Nov 20 00:36:00.139 I ns/openshift-image-registry pod/node-ca-b5q5l node/ created Nov 20 00:36:00.149 I ns/openshift-image-registry pod/node-ca-b5q5l Successfully assigned openshift-image-registry/node-ca-b5q5l to ip-10-0-132-239.us-west-2.compute.internal Nov 20 00:36:00.766 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 20 00:36:00.890 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Created container machine-config-daemon Nov 20 00:36:00.917 I ns/openshift-machine-config-operator pod/machine-config-daemon-dgg4g Started container machine-config-daemon Nov 20 00:36:08.327 I ns/openshift-image-registry pod/node-ca-b5q5l Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 20 00:36:08.464 I ns/openshift-image-registry pod/node-ca-b5q5l Created container node-ca Nov 20 00:36:08.491 I ns/openshift-image-registry pod/node-ca-b5q5l Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201120-004117.xml error: 1 fail, 39 pass, 39 skip (1h24m49s) --- '\n",
            "ID=24    : size=1         : b'2020/11/23 22:42:37 ci-operator version v20201123-4f43d72 2020/11/23 22:42:37 No source defined 2020/11/23 22:42:37 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/11/23 22:42:37 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-6yz8hxjr 2020/11/23 22:42:37 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/11/23 22:42:37 Creating namespace ci-op-6yz8hxjr 2020/11/23 22:42:37 Setting up pipeline imagestream for the test 2020/11/23 22:42:37 Created secret e2e-aws-serial-cluster-profile 2020/11/23 22:42:37 Created secret pull-secret 2020/11/23 22:42:37 Created PDB for pods with openshift.io/build.name label 2020/11/23 22:42:37 Created PDB for pods with created-by-ci label 2020/11/23 22:42:37 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:${component} 2020/11/23 22:42:39 Importing release image latest 2020/11/23 22:42:40 Executing pod \"release-images-latest-cli\" 2020/11/23 22:42:45 Executing pod \"release-images-latest\" 2020/11/23 22:43:32 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/11/23 22:43:32 Acquiring leases for \"e2e-aws-serial\" 2020/11/23 22:43:32 Acquiring lease for \"aws-quota-slice\" 2020/11/23 22:43:32 Acquired lease \"b315453f-673a-4c36-a4d3-64dd02315ee8\" for \"aws-quota-slice\" 2020/11/23 22:43:32 Executing template e2e-aws-serial 2020/11/23 22:43:32 Creating or restarting template instance 2020/11/23 22:43:32 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/11/23 22:43:32 Waiting for template instance to be ready 2020/11/23 22:43:34 Running pod e2e-aws-serial 2020/11/23 23:13:34 Container setup in pod e2e-aws-serial completed successfully 2020/11/24 00:36:24 Container test in pod e2e-aws-serial completed successfully 2020/11/24 00:42:54 Container teardown in pod e2e-aws-serial completed successfully 2020/11/24 00:43:02 Copied 109.18MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/11/24 00:43:34 pod didn\\'t start running within 15m0s: * Container artifacts is not ready with reason ContainerCreating * Container setup is not ready with reason ContainerCreating * Container teardown is not ready with reason ContainerCreating * Container test is not ready with reason ContainerCreating Found 16 events for Pod e2e-aws-serial: * 1x default-scheduler: Successfully assigned ci-op-6yz8hxjr/e2e-aws-serial to origin-ci-ig-n-1p7q * 1x kubelet: pulling image \"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:tests\" * 1x kubelet: Successfully pulled image \"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:tests\" * 1x kubelet: Created container * 1x kubelet: Started container * 1x kubelet: pulling image \"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:installer\" * 1x kubelet: Successfully pulled image \"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:installer\" * 1x kubelet: Created container * 1x kubelet: Started container * 1x kubelet: Container image \"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:tests\" already present on machine * 1x kubelet: Created container * 1x kubelet: Started container * 1x kubelet: pulling image \"quay.io/prometheus/busybox:latest\" * 1x kubelet: Successfully pulled image \"quay.io/prometheus/busybox:latest\" * 1x kubelet: Created container * 1x kubelet: Started container 2020/11/24 00:43:34 Releasing leases for \"e2e-aws-serial\" 2020/11/24 00:43:34 Releasing lease \"b315453f-673a-4c36-a4d3-64dd02315ee8\" for \"aws-quota-slice\" 2020/11/24 00:43:34 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/11/24 00:43:34 Ran for 2h0m57s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: pod didn\\'t start running within 15m0s: * Container artifacts is not ready with reason ContainerCreating * Container setup is not ready with reason ContainerCreating * Container teardown is not ready with reason ContainerCreating * Container test is not ready with reason ContainerCreating Found 16 events for Pod e2e-aws-serial: * 1x default-scheduler: Successfully assigned ci-op-6yz8hxjr/e2e-aws-serial to origin-ci-ig-n-1p7q * 1x kubelet: pulling image \"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:tests\" * 1x kubelet: Successfully pulled image \"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:tests\" * 1x kubelet: Created container * 1x kubelet: Started container * 1x kubelet: pulling image \"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:installer\" * 1x kubelet: Successfully pulled image \"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:installer\" * 1x kubelet: Created container * 1x kubelet: Started container * 1x kubelet: Container image \"registry.svc.ci.openshift.org/ci-op-6yz8hxjr/stable:tests\" already present on machine * 1x kubelet: Created container * 1x kubelet: Started container * 1x kubelet: pulling image \"quay.io/prometheus/busybox:latest\" * 1x kubelet: Successfully pulled image \"quay.io/prometheus/busybox:latest\" * 1x kubelet: Created container * 1x kubelet: Started container '\n",
            "ID=25    : size=6         : <*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \"release-images-latest-cli\" <*> <*> Executing pod \"release-images-latest\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \"e2e-aws-serial\" <*> <*> Acquiring lease for \"aws-quota-slice\" <*> <*> Acquired lease <*> for \"aws-quota-slice\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Container artifacts in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Releasing leases for \"e2e-aws-serial\" <*> <*> Releasing lease <*> for \"aws-quota-slice\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\n",
            "ID=26    : size=1         : b'2020/11/25 22:43:55 ci-operator version v20201125-2caa480 2020/11/25 22:43:55 No source defined 2020/11/25 22:43:55 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/11/25 22:43:55 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-yrkswzmk 2020/11/25 22:43:55 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/11/25 22:43:55 Creating namespace ci-op-yrkswzmk 2020/11/25 22:43:55 Setting up pipeline imagestream for the test 2020/11/25 22:43:55 Created secret e2e-aws-serial-cluster-profile 2020/11/25 22:43:55 Created secret pull-secret 2020/11/25 22:43:55 Created PDB for pods with openshift.io/build.name label 2020/11/25 22:43:55 Created PDB for pods with created-by-ci label 2020/11/25 22:43:55 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-yrkswzmk/stable:${component} 2020/11/25 22:43:57 Importing release image latest 2020/11/25 22:43:58 Executing pod \"release-images-latest-cli\" 2020/11/25 22:44:08 Executing pod \"release-images-latest\" 2020/11/25 22:45:40 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/11/25 22:45:40 Acquiring leases for \"e2e-aws-serial\" 2020/11/25 22:45:40 Acquiring lease for \"aws-quota-slice\" 2020/11/25 22:45:40 Acquired lease \"37f911f5-b8d2-4f47-99a0-9a2fde2bc334\" for \"aws-quota-slice\" 2020/11/25 22:45:40 Executing template e2e-aws-serial 2020/11/25 22:45:40 Creating or restarting template instance 2020/11/25 22:45:40 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/11/25 22:45:40 Waiting for template instance to be ready 2020/11/25 22:45:42 Running pod e2e-aws-serial 2020/11/25 23:17:07 Container setup in pod e2e-aws-serial completed successfully secret/support created started: (0/1/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (42.1s) 2020-11-25T23:18:04 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/2/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m39s) 2020-11-25T23:20:43 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/3/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (41.1s) 2020-11-25T23:21:24 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/4/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (26.2s) 2020-11-25T23:21:50 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/5/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-11-25T23:22:31 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/6/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m54s) 2020-11-25T23:25:25 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/7/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (37.4s) 2020-11-25T23:26:02 \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/8/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (37.1s) 2020-11-25T23:26:39 \"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/9/79) \"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\" skip [github.com/openshift/origin/test/extended/images/signatures.go:25]: disable because containers/image: https://github.com/containers/image/pull/570 skipped: (15.4s) 2020-11-25T23:26:55 \"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\" started: (0/10/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.1s) 2020-11-25T23:27:19 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/11/79) \"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m8s) 2020-11-25T23:28:27 \"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/12/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (50.5s) 2020-11-25T23:29:17 \"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/13/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m26s) 2020-11-25T23:30:43 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/14/79) \"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m36s) 2020-11-25T23:32:19 \"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/15/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-11-25T23:33:00 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/16/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (23s) 2020-11-25T23:33:23 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/17/79) \"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m11s) 2020-11-25T23:34:33 \"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/18/79) \"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m10s) 2020-11-25T23:35:43 \"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/19/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-11-25T23:36:23 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/20/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m44s) 2020-11-25T23:39:07 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/21/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m9s) 2020-11-25T23:41:17 \"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/22/79) \"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m11s) 2020-11-25T23:43:28 \"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/23/79) \"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m57s) 2020-11-25T23:45:25 \"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/24/79) \"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (20.8s) 2020-11-25T23:45:46 \"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/25/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-11-25T23:46:27 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/26/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-11-25T23:47:07 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/27/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" Nov 25 23:47:08.277: INFO: >>> kubeConfig: /tmp/admin.kubeconfig Nov 25 23:47:08.280: INFO: Waiting up to 30m0s for all (but 100) nodes to be schedulable Nov 25 23:47:08.795: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace \\'kube-system\\' to be running and ready Nov 25 23:47:09.068: INFO: 0 / 0 pods in namespace \\'kube-system\\' are running and ready (0 seconds elapsed) Nov 25 23:47:09.068: INFO: expected 0 pod replicas in namespace \\'kube-system\\', 0 are Running and Ready. Nov 25 23:47:09.068: INFO: Waiting up to 5m0s for all daemonsets in namespace \\'kube-system\\' to start Nov 25 23:47:09.158: INFO: e2e test version: v1.13.4-138-g41dc99c Nov 25 23:47:09.242: INFO: kube-apiserver version: v1.13.4-138-g41dc99c [BeforeEach] [Top Level] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/test/extended/util/test.go:69 [BeforeEach] [sig-storage] PersistentVolumes-local /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:153 STEP: Creating a kubernetes client Nov 25 23:47:09.246: INFO: >>> kubeConfig: /tmp/admin.kubeconfig STEP: Building a namespace api object, basename persistent-local-volumes-test Nov 25 23:47:14.482: INFO: About to run a Kube e2e test, ensuring namespace is privileged Nov 25 23:47:15.544: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled. STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-storage] PersistentVolumes-local /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:185 [BeforeEach] Local volume provisioner [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:436 STEP: Bootstrapping local volume provisioner STEP: Binding priviledged Pod Security Policy to the service account local-storage-admin STEP: Initializing local volume discovery base path on node ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:47:20.856: INFO: Running \\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c mkdir -p /tmp/e2e-tests-persistent-local-volumes-test-5lh44 -m 777\\' Nov 25 23:47:22.075: INFO: stderr: \"\" Nov 25 23:47:22.075: INFO: stdout: \"\" STEP: Initializing local volume discovery base path on node ip-10-0-142-135.us-west-2.compute.internal Nov 25 23:47:26.342: INFO: Running \\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-142-135.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c mkdir -p /tmp/e2e-tests-persistent-local-volumes-test-5lh44 -m 777\\' Nov 25 23:47:27.542: INFO: stderr: \"\" Nov 25 23:47:27.542: INFO: stdout: \"\" STEP: Initializing local volume discovery base path on node ip-10-0-156-74.us-west-2.compute.internal Nov 25 23:47:31.808: INFO: Running \\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-156-74.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c mkdir -p /tmp/e2e-tests-persistent-local-volumes-test-5lh44 -m 777\\' Nov 25 23:47:33.018: INFO: stderr: \"\" Nov 25 23:47:33.018: INFO: stdout: \"\" STEP: Creating local directory at path \"/tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b\" Nov 25 23:47:33.018: INFO: Running \\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c mkdir /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b -m 777\\' Nov 25 23:47:34.199: INFO: stderr: \"\" Nov 25 23:47:34.199: INFO: stdout: \"\" STEP: Mounting local directory at path \"/tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b\" Nov 25 23:47:34.199: INFO: Running \\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c sudo mount --bind /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b\\' Nov 25 23:47:35.536: INFO: stderr: \"\" Nov 25 23:47:35.536: INFO: stdout: \"\" [It] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:488 STEP: Creating a directory, not bind mounted, in discovery directory Nov 25 23:47:35.536: INFO: Running \\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c mkdir -p /tmp/e2e-tests-persistent-local-volumes-test-5lh44/notbindmount -m 777\\' Nov 25 23:47:36.694: INFO: stderr: \"\" Nov 25 23:47:36.694: INFO: stdout: \"\" STEP: Starting a provisioner daemonset STEP: Allowing provisioner to run for 30s and discover potential local PVs STEP: Examining provisioner logs for not an actual mountpoint message [AfterEach] Local volume provisioner [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:443 STEP: Unmounting the test mount point from \"/tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b\" Nov 25 23:48:07.156: INFO: Running \\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c [ ! -e /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b ] || sudo umount /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b\\' Nov 25 23:48:08.413: INFO: stderr: \"\" Nov 25 23:48:08.413: INFO: stdout: \"\" STEP: Removing the test mount point Nov 25 23:48:08.413: INFO: Running \\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c [ ! -e /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b ] || rm -r /tmp/e2e-tests-persistent-local-volumes-test-5lh44/vol-965cf18b-2f78-11eb-8afd-0a58ac10ac2b\\' Nov 25 23:48:09.597: INFO: stderr: \"\" Nov 25 23:48:09.597: INFO: stdout: \"\" STEP: Cleaning up persistent volume STEP: Cleaning up cluster role binding STEP: Unbinding priviledged Pod Security Policy to the service account local-storage-admin STEP: Removing the test discovery directory on node ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:48:12.043: INFO: Running \\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-138-252.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c [ ! -e /tmp/e2e-tests-persistent-local-volumes-test-5lh44 ] || rm -r /tmp/e2e-tests-persistent-local-volumes-test-5lh44\\' Nov 25 23:48:13.243: INFO: stderr: \"\" Nov 25 23:48:13.243: INFO: stdout: \"\" STEP: Removing the test discovery directory on node ip-10-0-142-135.us-west-2.compute.internal Nov 25 23:48:13.243: INFO: Running \\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-142-135.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c [ ! -e /tmp/e2e-tests-persistent-local-volumes-test-5lh44 ] || rm -r /tmp/e2e-tests-persistent-local-volumes-test-5lh44\\' Nov 25 23:48:14.560: INFO: stderr: \"\" Nov 25 23:48:14.560: INFO: stdout: \"\" STEP: Removing the test discovery directory on node ip-10-0-156-74.us-west-2.compute.internal Nov 25 23:48:14.560: INFO: Running \\'/usr/bin/kubectl --server=https://api.ci-op-yrkswzmk-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443 --kubeconfig=/tmp/admin.kubeconfig exec --namespace=e2e-tests-persistent-local-volumes-test-5lh44 hostexec-ip-10-0-156-74.us-west-2.compute.internal -- nsenter --mount=/rootfs/proc/1/ns/mnt -- sh -c [ ! -e /tmp/e2e-tests-persistent-local-volumes-test-5lh44 ] || rm -r /tmp/e2e-tests-persistent-local-volumes-test-5lh44\\' Nov 25 23:48:15.755: INFO: stderr: \"\" Nov 25 23:48:15.755: INFO: stdout: \"\" [AfterEach] [sig-storage] PersistentVolumes-local /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:154 STEP: Collecting events from namespace \"e2e-tests-persistent-local-volumes-test-5lh44\". STEP: Found 29 events. Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:19 +0000 UTC - event for hostexec-ip-10-0-138-252.us-west-2.compute.internal: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Pulled: Container image \"gcr.io/kubernetes-e2e-test-images/hostexec:1.1\" already present on machine Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:19 +0000 UTC - event for hostexec-ip-10-0-138-252.us-west-2.compute.internal: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Created: Created container hostexec Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:19 +0000 UTC - event for hostexec-ip-10-0-138-252.us-west-2.compute.internal: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Started: Started container hostexec Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:22 +0000 UTC - event for hostexec-ip-10-0-142-135.us-west-2.compute.internal: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Pulling: Pulling image \"gcr.io/kubernetes-e2e-test-images/hostexec:1.1\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:24 +0000 UTC - event for hostexec-ip-10-0-142-135.us-west-2.compute.internal: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Pulled: Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/hostexec:1.1\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:24 +0000 UTC - event for hostexec-ip-10-0-142-135.us-west-2.compute.internal: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Created: Created container hostexec Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:24 +0000 UTC - event for hostexec-ip-10-0-142-135.us-west-2.compute.internal: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Started: Started container hostexec Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:28 +0000 UTC - event for hostexec-ip-10-0-156-74.us-west-2.compute.internal: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Pulling: Pulling image \"gcr.io/kubernetes-e2e-test-images/hostexec:1.1\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:30 +0000 UTC - event for hostexec-ip-10-0-156-74.us-west-2.compute.internal: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Pulled: Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/hostexec:1.1\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:30 +0000 UTC - event for hostexec-ip-10-0-156-74.us-west-2.compute.internal: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Created: Created container hostexec Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:30 +0000 UTC - event for hostexec-ip-10-0-156-74.us-west-2.compute.internal: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Started: Started container hostexec Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:36 +0000 UTC - event for local-volume-provisioner: {daemonset-controller } SuccessfulCreate: Created pod: local-volume-provisioner-hksc9 Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:36 +0000 UTC - event for local-volume-provisioner: {daemonset-controller } SuccessfulCreate: Created pod: local-volume-provisioner-rgt7n Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:36 +0000 UTC - event for local-volume-provisioner: {daemonset-controller } SuccessfulCreate: Created pod: local-volume-provisioner-r4j57 Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:36 +0000 UTC - event for local-volume-provisioner-hksc9: {default-scheduler } Scheduled: Successfully assigned e2e-tests-persistent-local-volumes-test-5lh44/local-volume-provisioner-hksc9 to ip-10-0-156-74.us-west-2.compute.internal Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:36 +0000 UTC - event for local-volume-provisioner-r4j57: {default-scheduler } Scheduled: Successfully assigned e2e-tests-persistent-local-volumes-test-5lh44/local-volume-provisioner-r4j57 to ip-10-0-142-135.us-west-2.compute.internal Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:36 +0000 UTC - event for local-volume-provisioner-rgt7n: {default-scheduler } Scheduled: Successfully assigned e2e-tests-persistent-local-volumes-test-5lh44/local-volume-provisioner-rgt7n to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:44 +0000 UTC - event for local-volume-provisioner-hksc9: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Pulling: Pulling image \"quay.io/external_storage/local-volume-provisioner:v2.1.0\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:44 +0000 UTC - event for local-volume-provisioner-r4j57: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Pulling: Pulling image \"quay.io/external_storage/local-volume-provisioner:v2.1.0\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:44 +0000 UTC - event for local-volume-provisioner-rgt7n: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Pulling: Pulling image \"quay.io/external_storage/local-volume-provisioner:v2.1.0\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:55 +0000 UTC - event for local-volume-provisioner-hksc9: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Started: Started container provisioner Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:55 +0000 UTC - event for local-volume-provisioner-hksc9: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Pulled: Successfully pulled image \"quay.io/external_storage/local-volume-provisioner:v2.1.0\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:47:55 +0000 UTC - event for local-volume-provisioner-hksc9: {kubelet ip-10-0-156-74.us-west-2.compute.internal} Created: Created container provisioner Nov 25 23:48:16.017: INFO: At 2020-11-25 23:48:00 +0000 UTC - event for local-volume-provisioner-r4j57: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Created: Created container provisioner Nov 25 23:48:16.017: INFO: At 2020-11-25 23:48:00 +0000 UTC - event for local-volume-provisioner-r4j57: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Pulled: Successfully pulled image \"quay.io/external_storage/local-volume-provisioner:v2.1.0\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:48:00 +0000 UTC - event for local-volume-provisioner-r4j57: {kubelet ip-10-0-142-135.us-west-2.compute.internal} Started: Started container provisioner Nov 25 23:48:16.017: INFO: At 2020-11-25 23:48:15 +0000 UTC - event for local-volume-provisioner-rgt7n: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Pulled: Successfully pulled image \"quay.io/external_storage/local-volume-provisioner:v2.1.0\" Nov 25 23:48:16.017: INFO: At 2020-11-25 23:48:15 +0000 UTC - event for local-volume-provisioner-rgt7n: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Created: Created container provisioner Nov 25 23:48:16.017: INFO: At 2020-11-25 23:48:15 +0000 UTC - event for local-volume-provisioner-rgt7n: {kubelet ip-10-0-138-252.us-west-2.compute.internal} Started: Started container provisioner Nov 25 23:48:16.187: INFO: skipping dumping cluster info - cluster too large Nov 25 23:48:16.188: INFO: Waiting up to 3m0s for all (but 100) nodes to be ready STEP: Destroying namespace \"e2e-tests-persistent-local-volumes-test-5lh44\" for this suite. Nov 25 23:48:38.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Nov 25 23:48:44.640: INFO: namespace e2e-tests-persistent-local-volumes-test-5lh44 deletion completed in 28.365051104s Nov 25 23:48:44.641: INFO: Running AfterSuite actions on all nodes Nov 25 23:48:44.641: INFO: Running AfterSuite actions on node 1 fail [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:505]: Error getting logs from pod local-volume-provisioner-rgt7n in namespace e2e-tests-persistent-local-volumes-test-5lh44 Expected error: <*errors.StatusError | 0xc002185b90>: { ErrStatus: { TypeMeta: {Kind: \"\", APIVersion: \"\"}, ListMeta: {SelfLink: \"\", ResourceVersion: \"\", Continue: \"\"}, Status: \"Failure\", Message: \"the server rejected our request for an unknown reason (get pods local-volume-provisioner-rgt7n)\", Reason: \"BadRequest\", Details: { Name: \"local-volume-provisioner-rgt7n\", Group: \"\", Kind: \"pods\", UID: \"\", Causes: [ { Type: \"UnexpectedServerResponse\", Message: \"unknown\", Field: \"\", }, ], RetryAfterSeconds: 0, }, Code: 400, }, } the server rejected our request for an unknown reason (get pods local-volume-provisioner-rgt7n) not to have occurred Nov 25 23:47:37.246 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \"pod1-system-cluster-critical_kube-system(4f8730dc-2f78-11eb-99ad-02e5c45bf6c5)\": timeout expired waiting for volumes to attach or mount for pod \"kube-system\"/\"pod1-system-cluster-critical\". list of unmounted volumes=[default-token-kzggn]. list of unattached volumes=[default-token-kzggn] failed: (1m37s) 2020-11-25T23:48:44 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/28/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24s) 2020-11-25T23:49:08 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/29/79) \"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (42.1s) 2020-11-25T23:49:50 \"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/30/79) \"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.7s) 2020-11-25T23:50:11 \"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/31/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m18s) 2020-11-25T23:52:28 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/32/79) \"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:247]: Could not get controller-manager metrics - skipping skipped: (28.8s) 2020-11-25T23:52:57 \"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/33/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.2s) 2020-11-25T23:53:37 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/34/79) \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m50s) 2020-11-25T23:55:27 \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/35/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.5s) 2020-11-25T23:55:52 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/36/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.4s) 2020-11-25T23:56:16 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/37/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (54.3s) 2020-11-25T23:57:10 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/38/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-25T23:57:51 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/39/79) \"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.9s) 2020-11-25T23:58:12 \"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/40/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.2s) 2020-11-25T23:58:52 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/41/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24s) 2020-11-25T23:59:16 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/42/79) \"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m3s) 2020-11-26T00:01:19 \"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/43/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-11-26T00:01:59 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/44/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.2s) 2020-11-26T00:02:23 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/45/79) \"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (3m11s) 2020-11-26T00:05:34 \"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/46/79) \"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\" passed: (43.9s) 2020-11-26T00:06:18 \"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\" started: (1/47/79) \"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (2m6s) 2020-11-26T00:08:24 \"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/48/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m4s) 2020-11-26T00:10:28 \"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/49/79) \"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/scheduling/taint_based_evictions.go:63]: Requires at least 2 nodes (not -1) skipped: (20.9s) 2020-11-26T00:10:49 \"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/50/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m6s) 2020-11-26T00:12:56 \"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/51/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-26T00:13:36 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/52/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (22.9s) 2020-11-26T00:13:59 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/53/79) \"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m14s) 2020-11-26T00:15:13 \"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/54/79) \"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:81]: Environment does not support getting controller-manager metrics - skipping skipped: (21.3s) 2020-11-26T00:15:34 \"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/55/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.8s) 2020-11-26T00:16:15 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/56/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m44s) 2020-11-26T00:18:59 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/57/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-11-26T00:19:40 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/58/79) \"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m15s) 2020-11-26T00:20:54 \"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/59/79) \"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m26s) 2020-11-26T00:22:20 \"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/60/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-11-26T00:23:01 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/61/79) \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (38.2s) 2020-11-26T00:23:39 \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/62/79) \"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.9s) 2020-11-26T00:24:00 \"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/63/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m23s) 2020-11-26T00:25:23 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/64/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m29s) 2020-11-26T00:27:52 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/65/79) \"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.9s) 2020-11-26T00:28:13 \"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/66/79) \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m44s) 2020-11-26T00:30:57 \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/67/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-26T00:31:37 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/68/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-11-26T00:32:18 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/69/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.9s) 2020-11-26T00:32:59 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/70/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-11-26T00:33:39 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/71/79) \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (32.7s) 2020-11-26T00:34:12 \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/72/79) \"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/apps/daemon_set.go:378]: Requires at least 2 nodes (not -1) skipped: (21.6s) 2020-11-26T00:34:34 \"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/73/79) \"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\" passed: (1m1s) 2020-11-26T00:35:35 \"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\" started: (1/74/79) \"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:193]: Could not get controller-manager metrics - skipping skipped: (1m4s) 2020-11-26T00:36:38 \"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/75/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (49s) 2020-11-26T00:37:27 \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/76/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.5s) 2020-11-26T00:37:52 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/77/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-11-26T00:38:32 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/78/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-11-26T00:39:13 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/79/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m40s) 2020-11-26T00:40:53 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\" Timeline: Nov 25 23:17:23.031 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-143-191.us-west-2.compute.internal node/ip-10-0-143-191.us-west-2.compute.internal created Nov 25 23:17:25.260 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-143-191.us-west-2.compute.internal Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c18ce2736a4fed8091de16f868bc46e6a9a03b7956c2418bad42fbb041ffbafb\" already present on machine Nov 25 23:17:25.427 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-143-191.us-west-2.compute.internal Created container pruner Nov 25 23:17:25.511 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-143-191.us-west-2.compute.internal Started container pruner Nov 25 23:19:14.205 W ns/openshift-machine-config-operator pod/machine-config-daemon-nqlh7 node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 25 23:19:14.207 W ns/openshift-image-registry pod/node-ca-xt8dh node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 25 23:19:14.210 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-nqlh7 Nov 25 23:19:14.277 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-xt8dh Nov 25 23:19:14.277 I ns/openshift-machine-config-operator pod/machine-config-daemon-nqlh7 Stopping container machine-config-daemon Nov 25 23:19:14.277 I ns/openshift-image-registry pod/node-ca-xt8dh Stopping container node-ca Nov 25 23:19:14.277 W ns/openshift-marketplace pod/redhat-operators-7849bb68d6-45gqw node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 25 23:19:14.277 I ns/openshift-machine-config-operator pod/machine-config-daemon-nqlh7 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-nqlh7 Nov 25 23:19:14.277 I ns/openshift-monitoring pod/alertmanager-main-0 Marking for deletion Pod openshift-monitoring/alertmanager-main-0 Nov 25 23:19:14.277 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf Marking for deletion Pod openshift-monitoring/kube-state-metrics-7b4d49f7bd-5h5zf Nov 25 23:19:14.277 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-45gqw Marking for deletion Pod openshift-marketplace/redhat-operators-7849bb68d6-45gqw Nov 25 23:19:14.286 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 25 23:19:14.287 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-45gqw Stopping container redhat-operators Nov 25 23:19:14.287 I ns/openshift-image-registry pod/node-ca-xt8dh Marking for deletion Pod openshift-image-registry/node-ca-xt8dh Nov 25 23:19:14.288 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 0s Nov 25 23:19:14.291 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:19:14.357 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf Stopping container kube-state-metrics Nov 25 23:19:14.357 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf Stopping container kube-rbac-proxy-self Nov 25 23:19:14.358 I ns/openshift-marketplace replicaset/redhat-operators-7849bb68d6 Created pod: redhat-operators-7849bb68d6-7g8f6 Nov 25 23:19:14.358 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf Stopping container kube-rbac-proxy-main Nov 25 23:19:14.358 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Successfully assigned openshift-marketplace/redhat-operators-7849bb68d6-7g8f6 to ip-10-0-156-74.us-west-2.compute.internal Nov 25 23:19:14.358 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 node/ created Nov 25 23:19:14.358 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy Nov 25 23:19:14.358 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader Nov 25 23:19:14.358 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager Nov 25 23:19:14.358 I ns/openshift-monitoring replicaset/kube-state-metrics-7b4d49f7bd Created pod: kube-state-metrics-7b4d49f7bd-tjw9b Nov 25 23:19:14.358 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b node/ created Nov 25 23:19:14.366 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Successfully assigned openshift-monitoring/kube-state-metrics-7b4d49f7bd-tjw9b to ip-10-0-142-135.us-west-2.compute.internal Nov 25 23:19:14.366 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy (2 times) Nov 25 23:19:14.366 I ns/openshift-monitoring statefulset/alertmanager-main create Pod alertmanager-main-0 in StatefulSet alertmanager-main successful Nov 25 23:19:14.366 I ns/openshift-monitoring pod/alertmanager-main-0 Successfully assigned openshift-monitoring/alertmanager-main-0 to ip-10-0-156-74.us-west-2.compute.internal Nov 25 23:19:14.367 I ns/openshift-monitoring pod/alertmanager-main-0 node/ created Nov 25 23:19:14.415 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager (2 times) Nov 25 23:19:14.616 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader (2 times) Nov 25 23:19:15.119 E ns/openshift-marketplace pod/redhat-operators-7849bb68d6-45gqw node/ip-10-0-138-252.us-west-2.compute.internal container=redhat-operators container exited with code 2 (Error): Nov 25 23:19:16.553 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 25 23:19:16.554 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf node/ip-10-0-138-252.us-west-2.compute.internal container=kube-state-metrics container stopped being ready Nov 25 23:19:16.554 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf node/ip-10-0-138-252.us-west-2.compute.internal container=kube-rbac-proxy-main container stopped being ready Nov 25 23:19:16.554 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf node/ip-10-0-138-252.us-west-2.compute.internal container=kube-rbac-proxy-self container stopped being ready Nov 25 23:19:16.758 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-5h5zf node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:19:17.760 W ns/openshift-image-registry pod/node-ca-xt8dh node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:19:18.363 W ns/openshift-machine-config-operator pod/machine-config-daemon-nqlh7 node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:19:20.516 W ns/openshift-marketplace pod/redhat-operators-7849bb68d6-45gqw node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:19:21.665 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Nov 25 23:19:21.811 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Created container kube-rbac-proxy-main Nov 25 23:19:21.843 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Started container kube-rbac-proxy-main Nov 25 23:19:21.847 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Nov 25 23:19:21.852 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:69e43ce2394b34a984aca4c30461087dcb4cef9eb6c9a61467ac192d11782ca6\" already present on machine Nov 25 23:19:22.004 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Created container kube-rbac-proxy-self Nov 25 23:19:22.011 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Created container redhat-operators Nov 25 23:19:22.031 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Started container kube-rbac-proxy-self Nov 25 23:19:22.036 I ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Started container redhat-operators Nov 25 23:19:22.036 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c12c8ce2ac222f2205c8ab24dffef3ef34e3505c7a52d849b637ce3a7d91a388\" Nov 25 23:19:22.987 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b9d07e8eed67a34b50a3afaa36cd4b9f052697894e64b5c203d0675d12ab5ec\" already present on machine Nov 25 23:19:23.146 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager Nov 25 23:19:23.171 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager Nov 25 23:19:23.175 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Nov 25 23:19:23.329 I ns/openshift-monitoring pod/alertmanager-main-0 Created container config-reloader Nov 25 23:19:23.351 I ns/openshift-monitoring pod/alertmanager-main-0 Started container config-reloader Nov 25 23:19:23.357 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Nov 25 23:19:23.507 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager-proxy Nov 25 23:19:23.534 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager-proxy Nov 25 23:19:29.190 W ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Liveness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ Nov 25 23:19:29.888 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c12c8ce2ac222f2205c8ab24dffef3ef34e3505c7a52d849b637ce3a7d91a388\" Nov 25 23:19:30.034 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Created container kube-state-metrics Nov 25 23:19:30.057 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-tjw9b Started container kube-state-metrics Nov 25 23:19:34.720 W ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Readiness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ Nov 25 23:19:39.189 W ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Liveness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ (2 times) Nov 25 23:19:44.744 W ns/openshift-marketplace pod/redhat-operators-7849bb68d6-7g8f6 Readiness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ (2 times) Nov 25 23:20:30.702 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp node/ created Nov 25 23:20:30.707 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-mfswp Nov 25 23:20:30.716 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp Successfully assigned openshift-machine-config-operator/machine-config-daemon-mfswp to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:20:30.777 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4c7mx Nov 25 23:20:30.777 I ns/openshift-image-registry pod/node-ca-4c7mx Successfully assigned openshift-image-registry/node-ca-4c7mx to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:20:30.778 I ns/openshift-image-registry pod/node-ca-4c7mx node/ created Nov 25 23:20:31.418 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 25 23:20:31.549 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp Created container machine-config-daemon Nov 25 23:20:31.577 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp Started container machine-config-daemon Nov 25 23:20:39.453 I ns/openshift-image-registry pod/node-ca-4c7mx Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 25 23:20:39.578 I ns/openshift-image-registry pod/node-ca-4c7mx Created container node-ca Nov 25 23:20:39.603 I ns/openshift-image-registry pod/node-ca-4c7mx Started container node-ca Nov 25 23:23:41.200 W ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 25 23:23:41.201 W ns/openshift-image-registry pod/node-ca-4c7mx node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 25 23:23:41.271 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4c7mx Nov 25 23:23:41.271 I ns/openshift-image-registry pod/node-ca-4c7mx Marking for deletion Pod openshift-image-registry/node-ca-4c7mx Nov 25 23:23:41.271 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp Stopping container machine-config-daemon Nov 25 23:23:41.271 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-mfswp Nov 25 23:23:41.271 I ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-mfswp Nov 25 23:23:41.271 I ns/openshift-image-registry pod/node-ca-4c7mx Stopping container node-ca Nov 25 23:23:42.678 W ns/openshift-image-registry pod/node-ca-4c7mx node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 25 23:23:42.678 W ns/openshift-image-registry pod/node-ca-4c7mx node/ip-10-0-138-252.us-west-2.compute.internal container=node-ca container stopped being ready Nov 25 23:23:42.688 W ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 25 23:23:42.688 W ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp node/ip-10-0-138-252.us-west-2.compute.internal container=machine-config-daemon container stopped being ready Nov 25 23:23:50.521 W ns/openshift-machine-config-operator pod/machine-config-daemon-mfswp node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:23:50.596 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-glv4f Nov 25 23:23:50.596 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f Successfully assigned openshift-machine-config-operator/machine-config-daemon-glv4f to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:23:50.596 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f node/ created Nov 25 23:23:50.601 W ns/openshift-image-registry pod/node-ca-4c7mx node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:23:50.610 I ns/openshift-image-registry pod/node-ca-nzvxp node/ created Nov 25 23:23:50.676 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-nzvxp Nov 25 23:23:50.676 I ns/openshift-image-registry pod/node-ca-nzvxp Successfully assigned openshift-image-registry/node-ca-nzvxp to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:23:52.235 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 25 23:23:52.394 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f Created container machine-config-daemon Nov 25 23:23:52.413 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f Started container machine-config-daemon Nov 25 23:24:00.324 I ns/openshift-image-registry pod/node-ca-nzvxp Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 25 23:24:00.449 I ns/openshift-image-registry pod/node-ca-nzvxp Created container node-ca Nov 25 23:24:00.474 I ns/openshift-image-registry pod/node-ca-nzvxp Started container node-ca Nov 25 23:25:14.555 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (5 times) Nov 25 23:25:15.649 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (5 times) Nov 25 23:25:16.530 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (5 times) Nov 25 23:25:16.698 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (8 times) Nov 25 23:25:16.845 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (8 times) Nov 25 23:25:16.989 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (8 times) Nov 25 23:30:27.274 W ns/openshift-image-registry pod/node-ca-nzvxp node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 25 23:30:27.346 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-nzvxp Nov 25 23:30:27.346 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-glv4f Nov 25 23:30:27.346 I ns/openshift-image-registry pod/node-ca-nzvxp Stopping container node-ca Nov 25 23:30:27.346 W ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 25 23:30:27.347 I ns/openshift-image-registry pod/node-ca-nzvxp Marking for deletion Pod openshift-image-registry/node-ca-nzvxp Nov 25 23:30:27.347 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f Stopping container machine-config-daemon Nov 25 23:30:27.347 I ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-glv4f Nov 25 23:30:40.516 W ns/openshift-machine-config-operator pod/machine-config-daemon-glv4f node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:30:40.525 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 node/ created Nov 25 23:30:40.529 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-m7mr7 Nov 25 23:30:40.532 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 Successfully assigned openshift-machine-config-operator/machine-config-daemon-m7mr7 to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:30:40.593 W ns/openshift-image-registry pod/node-ca-nzvxp node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:30:40.597 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-ppmvr Nov 25 23:30:40.597 I ns/openshift-image-registry pod/node-ca-ppmvr Successfully assigned openshift-image-registry/node-ca-ppmvr to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:30:40.597 I ns/openshift-image-registry pod/node-ca-ppmvr node/ created Nov 25 23:30:42.017 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 25 23:30:42.170 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 Created container machine-config-daemon Nov 25 23:30:42.208 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 Started container machine-config-daemon Nov 25 23:30:49.670 I ns/openshift-image-registry pod/node-ca-ppmvr Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 25 23:30:49.798 I ns/openshift-image-registry pod/node-ca-ppmvr Created container node-ca Nov 25 23:30:49.825 I ns/openshift-image-registry pod/node-ca-ppmvr Started container node-ca Nov 25 23:35:14.643 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (6 times) Nov 25 23:35:15.527 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (6 times) Nov 25 23:35:16.480 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (6 times) Nov 25 23:35:16.658 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (9 times) Nov 25 23:35:16.807 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (9 times) Nov 25 23:35:16.987 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (9 times) Nov 25 23:45:13.532 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (10 times) Nov 25 23:45:13.678 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (10 times) Nov 25 23:45:13.814 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (10 times) Nov 25 23:45:15.214 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (7 times) Nov 25 23:45:16.195 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (7 times) Nov 25 23:45:17.405 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (7 times) Nov 25 23:45:34.135 I ns/kube-system pod/pod0-system-node-critical node/ created Nov 25 23:45:34.143 I ns/kube-system pod/pod0-system-node-critical Successfully assigned kube-system/pod0-system-node-critical to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:45:34.218 I ns/kube-system pod/pod1-system-cluster-critical node/ created Nov 25 23:45:34.226 I ns/kube-system pod/pod1-system-cluster-critical Successfully assigned kube-system/pod1-system-cluster-critical to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:45:34.305 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 0s Nov 25 23:45:34.308 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:45:34.394 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 0s Nov 25 23:45:34.397 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:45:34.569 W ns/kube-system pod/pod0-system-node-critical Failed create pod sandbox: rpc error: code = Unknown desc = error reading container (probably exited) json message: EOF Nov 25 23:47:07.382 - 97s I test=\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" running Nov 25 23:47:37.246 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \"pod1-system-cluster-critical_kube-system(4f8730dc-2f78-11eb-99ad-02e5c45bf6c5)\": timeout expired waiting for volumes to attach or mount for pod \"kube-system\"/\"pod1-system-cluster-critical\". list of unmounted volumes=[default-token-kzggn]. list of unattached volumes=[default-token-kzggn] Nov 25 23:48:44.653 I test=\"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" failed Nov 25 23:54:48.105 W ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 25 23:54:48.106 W ns/openshift-image-registry pod/node-ca-ppmvr node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 25 23:54:48.113 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-m7mr7 Nov 25 23:54:48.118 I ns/openshift-image-registry pod/node-ca-ppmvr Marking for deletion Pod openshift-image-registry/node-ca-ppmvr Nov 25 23:54:48.119 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-ppmvr Nov 25 23:54:48.122 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-m7mr7 Nov 25 23:54:48.122 I ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 Stopping container machine-config-daemon Nov 25 23:54:48.126 I ns/openshift-image-registry pod/node-ca-ppmvr Stopping container node-ca Nov 25 23:54:49.649 W ns/openshift-image-registry pod/node-ca-ppmvr node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 25 23:54:49.649 W ns/openshift-image-registry pod/node-ca-ppmvr node/ip-10-0-138-252.us-west-2.compute.internal container=node-ca container stopped being ready Nov 25 23:54:52.179 W ns/openshift-image-registry pod/node-ca-ppmvr node/ip-10-0-138-252.us-west-2.compute.internal pod has been pending longer than a minute Nov 25 23:55:00.593 W ns/openshift-machine-config-operator pod/machine-config-daemon-m7mr7 node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:55:00.593 W ns/openshift-image-registry pod/node-ca-ppmvr node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 25 23:55:13.543 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (11 times) Nov 25 23:55:14.885 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc node/ created Nov 25 23:55:14.892 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-qkdwc Nov 25 23:55:14.957 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc Successfully assigned openshift-machine-config-operator/machine-config-daemon-qkdwc to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:55:14.957 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-jv5vl Nov 25 23:55:14.957 I ns/openshift-image-registry pod/node-ca-jv5vl Successfully assigned openshift-image-registry/node-ca-jv5vl to ip-10-0-138-252.us-west-2.compute.internal Nov 25 23:55:14.957 I ns/openshift-image-registry pod/node-ca-jv5vl node/ created Nov 25 23:55:15.111 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (8 times) Nov 25 23:55:15.562 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 25 23:55:15.708 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc Created container machine-config-daemon Nov 25 23:55:15.733 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc Started container machine-config-daemon Nov 25 23:55:15.991 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (8 times) Nov 25 23:55:16.855 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (8 times) Nov 25 23:55:17.008 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (11 times) Nov 25 23:55:17.198 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (11 times) Nov 25 23:55:22.416 I ns/openshift-image-registry pod/node-ca-jv5vl Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 25 23:55:22.538 I ns/openshift-image-registry pod/node-ca-jv5vl Created container node-ca Nov 25 23:55:22.564 I ns/openshift-image-registry pod/node-ca-jv5vl Started container node-ca Nov 26 00:05:13.462 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (12 times) Nov 26 00:05:13.599 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (12 times) Nov 26 00:05:13.749 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (12 times) Nov 26 00:05:14.751 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (9 times) Nov 26 00:05:15.650 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (9 times) Nov 26 00:05:16.510 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (9 times) Nov 26 00:12:11.026 I ns/kube-system pod/critical-pod node/ created Nov 26 00:12:11.031 W ns/kube-system pod/critical-pod 0/6 nodes are available: 3 Insufficient memory, 3 node(s) had taints that the pod didn\\'t tolerate. Nov 26 00:12:11.108 W ns/kube-system pod/critical-pod 0/6 nodes are available: 3 Insufficient memory, 3 node(s) had taints that the pod didn\\'t tolerate. (2 times) Nov 26 00:12:20.522 I ns/kube-system pod/critical-pod Successfully assigned kube-system/critical-pod to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:12:28.122 I ns/kube-system pod/critical-pod Container image \"k8s.gcr.io/pause:3.1\" already present on machine Nov 26 00:12:28.256 I ns/kube-system pod/critical-pod Created container critical-pod Nov 26 00:12:28.275 I ns/kube-system pod/critical-pod Started container critical-pod Nov 26 00:12:29.629 W ns/kube-system pod/critical-pod node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 0s Nov 26 00:12:29.633 W ns/kube-system pod/critical-pod node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:12:29.638 I ns/kube-system pod/critical-pod Stopping container critical-pod Nov 26 00:12:29.678 W ns/kube-system pod/critical-pod MountVolume.SetUp failed for volume \"default-token-kzggn\" : object \"kube-system\"/\"default-token-kzggn\" not registered Nov 26 00:12:30.184 W ns/kube-system pod/critical-pod MountVolume.SetUp failed for volume \"default-token-kzggn\" : object \"kube-system\"/\"default-token-kzggn\" not registered (2 times) Nov 26 00:12:31.186 W ns/kube-system pod/critical-pod MountVolume.SetUp failed for volume \"default-token-kzggn\" : object \"kube-system\"/\"default-token-kzggn\" not registered (3 times) Nov 26 00:15:13.486 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (13 times) Nov 26 00:15:13.723 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (13 times) Nov 26 00:15:13.871 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (13 times) Nov 26 00:15:14.899 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (10 times) Nov 26 00:15:15.866 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (10 times) Nov 26 00:15:17.116 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (10 times) Nov 26 00:17:25.745 W ns/openshift-image-registry pod/node-ca-jv5vl node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 26 00:17:25.747 W ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 26 00:17:25.749 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-jv5vl Nov 26 00:17:25.752 I ns/openshift-image-registry pod/node-ca-jv5vl Marking for deletion Pod openshift-image-registry/node-ca-jv5vl Nov 26 00:17:25.754 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-qkdwc Nov 26 00:17:25.815 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-qkdwc Nov 26 00:17:25.815 I ns/openshift-image-registry pod/node-ca-jv5vl Stopping container node-ca Nov 26 00:17:25.815 I ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc Stopping container machine-config-daemon Nov 26 00:17:27.092 W ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 26 00:17:27.092 W ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc node/ip-10-0-138-252.us-west-2.compute.internal container=machine-config-daemon container stopped being ready Nov 26 00:17:27.102 W ns/openshift-image-registry pod/node-ca-jv5vl node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 26 00:17:27.102 W ns/openshift-image-registry pod/node-ca-jv5vl node/ip-10-0-138-252.us-west-2.compute.internal container=node-ca container stopped being ready Nov 26 00:17:30.516 W ns/openshift-machine-config-operator pod/machine-config-daemon-qkdwc node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:17:30.593 W ns/openshift-image-registry pod/node-ca-jv5vl node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:18:31.006 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 node/ created Nov 26 00:18:31.015 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 Successfully assigned openshift-machine-config-operator/machine-config-daemon-nvht9 to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:18:31.015 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-nvht9 Nov 26 00:18:31.023 I ns/openshift-image-registry pod/node-ca-749k6 node/ created Nov 26 00:18:31.076 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-749k6 Nov 26 00:18:31.076 I ns/openshift-image-registry pod/node-ca-749k6 Successfully assigned openshift-image-registry/node-ca-749k6 to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:18:31.682 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 26 00:18:31.798 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 Created container machine-config-daemon Nov 26 00:18:31.824 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 Started container machine-config-daemon Nov 26 00:18:39.118 I ns/openshift-image-registry pod/node-ca-749k6 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 26 00:18:39.190 I ns/openshift-image-registry pod/node-ca-749k6 Created container node-ca Nov 26 00:18:39.219 I ns/openshift-image-registry pod/node-ca-749k6 Started container node-ca Nov 26 00:23:24.250 W ns/openshift-image-registry pod/node-ca-749k6 node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 26 00:23:24.319 W ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 26 00:23:24.319 I ns/openshift-image-registry pod/node-ca-749k6 Marking for deletion Pod openshift-image-registry/node-ca-749k6 Nov 26 00:23:24.319 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-nvht9 Nov 26 00:23:24.319 I ns/openshift-image-registry pod/node-ca-749k6 Stopping container node-ca Nov 26 00:23:24.319 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-nvht9 Nov 26 00:23:24.319 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-749k6 Nov 26 00:23:24.319 I ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 Stopping container machine-config-daemon Nov 26 00:23:25.754 W ns/openshift-image-registry pod/node-ca-749k6 node/ip-10-0-138-252.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Nov 26 00:23:25.754 W ns/openshift-image-registry pod/node-ca-749k6 node/ip-10-0-138-252.us-west-2.compute.internal container=node-ca container stopped being ready Nov 26 00:23:30.517 W ns/openshift-machine-config-operator pod/machine-config-daemon-nvht9 node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:23:30.594 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-rnv4f Nov 26 00:23:30.594 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f Successfully assigned openshift-machine-config-operator/machine-config-daemon-rnv4f to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:23:30.594 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f node/ created Nov 26 00:23:30.598 W ns/openshift-image-registry pod/node-ca-749k6 node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:23:30.607 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4l9f2 Nov 26 00:23:30.608 I ns/openshift-image-registry pod/node-ca-4l9f2 node/ created Nov 26 00:23:30.675 I ns/openshift-image-registry pod/node-ca-4l9f2 Successfully assigned openshift-image-registry/node-ca-4l9f2 to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:23:32.017 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 26 00:23:32.152 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f Created container machine-config-daemon Nov 26 00:23:32.203 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f Started container machine-config-daemon Nov 26 00:23:39.480 I ns/openshift-image-registry pod/node-ca-4l9f2 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 26 00:23:39.585 I ns/openshift-image-registry pod/node-ca-4l9f2 Created container node-ca Nov 26 00:23:39.613 I ns/openshift-image-registry pod/node-ca-4l9f2 Started container node-ca Nov 26 00:25:14.541 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (11 times) Nov 26 00:25:15.614 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (11 times) Nov 26 00:25:15.772 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (14 times) Nov 26 00:25:15.919 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (14 times) Nov 26 00:25:16.065 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (14 times) Nov 26 00:25:17.028 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (11 times) Nov 26 00:29:23.304 W ns/openshift-image-registry pod/node-ca-4l9f2 node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 30s Nov 26 00:29:23.345 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4l9f2 Nov 26 00:29:23.345 W ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f node/ip-10-0-138-252.us-west-2.compute.internal graceful deletion within 600s Nov 26 00:29:23.346 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-rnv4f Nov 26 00:29:23.346 I ns/openshift-image-registry pod/node-ca-4l9f2 Stopping container node-ca Nov 26 00:29:23.346 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-rnv4f Nov 26 00:29:23.346 I ns/openshift-image-registry pod/node-ca-4l9f2 Marking for deletion Pod openshift-image-registry/node-ca-4l9f2 Nov 26 00:29:23.346 I ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f Stopping container machine-config-daemon Nov 26 00:29:30.518 W ns/openshift-image-registry pod/node-ca-4l9f2 node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:29:30.599 W ns/openshift-machine-config-operator pod/machine-config-daemon-rnv4f node/ip-10-0-138-252.us-west-2.compute.internal deleted Nov 26 00:30:28.561 I ns/openshift-machine-config-operator pod/machine-config-daemon-zvvmz node/ created Nov 26 00:30:28.572 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-zvvmz Nov 26 00:30:28.573 I ns/openshift-machine-config-operator pod/machine-config-daemon-zvvmz Successfully assigned openshift-machine-config-operator/machine-config-daemon-zvvmz to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:30:28.576 I ns/openshift-image-registry pod/node-ca-9d8ss node/ created Nov 26 00:30:28.580 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-9d8ss Nov 26 00:30:28.633 I ns/openshift-image-registry pod/node-ca-9d8ss Successfully assigned openshift-image-registry/node-ca-9d8ss to ip-10-0-138-252.us-west-2.compute.internal Nov 26 00:30:29.235 I ns/openshift-machine-config-operator pod/machine-config-daemon-zvvmz Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Nov 26 00:30:29.363 I ns/openshift-machine-config-operator pod/machine-config-daemon-zvvmz Created container machine-config-daemon Nov 26 00:30:29.389 I ns/openshift-machine-config-operator pod/machine-config-daemon-zvvmz Started container machine-config-daemon Nov 26 00:30:36.859 I ns/openshift-image-registry pod/node-ca-9d8ss Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 26 00:30:36.979 I ns/openshift-image-registry pod/node-ca-9d8ss Created container node-ca Nov 26 00:30:37.002 I ns/openshift-image-registry pod/node-ca-9d8ss Started container node-ca Nov 26 00:35:13.465 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (15 times) Nov 26 00:35:14.452 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (12 times) Nov 26 00:35:15.349 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (12 times) Nov 26 00:35:16.552 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (12 times) Nov 26 00:35:16.714 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (15 times) Nov 26 00:35:16.852 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (15 times) Nov 26 00:36:20.736 W persistentvolume/pvc-5160dc4a-2f7f-11eb-8c06-02e62f39bc9f Error deleting EBS volume \"vol-0b1b3084600ba3bff\" since volume is currently attached to \"i-083ba1ff7d598fddb\" Failing tests: [sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201126-004053.xml error: 1 fail, 39 pass, 39 skip (1h23m31s) 2020/11/26 00:40:57 Container test in pod e2e-aws-serial failed, exit code 1, reason Error 2020/11/26 01:03:32 Container teardown in pod e2e-aws-serial completed successfully 2020/11/26 01:03:41 Copied 123.49MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/11/26 01:04:17 Container artifacts in pod e2e-aws-serial completed successfully 2020/11/26 01:04:17 Releasing leases for \"e2e-aws-serial\" 2020/11/26 01:04:17 Releasing lease \"37f911f5-b8d2-4f47-99a0-9a2fde2bc334\" for \"aws-quota-slice\" 2020/11/26 01:04:17 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/11/26 01:04:17 Ran for 2h20m22s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-yrkswzmk/e2e-aws-serial failed after 2h18m34s (failed containers: test): ContainerFailed one or more containers exited Container test exited with code 1, reason Error --- or pod/machine-config-daemon-zvvmz Started container machine-config-daemon Nov 26 00:30:36.859 I ns/openshift-image-registry pod/node-ca-9d8ss Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Nov 26 00:30:36.979 I ns/openshift-image-registry pod/node-ca-9d8ss Created container node-ca Nov 26 00:30:37.002 I ns/openshift-image-registry pod/node-ca-9d8ss Started container node-ca Nov 26 00:35:13.465 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2b-5jpt2 (15 times) Nov 26 00:35:14.452 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-0 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-0 (12 times) Nov 26 00:35:15.349 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-1 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-1 (12 times) Nov 26 00:35:16.552 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-master-2 Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-master-2 (12 times) Nov 26 00:35:16.714 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-jf77k (15 times) Nov 26 00:35:16.852 I ns/openshift-machine-api machine/ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh Updated machine ci-op-yrkswzmk-7bc5c-ghfw8-worker-us-west-2a-qfgsh (15 times) Nov 26 00:36:20.736 W persistentvolume/pvc-5160dc4a-2f7f-11eb-8c06-02e62f39bc9f Error deleting EBS volume \"vol-0b1b3084600ba3bff\" since volume is currently attached to \"i-083ba1ff7d598fddb\" Failing tests: [sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201126-004053.xml error: 1 fail, 39 pass, 39 skip (1h23m31s) --- '\n",
            "ID=27    : size=1         : b'2020/12/01 22:49:34 ci-operator version v20201201-064c8f5 2020/12/01 22:49:34 No source defined 2020/12/01 22:49:34 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/01 22:49:34 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-lby5f363 2020/12/01 22:49:34 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/01 22:49:34 Creating namespace ci-op-lby5f363 2020/12/01 22:49:34 Setting up pipeline imagestream for the test 2020/12/01 22:49:34 Created secret e2e-aws-serial-cluster-profile 2020/12/01 22:49:34 Created secret pull-secret 2020/12/01 22:49:34 Created PDB for pods with openshift.io/build.name label 2020/12/01 22:49:34 Created PDB for pods with created-by-ci label 2020/12/01 22:49:34 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-lby5f363/stable:${component} 2020/12/01 22:49:36 Importing release image latest 2020/12/01 22:49:37 Executing pod \"release-images-latest-cli\" 2020/12/01 22:49:42 Executing pod \"release-images-latest\" 2020/12/01 22:50:29 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/01 22:50:29 Acquiring leases for \"e2e-aws-serial\" 2020/12/01 22:50:29 Acquiring 1 lease(s) for \"aws-quota-slice\" 2020/12/01 22:56:03 Acquired lease(s) [aeca4927-4e1d-407d-99f4-a405ff9a723b] for \"aws-quota-slice\" 2020/12/01 22:56:03 Executing template e2e-aws-serial 2020/12/01 22:56:03 Creating or restarting template instance 2020/12/01 22:56:03 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/01 22:56:03 Waiting for template instance to be ready 2020/12/01 22:56:05 Running pod e2e-aws-serial 2020/12/01 23:27:55 Container setup in pod e2e-aws-serial completed successfully secret/support created started: (0/1/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m45s) 2020-12-01T23:29:45 \"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/2/79) \"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (51.6s) 2020-12-01T23:30:37 \"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/3/79) \"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\" skip [github.com/openshift/origin/test/extended/images/signatures.go:25]: disable because containers/image: https://github.com/containers/image/pull/570 skipped: (8.9s) 2020-12-01T23:30:46 \"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\" started: (0/4/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (35.2s) 2020-12-01T23:31:21 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/5/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (14.7s) 2020-12-01T23:31:36 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/6/79) \"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m3s) 2020-12-01T23:32:39 \"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/7/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (36.7s) 2020-12-01T23:33:15 \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/8/79) \"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:193]: Could not get controller-manager metrics - skipping skipped: (58.5s) 2020-12-01T23:34:14 \"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/9/79) \"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m45s) 2020-12-01T23:35:59 \"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (0/10/79) \"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (10.2s) 2020-12-01T23:36:09 \"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/11/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.2s) 2020-12-01T23:36:38 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/12/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.1s) 2020-12-01T23:37:07 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/13/79) \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (23s) 2020-12-01T23:37:30 \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/14/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m28s) 2020-12-01T23:39:59 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/15/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (11.3s) 2020-12-01T23:40:10 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/16/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.2s) 2020-12-01T23:40:39 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/17/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (25.1s) 2020-12-01T23:41:04 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/18/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (11.8s) 2020-12-01T23:41:16 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/19/79) \"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m3s) 2020-12-01T23:42:19 \"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/20/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.6s) 2020-12-01T23:42:31 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/21/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" Dec 1 23:42:32.581: INFO: >>> kubeConfig: /tmp/admin.kubeconfig Dec 1 23:42:32.583: INFO: Waiting up to 30m0s for all (but 100) nodes to be schedulable Dec 1 23:42:32.720: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace \\'kube-system\\' to be running and ready Dec 1 23:42:32.787: INFO: 0 / 0 pods in namespace \\'kube-system\\' are running and ready (0 seconds elapsed) Dec 1 23:42:32.787: INFO: expected 0 pod replicas in namespace \\'kube-system\\', 0 are Running and Ready. Dec 1 23:42:32.787: INFO: Waiting up to 5m0s for all daemonsets in namespace \\'kube-system\\' to start Dec 1 23:42:32.808: INFO: e2e test version: v1.13.4-138-g41dc99c Dec 1 23:42:32.823: INFO: kube-apiserver version: v1.13.4-138-g41dc99c [BeforeEach] [Top Level] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/test/extended/util/test.go:69 [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:153 STEP: Creating a kubernetes client Dec 1 23:42:32.827: INFO: >>> kubeConfig: /tmp/admin.kubeconfig STEP: Building a namespace api object, basename sched-priority Dec 1 23:42:33.840: INFO: About to run a Kube e2e test, ensuring namespace is privileged Dec 1 23:42:34.071: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled. STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:71 Dec 1 23:42:34.087: INFO: Waiting up to 1m0s for all nodes to be ready Dec 1 23:43:34.353: INFO: Waiting for terminating namespaces to be deleted... Dec 1 23:43:34.371: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace \\'kube-system\\' to be running and ready Dec 1 23:43:34.424: INFO: 0 / 0 pods in namespace \\'kube-system\\' are running and ready (0 seconds elapsed) Dec 1 23:43:34.424: INFO: expected 0 pod replicas in namespace \\'kube-system\\', 0 are Running and Ready. [It] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:85 STEP: Trying to launch a pod with a label to get a node which can launch it. STEP: Trying to apply a label on the found node. STEP: verifying the node has the label kubernetes.io/e2e-node-topologyKey topologyvalue Dec 1 23:43:44.553: INFO: ComputeCpuMemFraction for node: ip-10-0-140-191.ec2.internal Dec 1 23:43:44.648: INFO: Pod for on the node: pod-with-label-security-s1, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.648: INFO: Pod for on the node: tuned-7svpm, Cpu: 10, Mem: 20971520 Dec 1 23:43:44.648: INFO: Pod for on the node: dns-default-vlt84, Cpu: 110, Mem: 283115520 Dec 1 23:43:44.648: INFO: Pod for on the node: node-ca-xlfl5, Cpu: 10, Mem: 10485760 Dec 1 23:43:44.648: INFO: Pod for on the node: router-default-79b887b8c9-9xqjr, Cpu: 100, Mem: 268435456 Dec 1 23:43:44.648: INFO: Pod for on the node: machine-config-daemon-q5dkj, Cpu: 20, Mem: 52428800 Dec 1 23:43:44.648: INFO: Pod for on the node: node-exporter-cbwlb, Cpu: 110, Mem: 230686720 Dec 1 23:43:44.648: INFO: Pod for on the node: multus-b6fwc, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.648: INFO: Pod for on the node: olm-operators-vm6pv, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.648: INFO: Pod for on the node: ovs-vvw9b, Cpu: 200, Mem: 419430400 Dec 1 23:43:44.648: INFO: Pod for on the node: sdn-zfr5g, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.648: INFO: Node: ip-10-0-140-191.ec2.internal, totalRequestedCpuResource: 760, cpuAllocatableMil: 3500, cpuFraction: 0.21714285714285714 Dec 1 23:43:44.648: INFO: Node: ip-10-0-140-191.ec2.internal, totalRequestedMemResource: 1600126976, memAllocatableVal: 16181800960, memFraction: 0.09888435656546353 Dec 1 23:43:44.648: INFO: ComputeCpuMemFraction for node: ip-10-0-142-186.ec2.internal Dec 1 23:43:44.699: INFO: Pod for on the node: tuned-99wxh, Cpu: 10, Mem: 20971520 Dec 1 23:43:44.699: INFO: Pod for on the node: dns-default-hnpdr, Cpu: 110, Mem: 283115520 Dec 1 23:43:44.700: INFO: Pod for on the node: image-registry-7dd97dddc5-rmdg7, Cpu: 100, Mem: 268435456 Dec 1 23:43:44.700: INFO: Pod for on the node: node-ca-4gncf, Cpu: 10, Mem: 10485760 Dec 1 23:43:44.700: INFO: Pod for on the node: router-default-79b887b8c9-s7546, Cpu: 100, Mem: 268435456 Dec 1 23:43:44.700: INFO: Pod for on the node: machine-config-daemon-wrg7t, Cpu: 20, Mem: 52428800 Dec 1 23:43:44.700: INFO: Pod for on the node: certified-operators-6d6b4db4dc-b97lm, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: community-operators-b58cc5bd7-rbdvk, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: redhat-operators-6448849fdd-v4kx7, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Dec 1 23:43:44.700: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Dec 1 23:43:44.700: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Dec 1 23:43:44.700: INFO: Pod for on the node: grafana-649f787944-pfq4s, Cpu: 200, Mem: 314572800 Dec 1 23:43:44.700: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-m4jmq, Cpu: 300, Mem: 629145600 Dec 1 23:43:44.700: INFO: Pod for on the node: node-exporter-pgcxm, Cpu: 110, Mem: 230686720 Dec 1 23:43:44.700: INFO: Pod for on the node: prometheus-adapter-5448dfb8fb-5bljf, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: prometheus-adapter-5448dfb8fb-kj62f, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Dec 1 23:43:44.700: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Dec 1 23:43:44.700: INFO: Pod for on the node: prometheus-operator-5d4588dd6-r9d9r, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: telemeter-client-78884c9754-rtv2q, Cpu: 210, Mem: 440401920 Dec 1 23:43:44.700: INFO: Pod for on the node: multus-sp57c, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Pod for on the node: ovs-prgfz, Cpu: 200, Mem: 419430400 Dec 1 23:43:44.700: INFO: Pod for on the node: sdn-crq7f, Cpu: 100, Mem: 209715200 Dec 1 23:43:44.700: INFO: Node: ip-10-0-142-186.ec2.internal, totalRequestedCpuResource: 2170, cpuAllocatableMil: 3500, cpuFraction: 0.62 Dec 1 23:43:44.700: INFO: Node: ip-10-0-142-186.ec2.internal, totalRequestedMemResource: 4510973952, memAllocatableVal: 16181792768, memFraction: 0.2787684910240966 Dec 1 23:43:44.727: INFO: Waiting for running... Dec 1 23:43:54.804: INFO: Waiting for running... STEP: Compute Cpu, Mem Fraction after create balanced pods. Dec 1 23:44:04.855: INFO: ComputeCpuMemFraction for node: ip-10-0-140-191.ec2.internal Dec 1 23:44:04.965: INFO: Pod for on the node: 0cc0d13d-342f-11eb-a9f6-0a58ac106ed2-0, Cpu: 1410, Mem: 8432589619 Dec 1 23:44:04.965: INFO: Pod for on the node: pod-with-label-security-s1, Cpu: 100, Mem: 209715200 Dec 1 23:44:04.965: INFO: Pod for on the node: tuned-7svpm, Cpu: 10, Mem: 20971520 Dec 1 23:44:04.965: INFO: Pod for on the node: dns-default-vlt84, Cpu: 110, Mem: 283115520 Dec 1 23:44:04.965: INFO: Pod for on the node: node-ca-xlfl5, Cpu: 10, Mem: 10485760 Dec 1 23:44:04.965: INFO: Pod for on the node: router-default-79b887b8c9-9xqjr, Cpu: 100, Mem: 268435456 Dec 1 23:44:04.965: INFO: Pod for on the node: machine-config-daemon-q5dkj, Cpu: 20, Mem: 52428800 Dec 1 23:44:04.965: INFO: Pod for on the node: node-exporter-cbwlb, Cpu: 110, Mem: 230686720 Dec 1 23:44:04.965: INFO: Pod for on the node: multus-b6fwc, Cpu: 100, Mem: 209715200 Dec 1 23:44:04.965: INFO: Pod for on the node: olm-operators-vm6pv, Cpu: 100, Mem: 209715200 Dec 1 23:44:04.965: INFO: Pod for on the node: ovs-vvw9b, Cpu: 200, Mem: 419430400 Dec 1 23:44:04.965: INFO: Pod for on the node: sdn-zfr5g, Cpu: 100, Mem: 209715200 Dec 1 23:44:04.965: INFO: Node: ip-10-0-140-191.ec2.internal, totalRequestedCpuResource: 2170, cpuAllocatableMil: 3500, cpuFraction: 0.62 Dec 1 23:44:04.965: INFO: Node: ip-10-0-140-191.ec2.internal, totalRequestedMemResource: 10032716595, memAllocatableVal: 16181800960, memFraction: 0.6199999999876404 STEP: Compute Cpu, Mem Fraction after create balanced pods. Dec 1 23:44:04.965: INFO: ComputeCpuMemFraction for node: ip-10-0-142-186.ec2.internal Dec 1 23:44:05.019: INFO: Pod for on the node: 12c294c9-342f-11eb-a9f6-0a58ac106ed2-0, Cpu: 0, Mem: 5521737564 Dec 1 23:44:05.019: INFO: Pod for on the node: tuned-99wxh, Cpu: 10, Mem: 20971520 Dec 1 23:44:05.019: INFO: Pod for on the node: dns-default-hnpdr, Cpu: 110, Mem: 283115520 Dec 1 23:44:05.019: INFO: Pod for on the node: image-registry-7dd97dddc5-rmdg7, Cpu: 100, Mem: 268435456 Dec 1 23:44:05.019: INFO: Pod for on the node: node-ca-4gncf, Cpu: 10, Mem: 10485760 Dec 1 23:44:05.019: INFO: Pod for on the node: router-default-79b887b8c9-s7546, Cpu: 100, Mem: 268435456 Dec 1 23:44:05.019: INFO: Pod for on the node: machine-config-daemon-wrg7t, Cpu: 20, Mem: 52428800 Dec 1 23:44:05.019: INFO: Pod for on the node: certified-operators-6d6b4db4dc-b97lm, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: community-operators-b58cc5bd7-rbdvk, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: redhat-operators-6448849fdd-v4kx7, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: alertmanager-main-0, Cpu: 300, Mem: 629145600 Dec 1 23:44:05.019: INFO: Pod for on the node: alertmanager-main-1, Cpu: 300, Mem: 629145600 Dec 1 23:44:05.019: INFO: Pod for on the node: alertmanager-main-2, Cpu: 300, Mem: 629145600 Dec 1 23:44:05.019: INFO: Pod for on the node: grafana-649f787944-pfq4s, Cpu: 200, Mem: 314572800 Dec 1 23:44:05.019: INFO: Pod for on the node: kube-state-metrics-7b4d49f7bd-m4jmq, Cpu: 300, Mem: 629145600 Dec 1 23:44:05.019: INFO: Pod for on the node: node-exporter-pgcxm, Cpu: 110, Mem: 230686720 Dec 1 23:44:05.019: INFO: Pod for on the node: prometheus-adapter-5448dfb8fb-5bljf, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: prometheus-adapter-5448dfb8fb-kj62f, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: prometheus-k8s-0, Cpu: 600, Mem: 1258291200 Dec 1 23:44:05.019: INFO: Pod for on the node: prometheus-k8s-1, Cpu: 600, Mem: 1258291200 Dec 1 23:44:05.019: INFO: Pod for on the node: prometheus-operator-5d4588dd6-r9d9r, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: telemeter-client-78884c9754-rtv2q, Cpu: 210, Mem: 440401920 Dec 1 23:44:05.019: INFO: Pod for on the node: multus-sp57c, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Pod for on the node: ovs-prgfz, Cpu: 200, Mem: 419430400 Dec 1 23:44:05.019: INFO: Pod for on the node: sdn-crq7f, Cpu: 100, Mem: 209715200 Dec 1 23:44:05.019: INFO: Node: ip-10-0-142-186.ec2.internal, totalRequestedCpuResource: 2170, cpuAllocatableMil: 3500, cpuFraction: 0.62 Dec 1 23:44:05.019: INFO: Node: ip-10-0-142-186.ec2.internal, totalRequestedMemResource: 10032711516, memAllocatableVal: 16181792768, memFraction: 0.6199999999901123 STEP: Trying to launch the pod with podAntiAffinity. STEP: Wait the pod becomes running STEP: Verify the pod was scheduled to the expected node. STEP: removing the label kubernetes.io/e2e-node-topologyKey off the node ip-10-0-140-191.ec2.internal STEP: verifying the node doesn\\'t have the label kubernetes.io/e2e-node-topologyKey [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/framework/framework.go:154 STEP: Collecting events from namespace \"e2e-tests-sched-priority-6pg47\". STEP: Found 14 events. Dec 1 23:44:15.183: INFO: At 2020-12-01 23:43:34 +0000 UTC - event for pod-with-label-security-s1: {default-scheduler } Scheduled: Successfully assigned e2e-tests-sched-priority-6pg47/pod-with-label-security-s1 to ip-10-0-140-191.ec2.internal Dec 1 23:44:15.183: INFO: At 2020-12-01 23:43:42 +0000 UTC - event for pod-with-label-security-s1: {kubelet ip-10-0-140-191.ec2.internal} Started: Started container pod-with-label-security-s1 Dec 1 23:44:15.183: INFO: At 2020-12-01 23:43:42 +0000 UTC - event for pod-with-label-security-s1: {kubelet ip-10-0-140-191.ec2.internal} Created: Created container pod-with-label-security-s1 Dec 1 23:44:15.183: INFO: At 2020-12-01 23:43:42 +0000 UTC - event for pod-with-label-security-s1: {kubelet ip-10-0-140-191.ec2.internal} Pulled: Container image \"k8s.gcr.io/pause:3.1\" already present on machine Dec 1 23:44:15.183: INFO: At 2020-12-01 23:43:52 +0000 UTC - event for 0cc0d13d-342f-11eb-a9f6-0a58ac106ed2-0: {kubelet ip-10-0-140-191.ec2.internal} Created: Created container 0cc0d13d-342f-11eb-a9f6-0a58ac106ed2-0 Dec 1 23:44:15.184: INFO: At 2020-12-01 23:43:52 +0000 UTC - event for 0cc0d13d-342f-11eb-a9f6-0a58ac106ed2-0: {kubelet ip-10-0-140-191.ec2.internal} Started: Started container 0cc0d13d-342f-11eb-a9f6-0a58ac106ed2-0 Dec 1 23:44:15.184: INFO: At 2020-12-01 23:43:52 +0000 UTC - event for 0cc0d13d-342f-11eb-a9f6-0a58ac106ed2-0: {kubelet ip-10-0-140-191.ec2.internal} Pulled: Container image \"k8s.gcr.io/pause:3.1\" already present on machine Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:02 +0000 UTC - event for 12c294c9-342f-11eb-a9f6-0a58ac106ed2-0: {kubelet ip-10-0-142-186.ec2.internal} Started: Started container 12c294c9-342f-11eb-a9f6-0a58ac106ed2-0 Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:02 +0000 UTC - event for 12c294c9-342f-11eb-a9f6-0a58ac106ed2-0: {kubelet ip-10-0-142-186.ec2.internal} Created: Created container 12c294c9-342f-11eb-a9f6-0a58ac106ed2-0 Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:02 +0000 UTC - event for 12c294c9-342f-11eb-a9f6-0a58ac106ed2-0: {kubelet ip-10-0-142-186.ec2.internal} Pulled: Container image \"k8s.gcr.io/pause:3.1\" already present on machine Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:05 +0000 UTC - event for pod-with-pod-antiaffinity: {default-scheduler } Scheduled: Successfully assigned e2e-tests-sched-priority-6pg47/pod-with-pod-antiaffinity to ip-10-0-140-191.ec2.internal Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:12 +0000 UTC - event for pod-with-pod-antiaffinity: {kubelet ip-10-0-140-191.ec2.internal} Pulled: Container image \"k8s.gcr.io/pause:3.1\" already present on machine Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:13 +0000 UTC - event for pod-with-pod-antiaffinity: {kubelet ip-10-0-140-191.ec2.internal} Created: Created container pod-with-pod-antiaffinity Dec 1 23:44:15.184: INFO: At 2020-12-01 23:44:13 +0000 UTC - event for pod-with-pod-antiaffinity: {kubelet ip-10-0-140-191.ec2.internal} Started: Started container pod-with-pod-antiaffinity Dec 1 23:44:15.216: INFO: skipping dumping cluster info - cluster too large Dec 1 23:44:15.216: INFO: Waiting up to 3m0s for all (but 100) nodes to be ready STEP: Destroying namespace \"e2e-tests-sched-priority-6pg47\" for this suite. Dec 1 23:44:39.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Dec 1 23:44:40.933: INFO: namespace e2e-tests-sched-priority-6pg47 deletion completed in 25.698566242s [AfterEach] [sig-scheduling] SchedulerPriorities [Serial] /go/src/github.com/openshift/origin/_output/local/go/src/github.com/openshift/origin/vendor/k8s.io/kubernetes/test/e2e/scheduling/priorities.go:68 Dec 1 23:44:40.934: INFO: Running AfterSuite actions on all nodes Dec 1 23:44:40.934: INFO: Running AfterSuite actions on node 1 fail [k8s.io/kubernetes/test/e2e/scheduling/priorities.go:143]: Expected <string>: ip-10-0-140-191.ec2.internal not to equal <string>: ip-10-0-140-191.ec2.internal Dec 01 23:42:52.294 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (79 times) failed: (2m9s) 2020-12-01T23:44:40 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/22/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.1s) 2020-12-01T23:45:10 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/23/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m36s) 2020-12-01T23:46:46 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/24/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29s) 2020-12-01T23:47:15 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/25/79) \"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (24.6s) 2020-12-01T23:47:40 \"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/26/79) \"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (10.1s) 2020-12-01T23:47:50 \"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/27/79) \"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/apps/daemon_set.go:378]: Requires at least 2 nodes (not -1) skipped: (10.3s) 2020-12-01T23:48:00 \"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/28/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.2s) 2020-12-01T23:48:29 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/29/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-12-01T23:48:58 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/30/79) \"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m52s) 2020-12-01T23:50:50 \"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/31/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m5s) 2020-12-01T23:52:56 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/32/79) \"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (10s) 2020-12-01T23:53:06 \"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/33/79) \"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m13s) 2020-12-01T23:54:19 \"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/34/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-12-01T23:54:47 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/35/79) \"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m47s) 2020-12-01T23:56:35 \"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/36/79) \"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m54s) 2020-12-01T23:58:29 \"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/37/79) \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (19.6s) 2020-12-01T23:58:48 \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/38/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.6s) 2020-12-01T23:59:01 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/39/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.1s) 2020-12-01T23:59:30 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/40/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m34s) 2020-12-02T00:02:03 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/41/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (11.4s) 2020-12-02T00:02:15 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/42/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (11.4s) 2020-12-02T00:02:26 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/43/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-12-02T00:02:55 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/44/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (11.2s) 2020-12-02T00:03:06 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/45/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.1s) 2020-12-02T00:03:35 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/46/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m13s) 2020-12-02T00:05:48 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/47/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.8s) 2020-12-02T00:06:01 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/48/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m45s) 2020-12-02T00:07:46 \"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/49/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.3s) 2020-12-02T00:08:15 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/50/79) \"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/scheduling/taint_based_evictions.go:63]: Requires at least 2 nodes (not -1) skipped: (9.9s) 2020-12-02T00:08:25 \"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/51/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.8s) 2020-12-02T00:08:38 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/52/79) \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m27s) 2020-12-02T00:11:05 \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/53/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m41s) 2020-12-02T00:12:46 \"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/54/79) \"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (2m14s) 2020-12-02T00:15:00 \"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/55/79) \"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (9.9s) 2020-12-02T00:15:10 \"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/56/79) \"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:247]: Could not get controller-manager metrics - skipping skipped: (20.3s) 2020-12-02T00:15:30 \"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/57/79) \"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m45s) 2020-12-02T00:17:14 \"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/58/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m14s) 2020-12-02T00:18:28 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/59/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (36.6s) 2020-12-02T00:19:05 \"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/60/79) \"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (59.2s) 2020-12-02T00:20:04 \"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/61/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (42.6s) 2020-12-02T00:20:47 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/62/79) \"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\" passed: (55.7s) 2020-12-02T00:21:43 \"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\" started: (1/63/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (11.1s) 2020-12-02T00:21:54 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/64/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (28.8s) 2020-12-02T00:22:23 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/65/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (21.7s) 2020-12-02T00:22:44 \"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/66/79) \"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (10.1s) 2020-12-02T00:22:54 \"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/67/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m11s) 2020-12-02T00:24:05 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/68/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (26.7s) 2020-12-02T00:24:32 \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/69/79) \"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\" passed: (31.8s) 2020-12-02T00:25:04 \"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\" started: (1/70/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (58.2s) 2020-12-02T00:26:02 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/71/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.6s) 2020-12-02T00:26:14 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/72/79) \"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:81]: Environment does not support getting controller-manager metrics - skipping skipped: (10s) 2020-12-02T00:26:24 \"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/73/79) \"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (59.4s) 2020-12-02T00:27:24 \"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/74/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (12.2s) 2020-12-02T00:27:36 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/75/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.5s) 2020-12-02T00:27:49 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/76/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m43s) 2020-12-02T00:30:32 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/77/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (29.3s) 2020-12-02T00:31:01 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/78/79) \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m47s) 2020-12-02T00:32:48 \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/79/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (12.5s) 2020-12-02T00:33:01 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" Timeline: Dec 01 23:28:01.238 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-136-103.ec2.internal node/ip-10-0-136-103.ec2.internal created Dec 01 23:28:01.239 I ns/openshift-console pod/console-57f8778695-qgcjq node/ip-10-0-148-134.ec2.internal created Dec 01 23:28:04.988 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-136-103.ec2.internal Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c18ce2736a4fed8091de16f868bc46e6a9a03b7956c2418bad42fbb041ffbafb\" already present on machine Dec 01 23:28:05.167 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-136-103.ec2.internal Created container pruner Dec 01 23:28:05.205 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-136-103.ec2.internal Started container pruner Dec 01 23:32:46.624 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (48 times) Dec 01 23:34:01.505 W persistentvolume/pvc-9769da37-342d-11eb-8d36-0afa806ccb57 Error deleting EBS volume \"vol-0c8c55087bd2498e5\" since volume is currently attached to \"i-009bfdb4b8165959d\" Dec 01 23:37:21.752 W ns/openshift-machine-config-operator pod/machine-config-daemon-bpw7t node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 01 23:37:21.753 W ns/openshift-image-registry pod/node-ca-k86sx node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.762 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-k86sx Dec 01 23:37:21.763 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf Marking for deletion Pod openshift-marketplace/redhat-operators-6448849fdd-txhwf Dec 01 23:37:21.771 W ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.772 I ns/openshift-machine-config-operator pod/machine-config-daemon-bpw7t Stopping container machine-config-daemon Dec 01 23:37:21.772 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-bpw7t Dec 01 23:37:21.772 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-lnbfq Marking for deletion Pod openshift-monitoring/prometheus-adapter-5448dfb8fb-lnbfq Dec 01 23:37:21.779 W ns/openshift-ingress pod/router-default-79b887b8c9-567nl node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.780 W ns/openshift-monitoring pod/alertmanager-main-1 node/ip-10-0-140-191.ec2.internal graceful deletion within 0s Dec 01 23:37:21.781 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.782 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.785 W ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-lnbfq node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.785 W ns/openshift-monitoring pod/grafana-649f787944-qr6mr node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:37:21.793 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 01 23:37:21.795 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq Marking for deletion Pod openshift-image-registry/image-registry-7dd97dddc5-5pgkq Dec 01 23:37:21.795 I ns/openshift-image-registry pod/node-ca-k86sx Stopping container node-ca Dec 01 23:37:21.798 W ns/openshift-monitoring pod/alertmanager-main-1 node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:21.804 I ns/openshift-monitoring pod/alertmanager-main-1 Marking for deletion Pod openshift-monitoring/alertmanager-main-1 Dec 01 23:37:21.818 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq Stopping container registry Dec 01 23:37:21.824 I ns/openshift-ingress pod/router-default-79b887b8c9-567nl Stopping container router Dec 01 23:37:21.831 I ns/openshift-ingress pod/router-default-79b887b8c9-567nl Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-567nl Dec 01 23:37:21.832 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager-proxy Dec 01 23:37:21.840 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container config-reloader Dec 01 23:37:21.847 I ns/openshift-monitoring pod/grafana-649f787944-qr6mr Marking for deletion Pod openshift-monitoring/grafana-649f787944-qr6mr Dec 01 23:37:21.848 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf Stopping container redhat-operators Dec 01 23:37:21.860 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager Dec 01 23:37:21.863 I ns/openshift-machine-config-operator pod/machine-config-daemon-bpw7t Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-bpw7t Dec 01 23:37:21.865 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 Stopping container kube-state-metrics Dec 01 23:37:21.872 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-rmdg7 node/ created Dec 01 23:37:21.874 I ns/openshift-image-registry pod/node-ca-k86sx Marking for deletion Pod openshift-image-registry/node-ca-k86sx Dec 01 23:37:21.874 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 Stopping container kube-rbac-proxy-self Dec 01 23:37:21.880 I ns/openshift-image-registry replicaset/image-registry-7dd97dddc5 Created pod: image-registry-7dd97dddc5-rmdg7 Dec 01 23:37:21.884 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 Marking for deletion Pod openshift-monitoring/kube-state-metrics-7b4d49f7bd-jr977 Dec 01 23:37:21.887 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-rmdg7 Successfully assigned openshift-image-registry/image-registry-7dd97dddc5-rmdg7 to ip-10-0-142-186.ec2.internal Dec 01 23:37:21.890 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 node/ created Dec 01 23:37:21.892 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-kj62f node/ created Dec 01 23:37:21.903 I ns/openshift-monitoring pod/prometheus-k8s-0 Marking for deletion Pod openshift-monitoring/prometheus-k8s-0 Dec 01 23:37:21.904 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 node/ created Dec 01 23:37:21.908 I ns/openshift-marketplace replicaset/redhat-operators-6448849fdd Created pod: redhat-operators-6448849fdd-v4kx7 Dec 01 23:37:21.918 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. Dec 01 23:37:21.924 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-kj62f Successfully assigned openshift-monitoring/prometheus-adapter-5448dfb8fb-kj62f to ip-10-0-142-186.ec2.internal Dec 01 23:37:21.932 I ns/openshift-monitoring replicaset/prometheus-adapter-5448dfb8fb Created pod: prometheus-adapter-5448dfb8fb-kj62f Dec 01 23:37:21.932 I ns/openshift-monitoring pod/alertmanager-main-1 Cancelling deletion of Pod openshift-monitoring/alertmanager-main-1 Dec 01 23:37:21.938 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 Successfully assigned openshift-marketplace/redhat-operators-6448849fdd-v4kx7 to ip-10-0-142-186.ec2.internal Dec 01 23:37:21.944 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-62gn2 Dec 01 23:37:21.949 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq node/ created Dec 01 23:37:21.960 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (2 times) Dec 01 23:37:21.972 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 Stopping container kube-rbac-proxy-main Dec 01 23:37:21.975 I ns/openshift-monitoring replicaset/kube-state-metrics-7b4d49f7bd Created pod: kube-state-metrics-7b4d49f7bd-m4jmq Dec 01 23:37:21.985 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (3 times) Dec 01 23:37:22.005 I ns/openshift-monitoring pod/alertmanager-main-1 node/ created Dec 01 23:37:22.013 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (4 times) Dec 01 23:37:22.016 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s node/ created Dec 01 23:37:22.022 I ns/openshift-monitoring statefulset/alertmanager-main create Pod alertmanager-main-1 in StatefulSet alertmanager-main successful Dec 01 23:37:22.022 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Successfully assigned openshift-monitoring/kube-state-metrics-7b4d49f7bd-m4jmq to ip-10-0-142-186.ec2.internal Dec 01 23:37:22.031 I ns/openshift-monitoring replicaset/grafana-649f787944 Created pod: grafana-649f787944-pfq4s Dec 01 23:37:22.038 I ns/openshift-monitoring pod/alertmanager-main-1 Successfully assigned openshift-monitoring/alertmanager-main-1 to ip-10-0-142-186.ec2.internal Dec 01 23:37:22.049 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Successfully assigned openshift-monitoring/grafana-649f787944-pfq4s to ip-10-0-142-186.ec2.internal Dec 01 23:37:22.083 W clusteroperator/image-registry changed Available to False: NoReplicasAvailable: The deployment does not have available replicas Dec 01 23:37:22.083 W clusteroperator/image-registry changed Progressing to True: DeploymentNotCompleted: The deployment has not completed Dec 01 23:37:22.168 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-lnbfq Stopping container prometheus-adapter Dec 01 23:37:22.194 W clusteroperator/image-registry changed Available to True: Ready: The registry is ready Dec 01 23:37:22.194 W clusteroperator/image-registry changed Progressing to False: Ready: The registry is ready Dec 01 23:37:22.205 W clusteroperator/image-registry changed Available to False: NoReplicasAvailable: The deployment does not have available replicas Dec 01 23:37:22.205 W clusteroperator/image-registry changed Progressing to True: DeploymentNotCompleted: The deployment has not completed Dec 01 23:37:22.374 I ns/openshift-monitoring pod/grafana-649f787944-qr6mr Stopping container grafana Dec 01 23:37:22.455 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 0/5 nodes are available: 2 node(s) didn\\'t match pod affinity/anti-affinity, 2 node(s) didn\\'t satisfy existing pods anti-affinity rules, 3 node(s) didn\\'t match node selector. Dec 01 23:37:22.568 I ns/openshift-monitoring pod/grafana-649f787944-qr6mr Stopping container grafana-proxy Dec 01 23:37:22.773 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container prometheus Dec 01 23:37:22.968 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container rules-configmap-reloader Dec 01 23:37:23.169 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container prom-label-proxy Dec 01 23:37:23.368 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager (2 times) Dec 01 23:37:23.568 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container alertmanager-proxy (2 times) Dec 01 23:37:23.771 I ns/openshift-monitoring pod/alertmanager-main-1 Stopping container config-reloader (2 times) Dec 01 23:37:23.969 W ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:23.969 W ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq node/ip-10-0-140-191.ec2.internal container=registry container stopped being ready Dec 01 23:37:24.371 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:24.371 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf node/ip-10-0-140-191.ec2.internal container=redhat-operators container stopped being ready Dec 01 23:37:25.169 W ns/openshift-ingress pod/router-default-79b887b8c9-567nl node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:25.169 W ns/openshift-ingress pod/router-default-79b887b8c9-567nl node/ip-10-0-140-191.ec2.internal container=router container stopped being ready Dec 01 23:37:25.571 E ns/openshift-ingress pod/router-default-79b887b8c9-567nl node/ip-10-0-140-191.ec2.internal container=router container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:37:25.776 W ns/openshift-ingress pod/router-default-79b887b8c9-567nl node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:25.786 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 Successfully assigned openshift-ingress/router-default-79b887b8c9-62gn2 to ip-10-0-140-191.ec2.internal Dec 01 23:37:26.169 W ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-lnbfq node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:26.169 W ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-lnbfq node/ip-10-0-140-191.ec2.internal container=prometheus-adapter container stopped being ready Dec 01 23:37:26.975 W ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-lnbfq node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:27.976 W ns/openshift-image-registry pod/node-ca-k86sx node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:28.015 I ns/openshift-image-registry pod/node-ca-qpb24 node/ created Dec 01 23:37:28.021 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-qpb24 Dec 01 23:37:28.027 I ns/openshift-image-registry pod/node-ca-qpb24 Successfully assigned openshift-image-registry/node-ca-qpb24 to ip-10-0-140-191.ec2.internal Dec 01 23:37:28.969 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:28.969 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 node/ip-10-0-140-191.ec2.internal container=kube-state-metrics container stopped being ready Dec 01 23:37:28.969 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 node/ip-10-0-140-191.ec2.internal container=kube-rbac-proxy-self container stopped being ready Dec 01 23:37:28.969 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 node/ip-10-0-140-191.ec2.internal container=kube-rbac-proxy-main container stopped being ready Dec 01 23:37:29.378 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-jr977 node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prom-label-proxy container stopped being ready Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus container stopped being ready Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus-config-reloader container stopped being ready Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=rules-configmap-reloader container stopped being ready Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=kube-rbac-proxy container stopped being ready Dec 01 23:37:29.772 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus-proxy container stopped being ready Dec 01 23:37:29.976 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:30.057 I ns/openshift-monitoring pod/prometheus-k8s-0 node/ created Dec 01 23:37:30.070 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-0 in StatefulSet prometheus-k8s successful Dec 01 23:37:30.073 I ns/openshift-monitoring pod/prometheus-k8s-0 Successfully assigned openshift-monitoring/prometheus-k8s-0 to ip-10-0-140-191.ec2.internal Dec 01 23:37:31.046 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf node/ip-10-0-140-191.ec2.internal pod has been pending longer than a minute Dec 01 23:37:31.046 W ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq node/ip-10-0-140-191.ec2.internal pod has been pending longer than a minute Dec 01 23:37:32.571 W ns/openshift-monitoring pod/grafana-649f787944-qr6mr node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:37:32.571 W ns/openshift-monitoring pod/grafana-649f787944-qr6mr node/ip-10-0-140-191.ec2.internal container=grafana container stopped being ready Dec 01 23:37:32.571 W ns/openshift-monitoring pod/grafana-649f787944-qr6mr node/ip-10-0-140-191.ec2.internal container=grafana-proxy container stopped being ready Dec 01 23:37:32.986 W ns/openshift-monitoring pod/grafana-649f787944-qr6mr node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:33.976 W ns/openshift-machine-config-operator pod/machine-config-daemon-bpw7t node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:33.990 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh node/ created Dec 01 23:37:33.998 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-hs2xh Dec 01 23:37:34.001 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh Successfully assigned openshift-machine-config-operator/machine-config-daemon-hs2xh to ip-10-0-140-191.ec2.internal Dec 01 23:37:34.890 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" already present on machine Dec 01 23:37:35.373 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-txhwf node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:35.748 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 Created container router Dec 01 23:37:35.772 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 Started container router Dec 01 23:37:36.122 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 01 23:37:36.268 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh Created container machine-config-daemon Dec 01 23:37:36.293 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh Started container machine-config-daemon Dec 01 23:37:36.309 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-rmdg7 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 01 23:37:36.377 W ns/openshift-image-registry pod/image-registry-7dd97dddc5-5pgkq node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:37:36.465 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-rmdg7 Created container registry Dec 01 23:37:36.491 I ns/openshift-image-registry pod/image-registry-7dd97dddc5-rmdg7 Started container registry Dec 01 23:37:36.577 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn (8 times) Dec 01 23:37:36.731 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:69e43ce2394b34a984aca4c30461087dcb4cef9eb6c9a61467ac192d11782ca6\" already present on machine Dec 01 23:37:36.743 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm (8 times) Dec 01 23:37:36.825 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Dec 01 23:37:36.934 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-kj62f Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\" already present on machine Dec 01 23:37:37.060 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 Created container redhat-operators Dec 01 23:37:37.109 I ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 Started container redhat-operators Dec 01 23:37:37.174 I ns/openshift-image-registry pod/node-ca-qpb24 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 01 23:37:37.196 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Created container kube-rbac-proxy-main Dec 01 23:37:37.207 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-kj62f Created container prometheus-adapter Dec 01 23:37:37.264 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Started container kube-rbac-proxy-main Dec 01 23:37:37.270 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Dec 01 23:37:37.309 I ns/openshift-image-registry pod/node-ca-qpb24 Created container node-ca Dec 01 23:37:37.336 I ns/openshift-image-registry pod/node-ca-qpb24 Started container node-ca Dec 01 23:37:37.355 I ns/openshift-monitoring pod/prometheus-adapter-5448dfb8fb-kj62f Started container prometheus-adapter Dec 01 23:37:37.391 I ns/openshift-monitoring pod/alertmanager-main-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b9d07e8eed67a34b50a3afaa36cd4b9f052697894e64b5c203d0675d12ab5ec\" already present on machine Dec 01 23:37:37.602 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Created container kube-rbac-proxy-self Dec 01 23:37:37.614 I ns/openshift-monitoring pod/alertmanager-main-1 Created container alertmanager Dec 01 23:37:37.709 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Started container kube-rbac-proxy-self Dec 01 23:37:37.909 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c12c8ce2ac222f2205c8ab24dffef3ef34e3505c7a52d849b637ce3a7d91a388\" Dec 01 23:37:37.919 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-0 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-0 (5 times) Dec 01 23:37:38.109 I ns/openshift-monitoring pod/alertmanager-main-1 Started container alertmanager Dec 01 23:37:38.309 I ns/openshift-monitoring pod/alertmanager-main-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Dec 01 23:37:38.514 I ns/openshift-monitoring pod/alertmanager-main-1 Created container config-reloader Dec 01 23:37:38.709 I ns/openshift-monitoring pod/alertmanager-main-1 Started container config-reloader Dec 01 23:37:38.909 I ns/openshift-monitoring pod/alertmanager-main-1 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Dec 01 23:37:38.919 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-1 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-1 (5 times) Dec 01 23:37:39.109 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c229ac6674c217359c243e4621aae565c2af46caaf231a077b7f7e3d6f07ef5a\" Dec 01 23:37:39.134 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (63 times) Dec 01 23:37:39.311 I ns/openshift-monitoring pod/alertmanager-main-1 Created container alertmanager-proxy Dec 01 23:37:39.509 I ns/openshift-monitoring pod/alertmanager-main-1 Started container alertmanager-proxy Dec 01 23:37:39.719 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c12c8ce2ac222f2205c8ab24dffef3ef34e3505c7a52d849b637ce3a7d91a388\" Dec 01 23:37:39.910 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Created container kube-state-metrics Dec 01 23:37:39.982 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine Dec 01 23:37:40.070 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-2 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-2 (5 times) Dec 01 23:37:40.108 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m4jmq Started container kube-state-metrics Dec 01 23:37:40.137 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus Dec 01 23:37:40.168 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus Dec 01 23:37:40.175 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\" already present on machine Dec 01 23:37:40.336 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus-config-reloader Dec 01 23:37:40.360 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus-config-reloader Dec 01 23:37:40.366 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Dec 01 23:37:40.513 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus-proxy Dec 01 23:37:40.545 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus-proxy Dec 01 23:37:40.551 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Dec 01 23:37:40.757 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container kube-rbac-proxy Dec 01 23:37:40.790 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container kube-rbac-proxy Dec 01 23:37:40.796 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\" already present on machine Dec 01 23:37:40.954 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prom-label-proxy Dec 01 23:37:40.981 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prom-label-proxy Dec 01 23:37:41.182 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Dec 01 23:37:41.382 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container rules-configmap-reloader Dec 01 23:37:41.582 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container rules-configmap-reloader Dec 01 23:37:41.976 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine (2 times) Dec 01 23:37:42.157 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus (2 times) Dec 01 23:37:42.183 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus (2 times) Dec 01 23:37:43.890 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c229ac6674c217359c243e4621aae565c2af46caaf231a077b7f7e3d6f07ef5a\" Dec 01 23:37:44.114 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Created container grafana Dec 01 23:37:44.151 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Started container grafana Dec 01 23:37:44.157 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Dec 01 23:37:44.354 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Created container grafana-proxy Dec 01 23:37:44.395 I ns/openshift-monitoring pod/grafana-649f787944-pfq4s Started container grafana-proxy Dec 01 23:37:45.835 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 Liveness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ Dec 01 23:37:46.000 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus container exited with code 1 (Error): Dec 01 23:37:46.390 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus container restarted Dec 01 23:37:48.623 W ns/openshift-marketplace pod/redhat-operators-6448849fdd-v4kx7 Readiness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ Dec 01 23:37:48.719 W clusteroperator/image-registry changed Available to True: Ready: The registry is ready Dec 01 23:37:48.719 W clusteroperator/image-registry changed Progressing to False: Ready: The registry is ready Dec 01 23:38:33.395 W ns/openshift-image-registry pod/node-ca-qpb24 node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:38:33.395 W ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 01 23:38:33.400 I ns/openshift-image-registry pod/node-ca-qpb24 Stopping container node-ca Dec 01 23:38:33.407 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh Stopping container machine-config-daemon Dec 01 23:38:33.411 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-qpb24 Dec 01 23:38:33.412 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-62gn2 Dec 01 23:38:33.416 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-hs2xh Dec 01 23:38:33.421 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 01 23:38:33.424 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 01 23:38:33.429 I ns/openshift-monitoring pod/prometheus-k8s-0 Marking for deletion Pod openshift-monitoring/prometheus-k8s-0 (2 times) Dec 01 23:38:33.434 I ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-hs2xh Dec 01 23:38:33.441 I ns/openshift-image-registry pod/node-ca-qpb24 Marking for deletion Pod openshift-image-registry/node-ca-qpb24 Dec 01 23:38:33.443 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container prom-label-proxy Dec 01 23:38:33.451 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container prometheus Dec 01 23:38:33.457 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container rules-configmap-reloader Dec 01 23:38:33.467 I ns/openshift-monitoring pod/prometheus-k8s-0 Stopping container prometheus-proxy Dec 01 23:38:33.473 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ created Dec 01 23:38:33.474 I ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 Stopping container router Dec 01 23:38:33.484 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-9xqjr Dec 01 23:38:33.491 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. Dec 01 23:38:33.513 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (2 times) Dec 01 23:38:34.882 W ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:38:34.882 W ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh node/ip-10-0-140-191.ec2.internal container=machine-config-daemon container stopped being ready Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus container stopped being ready Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus-config-reloader container stopped being ready Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus-proxy container stopped being ready Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=rules-configmap-reloader container stopped being ready Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=kube-rbac-proxy container stopped being ready Dec 01 23:38:34.897 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prom-label-proxy container stopped being ready Dec 01 23:38:35.095 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:38:35.095 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:38:35.095 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=rules-configmap-reloader container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:38:35.095 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prometheus-config-reloader container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:38:35.095 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=kube-rbac-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:38:35.095 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal container=prom-label-proxy container exited with code 137 (ContainerStatusUnknown): The container could not be located when the pod was terminated Dec 01 23:38:35.493 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:38:35.493 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 node/ip-10-0-140-191.ec2.internal container=router container stopped being ready Dec 01 23:38:35.899 W ns/openshift-ingress pod/router-default-79b887b8c9-62gn2 node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:38:35.910 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (3 times) Dec 01 23:38:36.292 W ns/openshift-image-registry pod/node-ca-qpb24 node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 01 23:38:36.292 W ns/openshift-image-registry pod/node-ca-qpb24 node/ip-10-0-140-191.ec2.internal container=node-ca container stopped being ready Dec 01 23:38:36.496 W ns/openshift-image-registry pod/node-ca-qpb24 node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:38:36.508 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (4 times) Dec 01 23:38:41.267 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:38:41.269 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (5 times) Dec 01 23:38:41.279 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (6 times) Dec 01 23:38:41.284 W ns/openshift-machine-config-operator pod/machine-config-daemon-hs2xh node/ip-10-0-140-191.ec2.internal deleted Dec 01 23:38:41.293 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (7 times) Dec 01 23:38:41.355 I ns/openshift-monitoring pod/prometheus-k8s-0 node/ created Dec 01 23:38:41.367 I ns/openshift-monitoring pod/prometheus-k8s-0 Successfully assigned openshift-monitoring/prometheus-k8s-0 to ip-10-0-142-186.ec2.internal Dec 01 23:38:41.368 I ns/openshift-monitoring statefulset/prometheus-k8s create Pod prometheus-k8s-0 in StatefulSet prometheus-k8s successful (2 times) Dec 01 23:38:49.733 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine Dec 01 23:38:49.910 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus Dec 01 23:38:49.937 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus Dec 01 23:38:49.944 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f37abe7431ced72e19d6ccae96af879f04ff921abcd87ee21930c40c93c54b9\" already present on machine Dec 01 23:38:50.133 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus-config-reloader Dec 01 23:38:50.168 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus-config-reloader Dec 01 23:38:50.174 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Dec 01 23:38:50.471 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus-proxy Dec 01 23:38:50.508 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus-proxy Dec 01 23:38:50.513 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Dec 01 23:38:50.698 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container kube-rbac-proxy Dec 01 23:38:50.725 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container kube-rbac-proxy Dec 01 23:38:50.732 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dfdd51896ecacbf76452dac07cb49fe693bf27b0bac000352b22747785b12bc2\" already present on machine Dec 01 23:38:50.907 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prom-label-proxy Dec 01 23:38:50.934 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prom-label-proxy Dec 01 23:38:50.940 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Dec 01 23:38:51.133 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container rules-configmap-reloader Dec 01 23:38:51.333 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container rules-configmap-reloader Dec 01 23:38:51.834 I ns/openshift-monitoring pod/prometheus-k8s-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2f7261bdec091e5acff4f4d60a155da567e8744958f059e24d55aff6c5e67be1\" already present on machine (2 times) Dec 01 23:38:51.835 E ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-142-186.ec2.internal container=prometheus container exited with code 1 (Error): Dec 01 23:38:52.002 I ns/openshift-monitoring pod/prometheus-k8s-0 Created container prometheus (2 times) Dec 01 23:38:52.041 I ns/openshift-monitoring pod/prometheus-k8s-0 Started container prometheus (2 times) Dec 01 23:38:53.843 W ns/openshift-monitoring pod/prometheus-k8s-0 node/ip-10-0-142-186.ec2.internal container=prometheus container restarted Dec 01 23:39:46.046 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ pod has been pending longer than a minute Dec 01 23:39:51.261 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (8 times) Dec 01 23:39:51.304 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr Successfully assigned openshift-ingress/router-default-79b887b8c9-9xqjr to ip-10-0-140-191.ec2.internal Dec 01 23:39:51.319 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj node/ created Dec 01 23:39:51.327 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-q5dkj Dec 01 23:39:51.329 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj Successfully assigned openshift-machine-config-operator/machine-config-daemon-q5dkj to ip-10-0-140-191.ec2.internal Dec 01 23:39:51.339 I ns/openshift-image-registry pod/node-ca-xlfl5 node/ created Dec 01 23:39:51.344 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-xlfl5 Dec 01 23:39:51.350 I ns/openshift-image-registry pod/node-ca-xlfl5 Successfully assigned openshift-image-registry/node-ca-xlfl5 to ip-10-0-140-191.ec2.internal Dec 01 23:39:53.354 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 01 23:39:53.474 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj Created container machine-config-daemon Dec 01 23:39:53.510 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj Started container machine-config-daemon Dec 01 23:40:00.175 I ns/openshift-image-registry pod/node-ca-xlfl5 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 01 23:40:00.302 I ns/openshift-image-registry pod/node-ca-xlfl5 Created container node-ca Dec 01 23:40:00.331 I ns/openshift-image-registry pod/node-ca-xlfl5 Started container node-ca Dec 01 23:40:00.343 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" already present on machine Dec 01 23:40:00.505 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr Created container router Dec 01 23:40:00.530 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr Started container router Dec 01 23:40:01.046 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ip-10-0-140-191.ec2.internal pod has been pending longer than a minute Dec 01 23:42:31.730 - 129s I test=\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" running Dec 01 23:42:52.294 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (79 times) Dec 01 23:44:40.950 I test=\"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" failed Dec 01 23:47:35.078 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (93 times) Dec 01 23:47:37.655 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-2 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-2 (6 times) Dec 01 23:47:37.812 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn (9 times) Dec 01 23:47:37.993 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm (9 times) Dec 01 23:47:39.291 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-0 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-0 (6 times) Dec 01 23:47:40.591 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-1 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-1 (6 times) Dec 01 23:52:38.436 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (109 times) Dec 01 23:57:37.750 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-1 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-1 (7 times) Dec 01 23:57:37.956 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (124 times) Dec 01 23:57:39.198 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-2 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-2 (7 times) Dec 01 23:57:39.447 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn (10 times) Dec 01 23:57:39.687 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm (10 times) Dec 01 23:57:41.247 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-0 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-0 (7 times) Dec 02 00:00:33.074 W ns/openshift-image-registry pod/node-ca-xlfl5 node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:00:33.075 W ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 02 00:00:33.080 I ns/openshift-image-registry pod/node-ca-xlfl5 Stopping container node-ca Dec 02 00:00:33.086 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-xlfl5 Dec 02 00:00:33.088 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-q5dkj Dec 02 00:00:33.089 I ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj Stopping container machine-config-daemon Dec 02 00:00:33.093 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:00:33.096 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-q5dkj Dec 02 00:00:33.096 I ns/openshift-image-registry pod/node-ca-xlfl5 Marking for deletion Pod openshift-image-registry/node-ca-xlfl5 Dec 02 00:00:33.104 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr Stopping container router Dec 02 00:00:33.107 I ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-9xqjr Dec 02 00:00:33.136 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x node/ created Dec 02 00:00:33.156 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-8j89x Dec 02 00:00:33.156 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. Dec 02 00:00:33.196 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (2 times) Dec 02 00:00:34.656 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 02 00:00:34.656 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ip-10-0-140-191.ec2.internal container=router container stopped being ready Dec 02 00:00:34.667 W ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 02 00:00:34.667 W ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj node/ip-10-0-140-191.ec2.internal container=machine-config-daemon container stopped being ready Dec 02 00:00:34.681 W ns/openshift-image-registry pod/node-ca-xlfl5 node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 02 00:00:34.681 W ns/openshift-image-registry pod/node-ca-xlfl5 node/ip-10-0-140-191.ec2.internal container=node-ca container stopped being ready Dec 02 00:00:35.670 W ns/openshift-image-registry pod/node-ca-xlfl5 node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:00:35.682 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (3 times) Dec 02 00:00:35.695 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (4 times) Dec 02 00:00:35.705 W ns/openshift-ingress pod/router-default-79b887b8c9-9xqjr node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:00:35.715 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (5 times) Dec 02 00:00:41.251 W ns/openshift-machine-config-operator pod/machine-config-daemon-q5dkj node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:00:41.265 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (6 times) Dec 02 00:00:41.273 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (7 times) Dec 02 00:01:38.129 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x Successfully assigned openshift-ingress/router-default-79b887b8c9-8j89x to ip-10-0-140-191.ec2.internal Dec 02 00:01:38.133 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq node/ created Dec 02 00:01:38.143 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq Successfully assigned openshift-machine-config-operator/machine-config-daemon-64zgq to ip-10-0-140-191.ec2.internal Dec 02 00:01:38.143 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-64zgq Dec 02 00:01:38.159 I ns/openshift-image-registry pod/node-ca-gk2lg node/ created Dec 02 00:01:38.169 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-gk2lg Dec 02 00:01:38.172 I ns/openshift-image-registry pod/node-ca-gk2lg Successfully assigned openshift-image-registry/node-ca-gk2lg to ip-10-0-140-191.ec2.internal Dec 02 00:01:39.494 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 02 00:01:39.636 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq Created container machine-config-daemon Dec 02 00:01:39.652 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq Started container machine-config-daemon Dec 02 00:01:46.046 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x node/ip-10-0-140-191.ec2.internal pod has been pending longer than a minute Dec 02 00:01:46.485 I ns/openshift-image-registry pod/node-ca-gk2lg Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 02 00:01:46.598 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" already present on machine Dec 02 00:01:46.628 I ns/openshift-image-registry pod/node-ca-gk2lg Created container node-ca Dec 02 00:01:46.663 I ns/openshift-image-registry pod/node-ca-gk2lg Started container node-ca Dec 02 00:01:46.781 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x Created container router Dec 02 00:01:46.809 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x Started container router Dec 02 00:02:45.194 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (140 times) Dec 02 00:07:37.820 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-2 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-2 (8 times) Dec 02 00:07:37.994 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn (11 times) Dec 02 00:07:38.145 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm (11 times) Dec 02 00:07:39.307 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-0 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-0 (8 times) Dec 02 00:07:40.526 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-1 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-1 (8 times) Dec 02 00:07:40.670 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (155 times) Dec 02 00:09:40.824 W ns/openshift-image-registry pod/node-ca-gk2lg node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:09:40.824 W ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 02 00:09:40.828 I ns/openshift-image-registry pod/node-ca-gk2lg Stopping container node-ca Dec 02 00:09:40.833 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-gk2lg Dec 02 00:09:40.836 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq Stopping container machine-config-daemon Dec 02 00:09:40.836 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-8j89x Dec 02 00:09:40.841 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:09:40.841 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-64zgq Dec 02 00:09:40.848 I ns/openshift-ingress pod/router-default-79b887b8c9-8j89x Stopping container router Dec 02 00:09:40.851 I ns/openshift-image-registry pod/node-ca-gk2lg Marking for deletion Pod openshift-image-registry/node-ca-gk2lg Dec 02 00:09:40.856 I ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-64zgq Dec 02 00:09:40.903 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn node/ created Dec 02 00:09:40.911 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-4ghjn Dec 02 00:09:40.920 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. Dec 02 00:09:40.941 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (2 times) Dec 02 00:09:41.555 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (3 times) Dec 02 00:09:42.345 E ns/openshift-ingress pod/router-default-79b887b8c9-8j89x node/ip-10-0-140-191.ec2.internal container=router container exited with code 2 (Error): I1202 00:01:46.839651 1 template.go:299] Starting template router (v4.1.46-202005230309)\\ I1202 00:01:46.842084 1 metrics.go:147] Router health and metrics port listening at 0.0.0.0:1936 on HTTP and HTTPS\\ E1202 00:01:46.849070 1 haproxy.go:392] can\\'t scrape HAProxy: dial unix /var/lib/haproxy/run/haproxy.sock: connect: no such file or directory\\ I1202 00:01:46.873381 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:01:46.873408 1 router.go:255] Router is including routes in all namespaces\\ I1202 00:01:47.112459 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:01:57.637461 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:02:02.634040 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ W1202 00:09:14.955570 1 reflector.go:341] github.com/openshift/router/pkg/router/controller/factory/factory.go:112: watch of *v1.Route ended with: The resourceVersion for the provided watch is too old.\\ Dec 02 00:09:43.748 W ns/openshift-machine-config-operator pod/machine-config-daemon-64zgq node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:09:43.757 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (4 times) Dec 02 00:09:44.353 W ns/openshift-image-registry pod/node-ca-gk2lg node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:09:44.364 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (5 times) Dec 02 00:09:44.958 W ns/openshift-ingress pod/router-default-79b887b8c9-8j89x node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:09:44.960 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (6 times) Dec 02 00:10:45.879 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn Successfully assigned openshift-ingress/router-default-79b887b8c9-4ghjn to ip-10-0-140-191.ec2.internal Dec 02 00:10:45.888 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw node/ created Dec 02 00:10:45.897 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-ghvjw Dec 02 00:10:45.902 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw Successfully assigned openshift-machine-config-operator/machine-config-daemon-ghvjw to ip-10-0-140-191.ec2.internal Dec 02 00:10:45.914 I ns/openshift-image-registry pod/node-ca-86jzh node/ created Dec 02 00:10:45.922 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-86jzh Dec 02 00:10:45.925 I ns/openshift-image-registry pod/node-ca-86jzh Successfully assigned openshift-image-registry/node-ca-86jzh to ip-10-0-140-191.ec2.internal Dec 02 00:10:46.046 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn node/ip-10-0-140-191.ec2.internal pod has been pending longer than a minute Dec 02 00:10:47.029 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 02 00:10:47.182 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw Created container machine-config-daemon Dec 02 00:10:47.215 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw Started container machine-config-daemon Dec 02 00:10:54.260 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" already present on machine Dec 02 00:10:54.426 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn Created container router Dec 02 00:10:54.456 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn Started container router Dec 02 00:10:54.927 I ns/openshift-image-registry pod/node-ca-86jzh Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 02 00:10:55.136 I ns/openshift-image-registry pod/node-ca-86jzh Created container node-ca Dec 02 00:10:55.168 I ns/openshift-image-registry pod/node-ca-86jzh Started container node-ca Dec 02 00:12:18.351 I ns/kube-system pod/critical-pod node/ created Dec 02 00:12:18.359 W ns/kube-system pod/critical-pod 0/5 nodes are available: 1 Insufficient cpu, 2 Insufficient memory, 3 node(s) had taints that the pod didn\\'t tolerate. Dec 02 00:12:18.384 W ns/kube-system pod/critical-pod 0/5 nodes are available: 1 Insufficient cpu, 2 Insufficient memory, 3 node(s) had taints that the pod didn\\'t tolerate. (2 times) Dec 02 00:12:21.260 I ns/kube-system pod/critical-pod Successfully assigned kube-system/critical-pod to ip-10-0-140-191.ec2.internal Dec 02 00:12:29.408 I ns/kube-system pod/critical-pod Container image \"k8s.gcr.io/pause:3.1\" already present on machine Dec 02 00:12:29.564 I ns/kube-system pod/critical-pod Created container critical-pod Dec 02 00:12:29.587 I ns/kube-system pod/critical-pod Started container critical-pod Dec 02 00:12:30.463 W ns/kube-system pod/critical-pod node/ip-10-0-140-191.ec2.internal graceful deletion within 0s Dec 02 00:12:30.472 W ns/kube-system pod/critical-pod node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:12:30.481 I ns/kube-system pod/critical-pod Stopping container critical-pod Dec 02 00:12:30.976 I ns/kube-system pod/critical-pod Pod sandbox changed, it will be killed and re-created. Dec 02 00:12:40.662 W ns/kube-system pod/critical-pod Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_critical-pod_kube-system_0a28c3c8-3433-11eb-8d36-0afa806ccb57_1(fb139e2781023c573ee27f21aabb092c36cbace677387e64dc57426d89baf264): Multus: Err adding pod to network \"openshift-sdn\": cannot set \"openshift-sdn\" ifname to \"eth0\": no netns: failed to Statfs \"/proc/97569/ns/net\": no such file or directory Dec 02 00:12:51.675 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (171 times) Dec 02 00:17:34.686 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (185 times) Dec 02 00:17:37.752 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-2 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-2 (9 times) Dec 02 00:17:37.930 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn (12 times) Dec 02 00:17:38.083 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm (12 times) Dec 02 00:17:39.254 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-0 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-0 (9 times) Dec 02 00:17:40.311 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-1 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-1 (9 times) Dec 02 00:18:17.550 W ns/openshift-image-registry pod/node-ca-86jzh node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:18:17.550 W ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 02 00:18:17.557 I ns/openshift-image-registry pod/node-ca-86jzh Stopping container node-ca Dec 02 00:18:17.559 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-ghvjw Dec 02 00:18:17.568 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-86jzh Dec 02 00:18:17.569 I ns/openshift-image-registry pod/node-ca-86jzh Marking for deletion Pod openshift-image-registry/node-ca-86jzh Dec 02 00:18:17.574 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:18:17.575 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-4ghjn Dec 02 00:18:17.583 I ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn Stopping container router Dec 02 00:18:17.585 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-ghvjw Dec 02 00:18:17.617 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh node/ created Dec 02 00:18:17.623 W ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. Dec 02 00:18:17.625 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-mgmrh Dec 02 00:18:17.635 I ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw Stopping container machine-config-daemon Dec 02 00:18:17.648 W ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (2 times) Dec 02 00:18:19.439 E ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn node/ip-10-0-140-191.ec2.internal container=router container exited with code 2 (Error): I1202 00:10:54.474576 1 template.go:299] Starting template router (v4.1.46-202005230309)\\ I1202 00:10:54.477889 1 metrics.go:147] Router health and metrics port listening at 0.0.0.0:1936 on HTTP and HTTPS\\ E1202 00:10:54.485444 1 haproxy.go:392] can\\'t scrape HAProxy: dial unix /var/lib/haproxy/run/haproxy.sock: connect: no such file or directory\\ I1202 00:10:54.509522 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:10:54.509559 1 router.go:255] Router is including routes in all namespaces\\ I1202 00:10:54.746997 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:10:59.745754 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:11:06.198267 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:11:11.175543 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ Dec 02 00:18:21.264 W ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (3 times) Dec 02 00:18:21.295 W ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh 0/5 nodes are available: 2 node(s) didn\\'t match pod affinity/anti-affinity, 2 node(s) didn\\'t satisfy existing pods anti-affinity rules, 3 node(s) didn\\'t match node selector. Dec 02 00:18:31.269 W ns/openshift-ingress pod/router-default-79b887b8c9-4ghjn node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:18:31.277 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh Successfully assigned openshift-ingress/router-default-79b887b8c9-mgmrh to ip-10-0-140-191.ec2.internal Dec 02 00:18:31.288 W ns/openshift-machine-config-operator pod/machine-config-daemon-ghvjw node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:18:31.303 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 node/ created Dec 02 00:18:31.312 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-nmb27 Dec 02 00:18:31.313 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 Successfully assigned openshift-machine-config-operator/machine-config-daemon-nmb27 to ip-10-0-140-191.ec2.internal Dec 02 00:18:31.320 W ns/openshift-image-registry pod/node-ca-86jzh node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:18:31.364 I ns/openshift-image-registry pod/node-ca-4hb9w node/ created Dec 02 00:18:31.374 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4hb9w Dec 02 00:18:31.374 I ns/openshift-image-registry pod/node-ca-4hb9w Successfully assigned openshift-image-registry/node-ca-4hb9w to ip-10-0-140-191.ec2.internal Dec 02 00:18:34.017 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 02 00:18:34.189 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 Created container machine-config-daemon Dec 02 00:18:34.237 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 Started container machine-config-daemon Dec 02 00:18:40.481 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" already present on machine Dec 02 00:18:40.663 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh Created container router Dec 02 00:18:40.693 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh Started container router Dec 02 00:18:41.625 I ns/openshift-image-registry pod/node-ca-4hb9w Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 02 00:18:41.787 I ns/openshift-image-registry pod/node-ca-4hb9w Created container node-ca Dec 02 00:18:41.814 I ns/openshift-image-registry pod/node-ca-4hb9w Started container node-ca Dec 02 00:22:38.157 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (201 times) Dec 02 00:22:47.067 I ns/kube-system pod/pod0-system-node-critical node/ created Dec 02 00:22:47.078 I ns/kube-system pod/pod0-system-node-critical Successfully assigned kube-system/pod0-system-node-critical to ip-10-0-140-191.ec2.internal Dec 02 00:22:47.086 I ns/kube-system pod/pod1-system-cluster-critical node/ created Dec 02 00:22:47.096 I ns/kube-system pod/pod1-system-cluster-critical Successfully assigned kube-system/pod1-system-cluster-critical to ip-10-0-140-191.ec2.internal Dec 02 00:22:47.105 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-140-191.ec2.internal graceful deletion within 0s Dec 02 00:22:47.108 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:22:47.126 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-140-191.ec2.internal graceful deletion within 0s Dec 02 00:22:47.130 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:24:50.095 W ns/kube-system pod/pod0-system-node-critical Unable to mount volumes for pod \"pod0-system-node-critical_kube-system(80e7893d-3434-11eb-bd30-0afb4b6e93b5)\": timeout expired waiting for volumes to attach or mount for pod \"kube-system\"/\"pod0-system-node-critical\". list of unmounted volumes=[default-token-hkpkk]. list of unattached volumes=[default-token-hkpkk] Dec 02 00:24:50.122 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \"pod1-system-cluster-critical_kube-system(80eabf7a-3434-11eb-bd30-0afb4b6e93b5)\": timeout expired waiting for volumes to attach or mount for pod \"kube-system\"/\"pod1-system-cluster-critical\". list of unmounted volumes=[default-token-hkpkk]. list of unattached volumes=[default-token-hkpkk] Dec 02 00:27:37.749 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-1 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-1 (10 times) Dec 02 00:27:37.973 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (216 times) Dec 02 00:27:39.308 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-2 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-2 (10 times) Dec 02 00:27:39.475 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-b6pkn (13 times) Dec 02 00:27:39.635 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1b-zvmnm (13 times) Dec 02 00:27:40.728 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-master-0 Updated machine ci-op-lby5f363-7bc5c-6s6wb-master-0 (10 times) Dec 02 00:28:51.696 W ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 02 00:28:51.714 W ns/openshift-image-registry pod/node-ca-4hb9w node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:28:51.721 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 Stopping container machine-config-daemon Dec 02 00:28:51.741 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-mgmrh Dec 02 00:28:51.741 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-nmb27 Dec 02 00:28:51.741 I ns/openshift-image-registry pod/node-ca-4hb9w Stopping container node-ca Dec 02 00:28:51.768 I ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-nmb27 Dec 02 00:28:51.768 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4hb9w Dec 02 00:28:51.768 W ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:28:51.787 I ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh Stopping container router Dec 02 00:28:51.792 I ns/openshift-image-registry pod/node-ca-4hb9w Marking for deletion Pod openshift-image-registry/node-ca-4hb9w Dec 02 00:28:51.844 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf node/ created Dec 02 00:28:51.854 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-4njgf Dec 02 00:28:51.858 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf 0/5 nodes are available: 2 node(s) didn\\'t match pod affinity/anti-affinity, 2 node(s) didn\\'t satisfy existing pods anti-affinity rules, 3 node(s) didn\\'t match node selector. Dec 02 00:28:51.879 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf 0/5 nodes are available: 2 node(s) didn\\'t match pod affinity/anti-affinity, 2 node(s) didn\\'t satisfy existing pods anti-affinity rules, 3 node(s) didn\\'t match node selector. (2 times) Dec 02 00:28:53.247 E ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh node/ip-10-0-140-191.ec2.internal container=router container exited with code 2 (Error): I1202 00:18:40.715027 1 template.go:299] Starting template router (v4.1.46-202005230309)\\ I1202 00:18:40.718399 1 metrics.go:147] Router health and metrics port listening at 0.0.0.0:1936 on HTTP and HTTPS\\ E1202 00:18:40.725316 1 haproxy.go:392] can\\'t scrape HAProxy: dial unix /var/lib/haproxy/run/haproxy.sock: connect: no such file or directory\\ I1202 00:18:40.749875 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:18:40.749920 1 router.go:255] Router is including routes in all namespaces\\ I1202 00:18:40.988609 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:18:51.118949 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:18:56.115382 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:24:43.997744 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:24:48.987123 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:24:53.995867 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1202 00:24:58.997010 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ Dec 02 00:28:53.856 W ns/openshift-machine-config-operator pod/machine-config-daemon-nmb27 node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:28:53.866 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf 0/5 nodes are available: 2 node(s) didn\\'t match pod affinity/anti-affinity, 2 node(s) didn\\'t satisfy existing pods anti-affinity rules, 3 node(s) didn\\'t match node selector. (3 times) Dec 02 00:28:53.872 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk node/ created Dec 02 00:28:53.881 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-j7hqk Dec 02 00:28:53.886 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk Successfully assigned openshift-machine-config-operator/machine-config-daemon-j7hqk to ip-10-0-140-191.ec2.internal Dec 02 00:28:54.651 W ns/openshift-image-registry pod/node-ca-4hb9w node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:28:54.661 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf 0/5 nodes are available: 2 node(s) didn\\'t match pod affinity/anti-affinity, 2 node(s) didn\\'t satisfy existing pods anti-affinity rules, 3 node(s) didn\\'t match node selector. (4 times) Dec 02 00:28:54.685 I ns/openshift-image-registry pod/node-ca-l2ztl node/ created Dec 02 00:28:54.691 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-l2ztl Dec 02 00:28:54.695 I ns/openshift-image-registry pod/node-ca-l2ztl Successfully assigned openshift-image-registry/node-ca-l2ztl to ip-10-0-140-191.ec2.internal Dec 02 00:28:55.163 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 02 00:28:55.322 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk Created container machine-config-daemon Dec 02 00:28:55.355 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk Started container machine-config-daemon Dec 02 00:28:56.068 W ns/openshift-ingress pod/router-default-79b887b8c9-mgmrh node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:28:56.071 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf Successfully assigned openshift-ingress/router-default-79b887b8c9-4njgf to ip-10-0-140-191.ec2.internal Dec 02 00:29:03.159 I ns/openshift-image-registry pod/node-ca-l2ztl Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 02 00:29:03.333 I ns/openshift-image-registry pod/node-ca-l2ztl Created container node-ca Dec 02 00:29:03.358 I ns/openshift-image-registry pod/node-ca-l2ztl Started container node-ca Dec 02 00:29:05.524 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" already present on machine Dec 02 00:29:05.684 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf Created container router Dec 02 00:29:05.717 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf Started container router Dec 02 00:32:04.465 W ns/openshift-image-registry pod/node-ca-l2ztl node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:32:04.466 W ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk node/ip-10-0-140-191.ec2.internal graceful deletion within 600s Dec 02 00:32:04.474 I ns/openshift-image-registry pod/node-ca-l2ztl Stopping container node-ca Dec 02 00:32:04.479 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-l2ztl Dec 02 00:32:04.481 I ns/openshift-image-registry pod/node-ca-l2ztl Marking for deletion Pod openshift-image-registry/node-ca-l2ztl Dec 02 00:32:04.482 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk Stopping container machine-config-daemon Dec 02 00:32:04.484 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-j7hqk Dec 02 00:32:04.486 I ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-j7hqk Dec 02 00:32:04.488 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf node/ip-10-0-140-191.ec2.internal graceful deletion within 30s Dec 02 00:32:04.495 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf Marking for deletion Pod openshift-ingress/router-default-79b887b8c9-4njgf Dec 02 00:32:04.497 I ns/openshift-ingress pod/router-default-79b887b8c9-4njgf Stopping container router Dec 02 00:32:04.538 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd node/ created Dec 02 00:32:04.542 I ns/openshift-ingress replicaset/router-default-79b887b8c9 Created pod: router-default-79b887b8c9-qbrcd Dec 02 00:32:04.561 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. Dec 02 00:32:04.583 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (2 times) Dec 02 00:32:06.193 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 02 00:32:06.193 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf node/ip-10-0-140-191.ec2.internal container=router container stopped being ready Dec 02 00:32:06.579 W ns/openshift-image-registry pod/node-ca-l2ztl node/ip-10-0-140-191.ec2.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 02 00:32:06.579 W ns/openshift-image-registry pod/node-ca-l2ztl node/ip-10-0-140-191.ec2.internal container=node-ca container stopped being ready Dec 02 00:32:11.254 W ns/openshift-machine-config-operator pod/machine-config-daemon-j7hqk node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:32:11.263 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (3 times) Dec 02 00:32:11.284 W ns/openshift-image-registry pod/node-ca-l2ztl node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:32:11.291 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (4 times) Dec 02 00:32:11.312 W ns/openshift-ingress pod/router-default-79b887b8c9-4njgf node/ip-10-0-140-191.ec2.internal deleted Dec 02 00:32:11.320 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (5 times) Dec 02 00:32:12.613 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (6 times) Dec 02 00:32:41.267 W ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd 0/5 nodes are available: 1 node(s) didn\\'t match pod affinity/anti-affinity, 1 node(s) didn\\'t satisfy existing pods anti-affinity rules, 1 node(s) had taints that the pod didn\\'t tolerate, 3 node(s) didn\\'t match node selector. (7 times) Dec 02 00:32:41.303 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Successfully assigned openshift-ingress/router-default-79b887b8c9-qbrcd to ip-10-0-140-191.ec2.internal Dec 02 00:32:41.311 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 node/ created Dec 02 00:32:41.317 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-jrcl2 Dec 02 00:32:41.321 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Successfully assigned openshift-machine-config-operator/machine-config-daemon-jrcl2 to ip-10-0-140-191.ec2.internal Dec 02 00:32:41.336 I ns/openshift-image-registry pod/node-ca-8rl2j node/ created Dec 02 00:32:41.342 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-8rl2j Dec 02 00:32:41.345 I ns/openshift-image-registry pod/node-ca-8rl2j Successfully assigned openshift-image-registry/node-ca-8rl2j to ip-10-0-140-191.ec2.internal Dec 02 00:32:43.551 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 02 00:32:43.689 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Created container machine-config-daemon Dec 02 00:32:43.721 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Started container machine-config-daemon Dec 02 00:32:44.597 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (232 times) Dec 02 00:32:49.885 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" already present on machine Dec 02 00:32:50.045 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Created container router Dec 02 00:32:50.071 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Started container router Dec 02 00:32:50.587 I ns/openshift-image-registry pod/node-ca-8rl2j Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 02 00:32:50.720 I ns/openshift-image-registry pod/node-ca-8rl2j Created container node-ca Dec 02 00:32:50.745 I ns/openshift-image-registry pod/node-ca-8rl2j Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201202-003301.xml error: 1 fail, 39 pass, 39 skip (1h5m0s) 2020/12/02 00:33:05 Container test in pod e2e-aws-serial failed, exit code 1, reason Error 2020/12/02 00:50:24 Copied 89.36MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/12/02 00:50:24 Releasing leases for \"e2e-aws-serial\" 2020/12/02 00:50:24 Releasing lease aeca4927-4e1d-407d-99f4-a405ff9a723b for \"aws-quota-slice\" 2020/12/02 00:50:25 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/02 00:50:25 Ran for 2h0m50s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-lby5f363/e2e-aws-serial failed after 1h54m14s (failed containers: test): ContainerFailed one or more containers exited Container test exited with code 1, reason Error --- ca-8rl2j Dec 02 00:32:41.345 I ns/openshift-image-registry pod/node-ca-8rl2j Successfully assigned openshift-image-registry/node-ca-8rl2j to ip-10-0-140-191.ec2.internal Dec 02 00:32:43.551 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 02 00:32:43.689 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Created container machine-config-daemon Dec 02 00:32:43.721 I ns/openshift-machine-config-operator pod/machine-config-daemon-jrcl2 Started container machine-config-daemon Dec 02 00:32:44.597 I ns/openshift-machine-api machine/ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t Updated machine ci-op-lby5f363-7bc5c-6s6wb-worker-us-east-1c-ww58t (232 times) Dec 02 00:32:49.885 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" already present on machine Dec 02 00:32:50.045 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Created container router Dec 02 00:32:50.071 I ns/openshift-ingress pod/router-default-79b887b8c9-qbrcd Started container router Dec 02 00:32:50.587 I ns/openshift-image-registry pod/node-ca-8rl2j Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 02 00:32:50.720 I ns/openshift-image-registry pod/node-ca-8rl2j Created container node-ca Dec 02 00:32:50.745 I ns/openshift-image-registry pod/node-ca-8rl2j Started container node-ca Failing tests: [sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201202-003301.xml error: 1 fail, 39 pass, 39 skip (1h5m0s) --- '\n",
            "ID=28    : size=1         : b'2020/12/02 22:50:26 ci-operator version v20201202-337afb4 2020/12/02 22:50:26 No source defined 2020/12/02 22:50:26 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/02 22:50:26 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-infqit17 2020/12/02 22:50:26 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/02 22:50:26 Creating namespace ci-op-infqit17 2020/12/02 22:50:27 Setting up pipeline imagestream for the test 2020/12/02 22:50:27 Created secret e2e-aws-serial-cluster-profile 2020/12/02 22:50:27 Created secret pull-secret 2020/12/02 22:50:27 Created PDB for pods with openshift.io/build.name label 2020/12/02 22:50:27 Created PDB for pods with created-by-ci label 2020/12/02 22:50:27 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-infqit17/stable:${component} 2020/12/02 22:50:29 Importing release image latest 2020/12/02 22:50:30 Executing pod \"release-images-latest-cli\" 2020/12/02 22:50:40 Executing pod \"release-images-latest\" 2020/12/02 22:51:32 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/02 22:51:32 Acquiring leases for \"e2e-aws-serial\" 2020/12/02 22:51:32 Acquiring 1 lease(s) for \"aws-quota-slice\" 2020/12/02 22:55:12 Acquired lease(s) [ac75a510-a9cb-4103-b019-8cc35b0dafa2] for \"aws-quota-slice\" 2020/12/02 22:55:12 Executing template e2e-aws-serial 2020/12/02 22:55:12 Creating or restarting template instance 2020/12/02 22:55:12 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/02 22:55:12 Waiting for template instance to be ready 2020/12/02 22:55:15 Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-2 (zones: us-east-2a us-east-2b) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=warning msg=\"Found override for ReleaseImage. Please be warned, this is not advised\" level=info msg=\"Consuming \\\\\"Openshift Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Common Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Master Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Worker Machines\\\\\" from target directory\" level=info msg=\"Creating infrastructure resources...\" level=error level=error msg=\"Error: Error applying plan:\" level=error level=error msg=\"1 error occurred:\" level=error msg=\"\\\\t* module.vpc.aws_vpc.new_vpc: 1 error occurred:\" level=error msg=\"\\\\t* aws_vpc.new_vpc: Error creating VPC: VpcLimitExceeded: The maximum number of VPCs has been reached.\" level=error msg=\"\\\\tstatus code: 400, request id: e23a1f18-c7b8-4c1b-9189-8c66150640f9\" level=error level=error level=error level=error level=error level=error msg=\"Terraform does not automatically rollback in the face of errors.\" level=error msg=\"Instead, your Terraform state file has been partially updated with\" level=error msg=\"any resources that successfully completed. Please address the error\" level=error msg=\"above and apply again to incrementally change your infrastructure.\" level=error level=error level=fatal msg=\"failed to fetch Cluster: failed to generate asset \\\\\"Cluster\\\\\": failed to create cluster: failed to apply using Terraform\" 2020/12/02 23:01:05 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2020/12/02 23:17:09 Copied 1.98MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/12/02 23:17:10 Releasing leases for \"e2e-aws-serial\" 2020/12/02 23:17:10 Releasing lease ac75a510-a9cb-4103-b019-8cc35b0dafa2 for \"aws-quota-slice\" 2020/12/02 23:17:10 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/02 23:17:10 Ran for 26m43s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-infqit17/e2e-aws-serial failed after 21m54s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-2 (zones: us-east-2a us-east-2b) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=warning msg=\"Found override for ReleaseImage. Please be warned, this is not advised\" level=info msg=\"Consuming \\\\\"Openshift Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Common Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Master Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Worker Machines\\\\\" from target directory\" level=info msg=\"Creating infrastructure resources...\" level=error level=error msg=\"Error: Error applying plan:\" level=error level=error msg=\"1 error occurred:\" level=error msg=\"\\\\t* module.vpc.aws_vpc.new_vpc: 1 error occurred:\" level=error msg=\"\\\\t* aws_vpc.new_vpc: Error creating VPC: VpcLimitExceeded: The maximum number of VPCs has been reached.\" level=error msg=\"\\\\tstatus code: 400, request id: e23a1f18-c7b8-4c1b-9189-8c66150640f9\" level=error level=error level=error level=error level=error level=error msg=\"Terraform does not automatically rollback in the face of errors.\" level=error msg=\"Instead, your Terraform state file has been partially updated with\" level=error msg=\"any resources that successfully completed. Please address the error\" level=error msg=\"above and apply again to incrementally change your infrastructure.\" level=error level=error level=fatal msg=\"failed to fetch Cluster: failed to generate asset \\\\\"Cluster\\\\\": failed to create cluster: failed to apply using Terraform\" --- '\n",
            "ID=29    : size=20        : <*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \"release-images-latest-cli\" <*> <*> Executing pod \"release-images-latest\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \"e2e-aws-serial\" <*> <*> Acquiring 1 lease(s) for \"aws-quota-slice\" <*> <*> Acquired lease(s) <*> for \"aws-quota-slice\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \"e2e-aws-serial\" <*> <*> Releasing lease <*> for \"aws-quota-slice\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\n",
            "ID=30    : size=1         : b'2020/12/04 22:52:06 ci-operator version v20201204-48e5852 2020/12/04 22:52:06 No source defined 2020/12/04 22:52:06 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/04 22:52:06 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-cydrph0x 2020/12/04 22:52:06 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/04 22:52:06 Creating namespace ci-op-cydrph0x 2020/12/04 22:52:07 Setting up pipeline imagestream for the test 2020/12/04 22:52:07 Created secret e2e-aws-serial-cluster-profile 2020/12/04 22:52:07 Created secret pull-secret 2020/12/04 22:52:07 Created PDB for pods with openshift.io/build.name label 2020/12/04 22:52:07 Created PDB for pods with created-by-ci label 2020/12/04 22:52:07 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-cydrph0x/stable:${component} 2020/12/04 22:53:09 Importing release image latest 2020/12/04 22:53:09 Executing pod \"release-images-latest-cli\" 2020/12/04 22:53:19 Executing pod \"release-images-latest\" 2020/12/04 22:54:17 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/04 22:54:17 Acquiring leases for \"e2e-aws-serial\" 2020/12/04 22:54:17 Acquiring 1 lease(s) for \"aws-quota-slice\" 2020/12/04 23:13:56 Acquired lease(s) [b315453f-673a-4c36-a4d3-64dd02315ee8] for \"aws-quota-slice\" 2020/12/04 23:13:56 Executing template e2e-aws-serial 2020/12/04 23:13:56 Creating or restarting template instance 2020/12/04 23:13:56 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/04 23:13:56 Waiting for template instance to be ready 2020/12/04 23:13:58 Running pod e2e-aws-serial 2020/12/04 23:43:58 Container setup in pod e2e-aws-serial completed successfully 2020/12/05 00:39:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:40:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:41:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:42:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:43:07 warning: Failed to get namespace ci-op-cydrph0x for heartbeating: the server was unable to return a response in the time allotted, but may still be processing the request (get namespaces ci-op-cydrph0x) 2020/12/05 00:43:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:44:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:45:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:46:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:50:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:51:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:52:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:53:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:54:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:55:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:56:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:57:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:58:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:59:28 warning: failed to get pod e2e-aws-serial: the server was unable to return a response in the time allotted, but may still be processing the request (get pods e2e-aws-serial) 2020/12/05 00:59:49 Container teardown in pod e2e-aws-serial completed successfully 2020/12/05 00:59:49 Container test in pod e2e-aws-serial completed successfully 2020/12/05 00:59:49 Pod e2e-aws-serial succeeded after 1h43m21s 2020/12/05 00:59:58 Copied 110.59MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/12/05 00:59:58 Releasing leases for \"e2e-aws-serial\" 2020/12/05 00:59:58 Releasing lease b315453f-673a-4c36-a4d3-64dd02315ee8 for \"aws-quota-slice\" 2020/12/05 00:59:58 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/05 00:59:59 Ran for 2h7m52s '\n",
            "ID=31    : size=1         : b'2020/12/07 14:25:12 ci-operator version v20201205-32c093e 2020/12/07 14:25:12 No source defined 2020/12/07 14:25:12 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/07 14:25:12 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-f70d4d2b 2020/12/07 14:25:12 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/07 14:25:12 Creating namespace ci-op-f70d4d2b 2020/12/07 14:25:12 Setting up pipeline imagestream for the test 2020/12/07 14:25:12 Created secret e2e-aws-serial-cluster-profile 2020/12/07 14:25:12 Created secret pull-secret 2020/12/07 14:25:12 Created PDB for pods with openshift.io/build.name label 2020/12/07 14:25:12 Created PDB for pods with created-by-ci label 2020/12/07 14:25:12 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-f70d4d2b/stable:${component} 2020/12/07 14:26:14 Importing release image latest 2020/12/07 14:26:14 Executing pod \"release-images-latest-cli\" 2020/12/07 14:26:25 Executing pod \"release-images-latest\" 2020/12/07 14:27:17 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/07 14:27:17 Acquiring leases for \"e2e-aws-serial\" 2020/12/07 14:27:17 Acquiring 1 lease(s) for \"aws-quota-slice\" 2020/12/07 14:27:17 Acquired lease(s) [6ae2b5d6-3410-4bfc-b78d-30e51990464b] for \"aws-quota-slice\" 2020/12/07 14:27:17 Executing template e2e-aws-serial 2020/12/07 14:27:17 Creating or restarting template instance 2020/12/07 14:27:17 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/07 14:27:17 Waiting for template instance to be ready 2020/12/07 14:27:19 Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-2 (zones: us-east-2a us-east-2b) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=fatal msg=\"failed to fetch Common Manifests: failed to fetch dependency of \\\\\"Common Manifests\\\\\": failed to generate asset \\\\\"DNS Config\\\\\": getting public zone for \\\\\"origin-ci-int-aws.dev.rhcloud.com\\\\\": listing hosted zones: Throttling: Rate exceeded\\ \\\\tstatus code: 400, request id: 490a799e-917b-460c-b363-8b02ee82550e\" 2020/12/07 14:27:39 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2020/12/07 14:28:50 Releasing leases for \"e2e-aws-serial\" 2020/12/07 14:28:50 Releasing lease 6ae2b5d6-3410-4bfc-b78d-30e51990464b for \"aws-quota-slice\" 2020/12/07 14:28:50 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/07 14:28:50 Ran for 3m38s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-f70d4d2b/e2e-aws-serial failed after 1m31s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-2 (zones: us-east-2a us-east-2b) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=fatal msg=\"failed to fetch Common Manifests: failed to fetch dependency of \\\\\"Common Manifests\\\\\": failed to generate asset \\\\\"DNS Config\\\\\": getting public zone for \\\\\"origin-ci-int-aws.dev.rhcloud.com\\\\\": listing hosted zones: Throttling: Rate exceeded\\ \\\\tstatus code: 400, request id: 490a799e-917b-460c-b363-8b02ee82550e\" --- '\n",
            "ID=32    : size=1         : b'2020/12/09 14:26:35 ci-operator version v20201208-a4e5d83 2020/12/09 14:26:35 No source defined 2020/12/09 14:26:35 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/09 14:26:35 could not get routes in namespace openshift-console: routes.route.openshift.io is forbidden: User \"system:serviceaccount:ci:ci-operator\" cannot list routes.route.openshift.io in the namespace \"openshift-console\": no RBAC policy matched 2020/12/09 14:26:35 Using namespace ci-op-ixrvw8d1 2020/12/09 14:26:35 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/09 14:26:35 Creating namespace ci-op-ixrvw8d1 2020/12/09 14:26:35 Setting up pipeline imagestream for the test 2020/12/09 14:26:35 Created secret e2e-aws-serial-cluster-profile 2020/12/09 14:26:35 Created secret pull-secret 2020/12/09 14:26:35 Created PDB for pods with openshift.io/build.name label 2020/12/09 14:26:35 Created PDB for pods with created-by-ci label 2020/12/09 14:26:35 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-ixrvw8d1/stable:${component} 2020/12/09 14:27:38 Importing release image latest 2020/12/09 14:27:39 Executing pod \"release-images-latest-cli\" 2020/12/09 14:27:54 Executing pod \"release-images-latest\" 2020/12/09 14:29:02 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/09 14:29:02 Acquiring leases for \"e2e-aws-serial\" 2020/12/09 14:29:02 Acquiring 1 lease(s) for \"aws-quota-slice\" 2020/12/09 14:29:02 Acquired lease(s) [2ac7db5a-35b5-4a53-b989-346f5c1cd386] for \"aws-quota-slice\" 2020/12/09 14:29:02 Executing template e2e-aws-serial 2020/12/09 14:29:02 Creating or restarting template instance 2020/12/09 14:29:02 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/09 14:29:02 Waiting for template instance to be ready 2020/12/09 14:29:04 Running pod e2e-aws-serial 2020/12/09 15:02:59 Container setup in pod e2e-aws-serial completed successfully 2020/12/09 15:56:35 warning: Failed to patch the ci-op-ixrvw8d1 namespace to update the ci.openshift.io/active annotation: namespaces \"ci-op-ixrvw8d1\" is forbidden: User \"system:serviceaccount:ci:ci-operator\" cannot patch namespaces in the namespace \"ci-op-ixrvw8d1\": no RBAC policy matched 2020/12/09 16:10:34 Container test in pod e2e-aws-serial completed successfully 2020/12/09 16:16:39 Container teardown in pod e2e-aws-serial completed successfully 2020/12/09 16:16:39 Pod e2e-aws-serial succeeded after 1h47m35s 2020/12/09 16:16:49 Copied 114.49MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/12/09 16:16:49 Releasing leases for \"e2e-aws-serial\" 2020/12/09 16:16:49 Releasing lease 2ac7db5a-35b5-4a53-b989-346f5c1cd386 for \"aws-quota-slice\" 2020/12/09 16:16:49 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/09 16:16:49 Ran for 1h50m14s '\n",
            "ID=33    : size=2         : <*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \"release-images-latest-cli\" <*> <*> Executing pod \"release-images-latest\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \"e2e-aws-serial\" <*> <*> Acquiring 1 lease(s) for \"aws-quota-slice\" <*> <*> Acquired lease(s) <*> for \"aws-quota-slice\" <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: <*> (zones: <*> <*> level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=fatal msg=\"failed to fetch Common Manifests: failed to fetch dependency of \\\\\"Common Manifests\\\\\": failed to generate asset \\\\\"DNS Config\\\\\": getting public zone for \\\\\"origin-ci-int-aws.dev.rhcloud.com\\\\\": listing hosted zones: Throttling: Rate exceeded\\ \\\\tstatus code: 400, request id: <*> <*> <*> Container setup in pod e2e-aws-serial failed, exit code 1, reason Error <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Releasing leases for \"e2e-aws-serial\" <*> <*> Releasing lease <*> for \"aws-quota-slice\" <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod <*> failed after <*> (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: <*> (zones: <*> <*> level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=fatal msg=\"failed to fetch Common Manifests: failed to fetch dependency of \\\\\"Common Manifests\\\\\": failed to generate asset \\\\\"DNS Config\\\\\": getting public zone for \\\\\"origin-ci-int-aws.dev.rhcloud.com\\\\\": listing hosted zones: Throttling: Rate exceeded\\ \\\\tstatus code: 400, request id: <*> --- '\n",
            "ID=34    : size=1         : b'2020/12/14 14:31:51 ci-operator version v20201212-78272b9 2020/12/14 14:31:51 No source defined 2020/12/14 14:31:51 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/14 14:31:51 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-3p9nymqb 2020/12/14 14:31:51 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/14 14:31:51 Creating namespace ci-op-3p9nymqb 2020/12/14 14:31:51 Setting up pipeline imagestream for the test 2020/12/14 14:31:51 Created secret e2e-aws-serial-cluster-profile 2020/12/14 14:31:51 Created secret pull-secret 2020/12/14 14:31:51 Created PDB for pods with openshift.io/build.name label 2020/12/14 14:31:51 Created PDB for pods with created-by-ci label 2020/12/14 14:31:51 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-3p9nymqb/stable:${component} 2020/12/14 14:32:53 Importing release image latest 2020/12/14 14:32:54 Executing pod \"release-images-latest-cli\" 2020/12/14 14:32:59 Executing pod \"release-images-latest\" 2020/12/14 14:33:52 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/14 14:33:52 Acquiring leases for \"e2e-aws-serial\" 2020/12/14 14:33:52 Acquiring 1 lease(s) for \"aws-quota-slice\" 2020/12/14 14:33:52 Acquired lease(s) [us-west-2--34] for \"aws-quota-slice\" 2020/12/14 14:33:52 Executing template e2e-aws-serial 2020/12/14 14:33:52 Creating or restarting template instance 2020/12/14 14:33:52 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/14 14:33:52 Waiting for template instance to be ready 2020/12/14 14:33:54 Running pod e2e-aws-serial 2020/12/14 15:04:45 Container setup in pod e2e-aws-serial completed successfully {\"component\":\"entrypoint\",\"file\":\"prow/entrypoint/run.go:169\",\"func\":\"k8s.io/test-infra/prow/entrypoint.Options.ExecuteProcess\",\"level\":\"error\",\"msg\":\"Entrypoint received interrupt: terminated\",\"severity\":\"error\",\"time\":\"2020-12-14T16:13:37Z\"} time=\"2020-12-14T16:13:37Z\" level=info msg=\"Received signal.\" signal=interrupt '\n",
            "ID=35    : size=1         : b'2020/12/16 14:33:43 ci-operator version v20201215-fbe31de 2020/12/16 14:33:43 No source defined 2020/12/16 14:33:43 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2020/12/16 14:33:43 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-16pnc85m 2020/12/16 14:33:43 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2020/12/16 14:33:43 Creating namespace ci-op-16pnc85m 2020/12/16 14:33:43 Setting up pipeline imagestream for the test 2020/12/16 14:33:43 Created secret e2e-aws-serial-cluster-profile 2020/12/16 14:33:43 Created secret pull-secret 2020/12/16 14:33:43 Created PDB for pods with openshift.io/build.name label 2020/12/16 14:33:43 Created PDB for pods with created-by-ci label 2020/12/16 14:33:43 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-16pnc85m/stable:${component} 2020/12/16 14:34:46 Importing release image latest 2020/12/16 14:34:46 Executing pod \"release-images-latest-cli\" 2020/12/16 14:34:57 Executing pod \"release-images-latest\" 2020/12/16 14:35:45 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2020/12/16 14:35:45 Acquiring leases for \"e2e-aws-serial\" 2020/12/16 14:35:45 Acquiring 1 lease(s) for \"aws-quota-slice\" 2020/12/16 14:35:45 Acquired lease(s) [us-west-2--06] for \"aws-quota-slice\" 2020/12/16 14:35:45 Executing template e2e-aws-serial 2020/12/16 14:35:45 Creating or restarting template instance 2020/12/16 14:35:45 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2020/12/16 14:35:45 Waiting for template instance to be ready 2020/12/16 14:35:47 Running pod e2e-aws-serial 2020/12/16 15:05:07 Container setup in pod e2e-aws-serial completed successfully secret/support created started: (0/1/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.1s) 2020-12-16T15:05:53 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (0/2/79) \"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" Dec 16 15:05:54.612: INFO: >>> kubeConfig: /tmp/admin.kubeconfig Dec 16 15:05:54.614: INFO: Waiting up to 30m0s for all (but 100) nodes to be schedulable Dec 16 15:05:54.783: INFO: Unexpected error listing nodes: Get https://api.ci-op-16pnc85m-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443/api/v1/nodes?fieldSelector=spec.unschedulable%3Dfalse&resourceVersion=0: dial tcp 44.241.182.81:6443: connect: connection refused Dec 16 15:05:54.783: INFO: Unexpected error occurred: Get https://api.ci-op-16pnc85m-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443/api/v1/nodes?fieldSelector=spec.unschedulable%3Dfalse&resourceVersion=0: dial tcp 44.241.182.81:6443: connect: connection refused Dec 16 15:05:54.783: INFO: Running AfterSuite actions on all nodes Dec 16 15:05:54.783: INFO: Running AfterSuite actions on node 1 fail [k8s.io/kubernetes/test/e2e/e2e.go:101]: Expected error: <*url.Error | 0xc002903230>: { Op: \"Get\", URL: \"https://api.ci-op-16pnc85m-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443/api/v1/nodes?fieldSelector=spec.unschedulable%3Dfalse&resourceVersion=0\", Err: { Op: \"dial\", Net: \"tcp\", Source: nil, Addr: {IP: \",\\\\xf1\\\\xb6Q\", Port: 6443, Zone: \"\"}, Err: {Syscall: \"connect\", Err: 0x6f}, }, } Get https://api.ci-op-16pnc85m-7bc5c.origin-ci-int-aws.dev.rhcloud.com:6443/api/v1/nodes?fieldSelector=spec.unschedulable%3Dfalse&resourceVersion=0: dial tcp 44.241.182.81:6443: connect: connection refused not to have occurred Dec 16 15:05:54.560 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5af7bf25b394983f3ddba543e031d0cf9b89af18f637d5fbe7e1142172ab0cb\" already present on machine Dec 16 15:05:54.708 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Created container kube-apiserver-6 Dec 16 15:05:54.741 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Started container kube-apiserver-6 Dec 16 15:05:54.747 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c18ce2736a4fed8091de16f868bc46e6a9a03b7956c2418bad42fbb041ffbafb\" already present on machine failed: (1.1s) 2020-12-16T15:05:54 \"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/3/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m44s) 2020-12-16T15:08:38 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] doesn\\'t evict pod with tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/4/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.1s) 2020-12-16T15:09:18 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/5/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.8s) 2020-12-16T15:09:59 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should create sc, pod, pv, and pvc, read/write to the pv, and delete all created resources [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/6/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (39.2s) 2020-12-16T15:10:38 \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/7/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.9s) 2020-12-16T15:11:19 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/8/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m52s) 2020-12-16T15:13:11 \"[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/9/79) \"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\" skip [github.com/openshift/origin/test/extended/images/signatures.go:25]: disable because containers/image: https://github.com/containers/image/pull/570 skipped: (15.6s) 2020-12-16T15:13:26 \"[registry][Serial][Suite:openshift/registry/serial] Image signature workflow can push a signed image to openshift registry and verify it [Suite:openshift/conformance/serial]\" started: (1/10/79) \"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m17s) 2020-12-16T15:14:43 \"[sig-apps] Daemon set [Serial] should run and stop complex daemon with node affinity [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/11/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-12-16T15:15:24 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/12/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (34.5s) 2020-12-16T15:15:58 \"[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/13/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.3s) 2020-12-16T15:16:38 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should create and delete block persistent volumes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/14/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-12-16T15:17:19 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/15/79) \"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/apps/daemon_set.go:378]: Requires at least 2 nodes (not -1) skipped: (21.3s) 2020-12-16T15:17:40 \"[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/16/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (26.3s) 2020-12-16T15:18:07 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set same fsGroup for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/17/79) \"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m7s) 2020-12-16T15:19:14 \"[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/18/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m3s) 2020-12-16T15:21:17 \"[sig-scheduling] SchedulerPreemption [Serial] validates pod anti-affinity works in preemption [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/19/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (50.4s) 2020-12-16T15:22:08 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/20/79) \"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:247]: Could not get controller-manager metrics - skipping skipped: (28.7s) 2020-12-16T15:22:36 \"[sig-storage] [Serial] Volume metrics should create metrics for total number of volumes in A/D Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/21/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-12-16T15:23:17 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with defaults [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/22/79) \"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m50s) 2020-12-16T15:25:07 \"[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/23/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24s) 2020-12-16T15:25:31 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/24/79) \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (36.6s) 2020-12-16T15:26:08 \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/25/79) \"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (41.2s) 2020-12-16T15:26:49 \"[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/26/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.1s) 2020-12-16T15:27:13 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and write from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/27/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24s) 2020-12-16T15:27:37 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should not set different fsGroups for two pods simultaneously [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/28/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.8s) 2020-12-16T15:28:18 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/29/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.8s) 2020-12-16T15:28:58 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/30/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m38s) 2020-12-16T15:30:36 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/31/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (22.6s) 2020-12-16T15:30:59 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/32/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m28s) 2020-12-16T15:33:27 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be scheduled to node that don\\'t match the PodAntiAffinity terms [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/33/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m54s) 2020-12-16T15:36:21 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/34/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.8s) 2020-12-16T15:37:01 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/35/79) \"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.9s) 2020-12-16T15:37:22 \"[sig-storage] [Serial] Volume metrics PVController should create bound pv/pvc count metrics for pvc controller after creating both pv and pvc [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/36/79) \"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (21.2s) 2020-12-16T15:37:44 \"[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/37/79) \"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m0s) 2020-12-16T15:39:43 \"[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/38/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.4s) 2020-12-16T15:40:08 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/39/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (50.9s) 2020-12-16T15:40:59 \"[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/40/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-12-16T15:41:39 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/41/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.7s) 2020-12-16T15:42:20 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/42/79) \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m53s) 2020-12-16T15:44:13 \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/43/79) \"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m12s) 2020-12-16T15:45:25 \"[sig-api-machinery] Namespaces [Serial] should delete fast enough (90 percent of 100 namespaces in 150 seconds) [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/44/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (22.6s) 2020-12-16T15:45:47 \"[sig-scheduling] SchedulerPredicates [Serial] validates that NodeAffinity is respected if not matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/45/79) \"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m14s) 2020-12-16T15:48:01 \"[sig-storage] [Serial] Volume metrics should create volume metrics with the correct PVC ref [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/46/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m36s) 2020-12-16T15:50:37 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should be preferably scheduled to nodes pod can tolerate [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/47/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-12-16T15:51:18 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (xfs)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/48/79) \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (35.1s) 2020-12-16T15:51:53 \"[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should update the taint on a node [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/49/79) \"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (54.4s) 2020-12-16T15:52:47 \"[sig-scheduling] SchedulerPredicates [Serial] validates that required NodeAffinity setting is respected if matching [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/50/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-12-16T15:53:28 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/51/79) \"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.9s) 2020-12-16T15:53:49 \"[sig-storage] [Serial] Volume metrics PVController should create unbound pv count metrics for pvc controller after creating pv only [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/52/79) \"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:81]: Environment does not support getting controller-manager metrics - skipping skipped: (21s) 2020-12-16T15:54:10 \"[sig-storage] [Serial] Volume metrics should create prometheus metrics for volume provisioning and attach/detach [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/53/79) \"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m32s) 2020-12-16T15:56:42 \"[sig-scheduling] SchedulerPriorities [Serial] Pod should avoid nodes that have avoidPod annotation [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/54/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-12-16T15:57:23 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should be mountable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/55/79) \"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m57s) 2020-12-16T15:59:20 \"[k8s.io] EquivalenceCache [Serial] validates pod anti-affinity works properly when new replica pod is scheduled [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/56/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m33s) 2020-12-16T16:00:53 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] evicts pods from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/57/79) \"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\" passed: (48.4s) 2020-12-16T16:01:41 \"[sig-network] Service endpoints latency should not be very high [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s] [Serial]\" started: (1/58/79) \"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m22s) 2020-12-16T16:03:03 \"[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/59/79) \"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\" passed: (1m2s) 2020-12-16T16:04:05 \"[k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s. [Suite:openshift/conformance/serial] [Suite:k8s] [Serial]\" started: (1/60/79) \"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (3m18s) 2020-12-16T16:07:23 \"[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/61/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.7s) 2020-12-16T16:07:48 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] One pod requesting one prebound PVC should be able to mount volume and read from pod1 [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/62/79) \"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m16s) 2020-12-16T16:09:03 \"[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/63/79) \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m44s) 2020-12-16T16:11:47 \"[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] only evicts pods without tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/64/79) \"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:366]: Environment does not support getting controller-manager metrics - skipping skipped: (20.8s) 2020-12-16T16:12:08 \"[sig-storage] [Serial] Volume metrics PVController should create unbound pvc count metrics for pvc controller after creating pvc only [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/65/79) \"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/scheduling/taint_based_evictions.go:63]: Requires at least 2 nodes (not -1) skipped: (21s) 2020-12-16T16:12:29 \"[sig-scheduling] TaintBasedEvictions [Serial] Checks that the node becomes unreachable [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/66/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-12-16T16:13:10 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/67/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.6s) 2020-12-16T16:13:50 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (block volmode)] volumeMode should fail in binding dynamic provisioned PV to PVC [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/68/79) \"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m53s) 2020-12-16T16:15:43 \"[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/69/79) \"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (21.4s) 2020-12-16T16:16:05 \"[sig-scheduling] PodPriorityResolution [Serial] validates critical system priorities are created and resolved [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/70/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.5s) 2020-12-16T16:16:45 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/71/79) \"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" passed: (1m58s) 2020-12-16T16:18:43 \"[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] [Suite:openshift/conformance/serial/minimal] [Suite:k8s]\" started: (1/72/79) \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (2m40s) 2020-12-16T16:21:23 \"[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] eventually evict pod with finite tolerations from tainted nodes [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/73/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m16s) 2020-12-16T16:22:39 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should discover dynamically created local persistent volume mountpoint in discovery directory [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/74/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-12-16T16:23:19 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/75/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.9s) 2020-12-16T16:23:44 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Set fsGroup for local volume should set fsGroup for one pod [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/76/79) \"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/volume_metrics.go:193]: Could not get controller-manager metrics - skipping skipped: (57.9s) 2020-12-16T16:24:42 \"[sig-storage] [Serial] Volume metrics should create metrics for total time taken in volume operations in P/V Controller [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/77/79) \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\" passed: (1m50s) 2020-12-16T16:26:33 \"[sig-storage] PersistentVolumes-local Local volume provisioner [Serial] should create and recreate local persistent volume [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/78/79) \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/drivers/csi.go:263]: Only supported for providers [gce gke] (not aws) skipped: (40.4s) 2020-12-16T16:27:13 \"[sig-storage] CSI Volumes [Driver: pd.csi.storage.gke.io][Serial] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path [Suite:openshift/conformance/serial] [Suite:k8s]\" started: (1/79/79) \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" skip [k8s.io/kubernetes/test/e2e/storage/persistent_volumes-local.go:1968]: Requires at least 1 scsi fs localSSD skipped: (24.2s) 2020-12-16T16:27:37 \"[sig-storage] PersistentVolumes-local [Volume type: gce-localssd-scsi-fs] [Serial] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2 [Suite:openshift/conformance/serial] [Suite:k8s]\" Timeline: Dec 16 15:05:14.483 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d58cec13cbff1c54a3ccacde2a1727765493d8f785a98aea335d6b5baf931eeb\" already present on machine Dec 16 15:05:14.483 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal Created container pruner Dec 16 15:05:14.483 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal Started container pruner Dec 16 15:05:14.509 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal node/ip-10-0-153-107.us-west-2.compute.internal created Dec 16 15:05:14.509 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal node/ip-10-0-137-23.us-west-2.compute.internal created Dec 16 15:05:14.509 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-137-23.us-west-2.compute.internal node/ip-10-0-137-23.us-west-2.compute.internal created Dec 16 15:05:14.509 I ns/openshift-kube-apiserver pod/installer-6-ip-10-0-153-107.us-west-2.compute.internal node/ip-10-0-153-107.us-west-2.compute.internal created Dec 16 15:05:43.644 W ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal node/ip-10-0-153-107.us-west-2.compute.internal pod has been pending longer than a minute Dec 16 15:05:53.601 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d58cec13cbff1c54a3ccacde2a1727765493d8f785a98aea335d6b5baf931eeb\" already present on machine Dec 16 15:05:53.601 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal Created container pruner Dec 16 15:05:53.601 I ns/openshift-kube-scheduler pod/revision-pruner-5-ip-10-0-137-23.us-west-2.compute.internal Started container pruner Dec 16 15:05:53.725 - 1s I test=\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" running Dec 16 15:05:54.560 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5af7bf25b394983f3ddba543e031d0cf9b89af18f637d5fbe7e1142172ab0cb\" already present on machine Dec 16 15:05:54.708 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Created container kube-apiserver-6 Dec 16 15:05:54.741 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Started container kube-apiserver-6 Dec 16 15:05:54.747 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c18ce2736a4fed8091de16f868bc46e6a9a03b7956c2418bad42fbb041ffbafb\" already present on machine Dec 16 15:05:54.795 I test=\"[sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s]\" failed Dec 16 15:05:54.916 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Created container kube-apiserver-cert-syncer-6 Dec 16 15:05:54.948 I ns/openshift-kube-apiserver pod/kube-apiserver-ip-10-0-153-107.us-west-2.compute.internal Started container kube-apiserver-cert-syncer-6 Dec 16 15:05:55.559 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (5 times) Dec 16 15:05:56.414 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (5 times) Dec 16 15:05:57.245 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (5 times) Dec 16 15:05:57.486 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (8 times) Dec 16 15:05:57.680 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (8 times) Dec 16 15:05:57.842 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (8 times) Dec 16 15:06:09.721 W clusteroperator/kube-apiserver changed Progressing to False: AsExpected: Progressing: 3 nodes are at revision 6 Dec 16 15:06:09.724 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Updated node \"ip-10-0-153-107.us-west-2.compute.internal\" from revision 3 to 6 Dec 16 15:06:09.773 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Updated ConfigMap/revision-status-6 -n openshift-kube-apiserver: cause by changes in data.status Dec 16 15:06:12.504 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-153-107.us-west-2.compute.internal node/ip-10-0-153-107.us-west-2.compute.internal created Dec 16 15:06:12.508 I ns/openshift-kube-apiserver-operator deployment/kube-apiserver-operator Created Pod/revision-pruner-6-ip-10-0-153-107.us-west-2.compute.internal -n openshift-kube-apiserver because it was missing Dec 16 15:06:20.415 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-153-107.us-west-2.compute.internal Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c18ce2736a4fed8091de16f868bc46e6a9a03b7956c2418bad42fbb041ffbafb\" already present on machine Dec 16 15:06:20.552 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-153-107.us-west-2.compute.internal Created container pruner Dec 16 15:06:20.577 I ns/openshift-kube-apiserver pod/revision-pruner-6-ip-10-0-153-107.us-west-2.compute.internal Started container pruner Dec 16 15:07:04.607 W ns/openshift-image-registry pod/node-ca-cv7r9 node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:07:04.609 W ns/openshift-machine-config-operator pod/machine-config-daemon-w7xx2 node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 15:07:04.684 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-cv7r9 Dec 16 15:07:04.684 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-mf6bl Marking for deletion Pod openshift-marketplace/community-operators-6c6dcf4d66-mf6bl Dec 16 15:07:04.684 I ns/openshift-image-registry pod/node-ca-cv7r9 Stopping container node-ca Dec 16 15:07:04.684 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-w7xx2 Dec 16 15:07:04.684 I ns/openshift-image-registry pod/node-ca-cv7r9 Marking for deletion Pod openshift-image-registry/node-ca-cv7r9 Dec 16 15:07:04.684 W ns/openshift-ingress pod/router-default-5cd9b66f55-t6qsz node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:07:04.692 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-wm9g4 Marking for deletion Pod openshift-monitoring/prometheus-adapter-57c689dc7-wm9g4 Dec 16 15:07:04.692 I ns/openshift-machine-config-operator pod/machine-config-daemon-w7xx2 Stopping container machine-config-daemon Dec 16 15:07:04.693 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:07:04.693 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-mf6bl node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:07:04.694 I ns/openshift-ingress pod/router-default-5cd9b66f55-t6qsz Marking for deletion Pod openshift-ingress/router-default-5cd9b66f55-t6qsz Dec 16 15:07:04.695 W ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-wm9g4 node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:07:04.769 I ns/openshift-machine-config-operator pod/machine-config-daemon-w7xx2 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-w7xx2 Dec 16 15:07:04.770 I ns/openshift-ingress pod/router-default-5cd9b66f55-t6qsz Stopping container router Dec 16 15:07:04.770 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk Marking for deletion Pod openshift-monitoring/kube-state-metrics-7b4d49f7bd-xpvhk Dec 16 15:07:04.770 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-mf6bl Stopping container community-operators Dec 16 15:07:04.770 I ns/openshift-monitoring pod/alertmanager-main-0 Marking for deletion Pod openshift-monitoring/alertmanager-main-0 Dec 16 15:07:04.770 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk Stopping container kube-rbac-proxy-main Dec 16 15:07:04.770 I ns/openshift-marketplace replicaset/community-operators-6c6dcf4d66 Created pod: community-operators-6c6dcf4d66-wjdqs Dec 16 15:07:04.770 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 0s Dec 16 15:07:04.771 W ns/openshift-monitoring pod/alertmanager-main-0 node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:04.771 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs node/ created Dec 16 15:07:04.771 I ns/openshift-ingress pod/router-default-5cd9b66f55-4bsvp node/ created Dec 16 15:07:04.771 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-nrbvw node/ created Dec 16 15:07:04.778 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk Stopping container kube-state-metrics Dec 16 15:07:04.778 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Successfully assigned openshift-marketplace/community-operators-6c6dcf4d66-wjdqs to ip-10-0-140-148.us-west-2.compute.internal Dec 16 15:07:04.778 I ns/openshift-ingress pod/router-default-5cd9b66f55-4bsvp Successfully assigned openshift-ingress/router-default-5cd9b66f55-4bsvp to ip-10-0-151-168.us-west-2.compute.internal Dec 16 15:07:04.778 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk Stopping container kube-rbac-proxy-self Dec 16 15:07:04.778 I ns/openshift-monitoring replicaset/prometheus-adapter-57c689dc7 Created pod: prometheus-adapter-57c689dc7-nrbvw Dec 16 15:07:04.780 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-nrbvw Successfully assigned openshift-monitoring/prometheus-adapter-57c689dc7-nrbvw to ip-10-0-140-148.us-west-2.compute.internal Dec 16 15:07:04.780 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Successfully assigned openshift-monitoring/kube-state-metrics-7b4d49f7bd-m8r8n to ip-10-0-140-148.us-west-2.compute.internal Dec 16 15:07:04.780 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager Dec 16 15:07:04.780 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n node/ created Dec 16 15:07:04.855 I ns/openshift-ingress replicaset/router-default-5cd9b66f55 Created pod: router-default-5cd9b66f55-4bsvp Dec 16 15:07:04.855 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-wm9g4 Stopping container prometheus-adapter Dec 16 15:07:04.855 I ns/openshift-monitoring replicaset/kube-state-metrics-7b4d49f7bd Created pod: kube-state-metrics-7b4d49f7bd-m8r8n Dec 16 15:07:04.855 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy Dec 16 15:07:04.855 I ns/openshift-monitoring statefulset/alertmanager-main create Pod alertmanager-main-0 in StatefulSet alertmanager-main successful Dec 16 15:07:04.855 I ns/openshift-monitoring pod/alertmanager-main-0 Successfully assigned openshift-monitoring/alertmanager-main-0 to ip-10-0-151-168.us-west-2.compute.internal Dec 16 15:07:04.855 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader Dec 16 15:07:04.856 I ns/openshift-monitoring pod/alertmanager-main-0 node/ created Dec 16 15:07:05.017 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager (2 times) Dec 16 15:07:05.221 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container alertmanager-proxy (2 times) Dec 16 15:07:05.417 I ns/openshift-monitoring pod/alertmanager-main-0 Stopping container config-reloader (2 times) Dec 16 15:07:06.155 E ns/openshift-marketplace pod/community-operators-6c6dcf4d66-mf6bl node/ip-10-0-131-181.us-west-2.compute.internal container=community-operators container exited with code 2 (Error): Dec 16 15:07:08.153 W ns/openshift-image-registry pod/node-ca-cv7r9 node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:07:08.153 W ns/openshift-image-registry pod/node-ca-cv7r9 node/ip-10-0-131-181.us-west-2.compute.internal container=node-ca container stopped being ready Dec 16 15:07:08.552 W ns/openshift-machine-config-operator pod/machine-config-daemon-w7xx2 node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:07:08.552 W ns/openshift-machine-config-operator pod/machine-config-daemon-w7xx2 node/ip-10-0-131-181.us-west-2.compute.internal container=machine-config-daemon container stopped being ready Dec 16 15:07:08.818 W ns/openshift-machine-config-operator pod/machine-config-daemon-w7xx2 node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:09.154 E ns/openshift-ingress pod/router-default-5cd9b66f55-t6qsz node/ip-10-0-131-181.us-west-2.compute.internal container=router container exited with code 2 (Error): check ok : 0 retry attempt(s).\\ I1216 15:04:13.241408 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1216 15:04:18.234601 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1216 15:04:23.238772 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1216 15:04:33.295498 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1216 15:04:38.287234 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1216 15:04:43.283336 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1216 15:04:57.599531 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ E1216 15:05:52.927963 1 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=39, ErrCode=NO_ERROR, debug=\"\"\\ E1216 15:05:52.928002 1 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=39, ErrCode=NO_ERROR, debug=\"\"\\ E1216 15:05:52.927966 1 streamwatcher.go:109] Unable to decode an event from the watch stream: http2: server sent GOAWAY and closed the connection; LastStreamID=39, ErrCode=NO_ERROR, debug=\"\"\\ I1216 15:05:59.741117 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ I1216 15:06:09.696023 1 router.go:482] Router reloaded:\\ - Proxy protocol on, checking http://localhost:80 ...\\ - Health check ok : 0 retry attempt(s).\\ Dec 16 15:07:09.382 W ns/openshift-ingress pod/router-default-5cd9b66f55-t6qsz node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:09.753 E ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-wm9g4 node/ip-10-0-131-181.us-west-2.compute.internal container=prometheus-adapter container exited with code 2 (Error): Dec 16 15:07:09.957 W ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-wm9g4 node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:10.952 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:07:10.952 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk node/ip-10-0-131-181.us-west-2.compute.internal container=kube-state-metrics container stopped being ready Dec 16 15:07:10.952 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk node/ip-10-0-131-181.us-west-2.compute.internal container=kube-rbac-proxy-main container stopped being ready Dec 16 15:07:10.952 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk node/ip-10-0-131-181.us-west-2.compute.internal container=kube-rbac-proxy-self container stopped being ready Dec 16 15:07:11.158 W ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-xpvhk node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:11.817 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-mf6bl node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:12.356 W ns/openshift-image-registry pod/node-ca-cv7r9 node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:07:12.536 I ns/openshift-ingress pod/router-default-5cd9b66f55-4bsvp Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" Dec 16 15:07:12.767 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6b9d07e8eed67a34b50a3afaa36cd4b9f052697894e64b5c203d0675d12ab5ec\" already present on machine Dec 16 15:07:12.913 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager Dec 16 15:07:12.937 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager Dec 16 15:07:12.942 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:81854d02884cad16cda48a70c62f763b8888a251ec51c5e5f85395734d92f753\" already present on machine Dec 16 15:07:13.109 I ns/openshift-monitoring pod/alertmanager-main-0 Created container config-reloader Dec 16 15:07:13.134 I ns/openshift-monitoring pod/alertmanager-main-0 Started container config-reloader Dec 16 15:07:13.144 I ns/openshift-monitoring pod/alertmanager-main-0 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dad532a6a92692c32fcc20b12c38627f63ef0f5a66f58828376876a80dfcaead\" already present on machine Dec 16 15:07:13.264 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-nrbvw Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\" Dec 16 15:07:13.275 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:69e43ce2394b34a984aca4c30461087dcb4cef9eb6c9a61467ac192d11782ca6\" already present on machine Dec 16 15:07:13.282 I ns/openshift-monitoring pod/alertmanager-main-0 Created container alertmanager-proxy Dec 16 15:07:13.307 I ns/openshift-monitoring pod/alertmanager-main-0 Started container alertmanager-proxy Dec 16 15:07:13.432 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Created container community-operators Dec 16 15:07:13.461 I ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Started container community-operators Dec 16 15:07:13.959 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Dec 16 15:07:14.152 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Created container kube-rbac-proxy-main Dec 16 15:07:14.213 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Started container kube-rbac-proxy-main Dec 16 15:07:14.218 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1239a6c4198162384a6f0dd7bb0124bd9ee18a73ef3e491fef889c827d70e7d\" already present on machine Dec 16 15:07:14.406 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Created container kube-rbac-proxy-self Dec 16 15:07:14.433 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Started container kube-rbac-proxy-self Dec 16 15:07:14.439 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c12c8ce2ac222f2205c8ab24dffef3ef34e3505c7a52d849b637ce3a7d91a388\" Dec 16 15:07:17.482 I ns/openshift-ingress pod/router-default-5cd9b66f55-4bsvp Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:414198fa3777f3e932dc68645a92a7c63f72d6ed80b66093b3875374ad3b04b9\" Dec 16 15:07:17.636 I ns/openshift-ingress pod/router-default-5cd9b66f55-4bsvp Created container router Dec 16 15:07:17.664 I ns/openshift-ingress pod/router-default-5cd9b66f55-4bsvp Started container router Dec 16 15:07:18.271 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-nrbvw Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aba78208445b9979e6233bb8efb4eb66c996868536bb96c5ae891f1c0884fd9d\" Dec 16 15:07:18.462 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-nrbvw Created container prometheus-adapter Dec 16 15:07:18.524 I ns/openshift-monitoring pod/prometheus-adapter-57c689dc7-nrbvw Started container prometheus-adapter Dec 16 15:07:18.819 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c12c8ce2ac222f2205c8ab24dffef3ef34e3505c7a52d849b637ce3a7d91a388\" Dec 16 15:07:19.003 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Created container kube-state-metrics Dec 16 15:07:19.034 I ns/openshift-monitoring pod/kube-state-metrics-7b4d49f7bd-m8r8n Started container kube-state-metrics Dec 16 15:07:22.153 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Readiness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ Dec 16 15:07:23.257 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Liveness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 Dec 16 15:07:32.154 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Readiness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ (2 times) Dec 16 15:07:33.258 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Liveness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 (2 times) Dec 16 15:07:42.151 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Readiness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ (3 times) Dec 16 15:07:43.259 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Liveness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 (3 times) Dec 16 15:07:52.160 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Readiness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ (4 times) Dec 16 15:07:53.259 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Liveness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 (4 times) Dec 16 15:08:02.163 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Readiness probe failed: timeout: failed to connect service \"localhost:50051\" within 1s\\ (5 times) Dec 16 15:08:03.258 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Liveness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 (5 times) Dec 16 15:08:09.876 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg node/ created Dec 16 15:08:09.953 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-v6jzg Dec 16 15:08:09.953 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg Successfully assigned openshift-machine-config-operator/machine-config-daemon-v6jzg to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:08:09.953 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4wd4w Dec 16 15:08:09.953 I ns/openshift-image-registry pod/node-ca-4wd4w node/ created Dec 16 15:08:09.961 I ns/openshift-image-registry pod/node-ca-4wd4w Successfully assigned openshift-image-registry/node-ca-4wd4w to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:08:10.612 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 16 15:08:10.729 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg Created container machine-config-daemon Dec 16 15:08:10.766 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg Started container machine-config-daemon Dec 16 15:08:13.258 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Liveness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 (6 times) Dec 16 15:08:14.256 W ns/openshift-marketplace pod/community-operators-6c6dcf4d66-wjdqs Readiness probe errored: rpc error: code = Unknown desc = command error: command timed out, stdout: , stderr: , exit code -1 Dec 16 15:08:17.740 I ns/openshift-image-registry pod/node-ca-4wd4w Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 16 15:08:17.854 I ns/openshift-image-registry pod/node-ca-4wd4w Created container node-ca Dec 16 15:08:17.877 I ns/openshift-image-registry pod/node-ca-4wd4w Started container node-ca Dec 16 15:15:55.575 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (6 times) Dec 16 15:15:56.478 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (6 times) Dec 16 15:15:57.524 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (6 times) Dec 16 15:15:57.687 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (9 times) Dec 16 15:15:57.832 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (9 times) Dec 16 15:15:57.968 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (9 times) Dec 16 15:25:52.549 W ns/openshift-image-registry pod/node-ca-4wd4w node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:25:52.626 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4wd4w Dec 16 15:25:52.626 I ns/openshift-image-registry pod/node-ca-4wd4w Stopping container node-ca Dec 16 15:25:52.626 W ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 15:25:52.626 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-v6jzg Dec 16 15:25:52.626 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-v6jzg Dec 16 15:25:52.626 I ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg Stopping container machine-config-daemon Dec 16 15:25:52.626 I ns/openshift-image-registry pod/node-ca-4wd4w Marking for deletion Pod openshift-image-registry/node-ca-4wd4w Dec 16 15:25:53.898 W ns/openshift-image-registry pod/node-ca-4wd4w node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:25:53.898 W ns/openshift-image-registry pod/node-ca-4wd4w node/ip-10-0-131-181.us-west-2.compute.internal container=node-ca container stopped being ready Dec 16 15:25:53.905 W ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:25:53.905 W ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg node/ip-10-0-131-181.us-west-2.compute.internal container=machine-config-daemon container stopped being ready Dec 16 15:25:55.682 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (7 times) Dec 16 15:25:56.787 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (7 times) Dec 16 15:25:56.954 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (10 times) Dec 16 15:25:57.093 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (10 times) Dec 16 15:25:57.312 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (10 times) Dec 16 15:25:58.311 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (7 times) Dec 16 15:25:58.644 W ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg node/ip-10-0-131-181.us-west-2.compute.internal pod has been pending longer than a minute Dec 16 15:25:58.644 W ns/openshift-image-registry pod/node-ca-4wd4w node/ip-10-0-131-181.us-west-2.compute.internal pod has been pending longer than a minute Dec 16 15:26:00.435 W ns/openshift-machine-config-operator pod/machine-config-daemon-v6jzg node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:26:00.516 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-hxc2m Dec 16 15:26:00.516 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m Successfully assigned openshift-machine-config-operator/machine-config-daemon-hxc2m to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:26:00.516 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m node/ created Dec 16 15:26:00.521 W ns/openshift-image-registry pod/node-ca-4wd4w node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:26:00.528 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-gbg6f Dec 16 15:26:00.528 I ns/openshift-image-registry pod/node-ca-gbg6f node/ created Dec 16 15:26:00.601 I ns/openshift-image-registry pod/node-ca-gbg6f Successfully assigned openshift-image-registry/node-ca-gbg6f to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:26:01.942 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 16 15:26:02.063 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m Created container machine-config-daemon Dec 16 15:26:02.095 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m Started container machine-config-daemon Dec 16 15:26:09.604 I ns/openshift-image-registry pod/node-ca-gbg6f Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 16 15:26:09.735 I ns/openshift-image-registry pod/node-ca-gbg6f Created container node-ca Dec 16 15:26:09.757 I ns/openshift-image-registry pod/node-ca-gbg6f Started container node-ca Dec 16 15:34:37.292 W ns/openshift-image-registry pod/node-ca-gbg6f node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:34:37.292 W ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 15:34:37.359 I ns/openshift-image-registry pod/node-ca-gbg6f Stopping container node-ca Dec 16 15:34:37.359 I ns/openshift-image-registry pod/node-ca-gbg6f Marking for deletion Pod openshift-image-registry/node-ca-gbg6f Dec 16 15:34:37.359 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-hxc2m Dec 16 15:34:37.359 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m Stopping container machine-config-daemon Dec 16 15:34:37.359 I ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-hxc2m Dec 16 15:34:37.359 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-gbg6f Dec 16 15:34:38.774 W ns/openshift-image-registry pod/node-ca-gbg6f node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:34:38.774 W ns/openshift-image-registry pod/node-ca-gbg6f node/ip-10-0-131-181.us-west-2.compute.internal container=node-ca container stopped being ready Dec 16 15:34:38.809 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-pv6t6 Dec 16 15:34:38.809 W ns/openshift-machine-config-operator pod/machine-config-daemon-hxc2m node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:34:38.810 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 node/ created Dec 16 15:34:38.855 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 Successfully assigned openshift-machine-config-operator/machine-config-daemon-pv6t6 to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:34:39.415 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 16 15:34:39.518 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 Created container machine-config-daemon Dec 16 15:34:39.544 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 Started container machine-config-daemon Dec 16 15:34:43.644 W ns/openshift-image-registry pod/node-ca-gbg6f node/ip-10-0-131-181.us-west-2.compute.internal pod has been pending longer than a minute Dec 16 15:34:50.430 W ns/openshift-image-registry pod/node-ca-gbg6f node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:34:50.508 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-4fhbr Dec 16 15:34:50.508 I ns/openshift-image-registry pod/node-ca-4fhbr Successfully assigned openshift-image-registry/node-ca-4fhbr to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:34:50.508 I ns/openshift-image-registry pod/node-ca-4fhbr node/ created Dec 16 15:34:58.794 I ns/openshift-image-registry pod/node-ca-4fhbr Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 16 15:34:58.947 I ns/openshift-image-registry pod/node-ca-4fhbr Created container node-ca Dec 16 15:34:58.986 I ns/openshift-image-registry pod/node-ca-4fhbr Started container node-ca Dec 16 15:35:55.827 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (8 times) Dec 16 15:35:56.702 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (8 times) Dec 16 15:35:57.535 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (8 times) Dec 16 15:35:57.688 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (11 times) Dec 16 15:35:57.887 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (11 times) Dec 16 15:35:58.060 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (11 times) Dec 16 15:39:04.823 I ns/kube-system pod/critical-pod node/ created Dec 16 15:39:04.828 W ns/kube-system pod/critical-pod 0/6 nodes are available: 3 Insufficient memory, 3 node(s) had taints that the pod didn\\'t tolerate. Dec 16 15:39:04.904 W ns/kube-system pod/critical-pod 0/6 nodes are available: 3 Insufficient memory, 3 node(s) had taints that the pod didn\\'t tolerate. (2 times) Dec 16 15:39:07.196 I ns/kube-system pod/critical-pod Successfully assigned kube-system/critical-pod to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:39:14.702 I ns/kube-system pod/critical-pod Container image \"k8s.gcr.io/pause:3.1\" already present on machine Dec 16 15:39:14.837 I ns/kube-system pod/critical-pod Created container critical-pod Dec 16 15:39:14.857 I ns/kube-system pod/critical-pod Started container critical-pod Dec 16 15:39:17.408 W ns/kube-system pod/critical-pod node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 0s Dec 16 15:39:17.411 W ns/kube-system pod/critical-pod node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:39:17.415 I ns/kube-system pod/critical-pod Stopping container critical-pod Dec 16 15:39:17.423 I ns/kube-system pod/critical-pod Stopping container critical-pod (2 times) Dec 16 15:43:30.258 W ns/openshift-image-registry pod/node-ca-4fhbr node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 15:43:30.260 W ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 15:43:30.266 I ns/openshift-image-registry pod/node-ca-4fhbr Stopping container node-ca Dec 16 15:43:30.268 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-4fhbr Dec 16 15:43:30.268 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-pv6t6 Dec 16 15:43:30.272 I ns/openshift-image-registry pod/node-ca-4fhbr Marking for deletion Pod openshift-image-registry/node-ca-4fhbr Dec 16 15:43:30.272 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-pv6t6 Dec 16 15:43:30.273 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 Stopping container machine-config-daemon Dec 16 15:43:31.596 W ns/openshift-image-registry pod/node-ca-4fhbr node/ip-10-0-131-181.us-west-2.compute.internal invariant violation (bug): pod should not transition Running->Pending even when terminated Dec 16 15:43:31.596 W ns/openshift-image-registry pod/node-ca-4fhbr node/ip-10-0-131-181.us-west-2.compute.internal container=node-ca container stopped being ready Dec 16 15:43:40.430 W ns/openshift-image-registry pod/node-ca-4fhbr node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:43:40.509 W ns/openshift-machine-config-operator pod/machine-config-daemon-pv6t6 node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 15:44:00.613 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp node/ created Dec 16 15:44:00.619 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-ms9vp Dec 16 15:44:00.682 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp Successfully assigned openshift-machine-config-operator/machine-config-daemon-ms9vp to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:44:00.682 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-rd77v Dec 16 15:44:00.682 I ns/openshift-image-registry pod/node-ca-rd77v Successfully assigned openshift-image-registry/node-ca-rd77v to ip-10-0-131-181.us-west-2.compute.internal Dec 16 15:44:00.683 I ns/openshift-image-registry pod/node-ca-rd77v node/ created Dec 16 15:44:01.284 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 16 15:44:01.414 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp Created container machine-config-daemon Dec 16 15:44:01.437 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp Started container machine-config-daemon Dec 16 15:44:08.337 I ns/openshift-image-registry pod/node-ca-rd77v Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 16 15:44:08.459 I ns/openshift-image-registry pod/node-ca-rd77v Created container node-ca Dec 16 15:44:08.482 I ns/openshift-image-registry pod/node-ca-rd77v Started container node-ca Dec 16 15:45:55.557 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (9 times) Dec 16 15:45:56.605 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (9 times) Dec 16 15:45:57.647 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (9 times) Dec 16 15:45:57.798 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (12 times) Dec 16 15:45:57.942 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (12 times) Dec 16 15:45:58.086 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (12 times) Dec 16 15:55:55.562 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (10 times) Dec 16 15:55:56.411 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (10 times) Dec 16 15:55:57.262 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (10 times) Dec 16 15:55:57.415 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (13 times) Dec 16 15:55:57.553 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (13 times) Dec 16 15:55:57.711 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (13 times) Dec 16 16:00:30.393 W ns/openshift-image-registry pod/node-ca-rd77v node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 16:00:30.396 W ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 16:00:30.466 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-rd77v Dec 16 16:00:30.466 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-ms9vp Dec 16 16:00:30.466 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-ms9vp Dec 16 16:00:30.466 I ns/openshift-image-registry pod/node-ca-rd77v Marking for deletion Pod openshift-image-registry/node-ca-rd77v Dec 16 16:00:30.466 I ns/openshift-image-registry pod/node-ca-rd77v Stopping container node-ca Dec 16 16:00:30.466 I ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp Stopping container machine-config-daemon Dec 16 16:00:40.511 W ns/openshift-machine-config-operator pod/machine-config-daemon-ms9vp node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:00:40.525 W ns/openshift-image-registry pod/node-ca-rd77v node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:00:40.617 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb node/ created Dec 16 16:00:40.624 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-sz5hb Dec 16 16:00:40.625 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb Successfully assigned openshift-machine-config-operator/machine-config-daemon-sz5hb to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:00:40.634 I ns/openshift-image-registry pod/node-ca-h28qx node/ created Dec 16 16:00:40.638 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-h28qx Dec 16 16:00:40.642 I ns/openshift-image-registry pod/node-ca-h28qx Successfully assigned openshift-image-registry/node-ca-h28qx to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:00:42.317 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 16 16:00:42.431 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb Created container machine-config-daemon Dec 16 16:00:42.462 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb Started container machine-config-daemon Dec 16 16:00:50.470 I ns/openshift-image-registry pod/node-ca-h28qx Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 16 16:00:50.600 I ns/openshift-image-registry pod/node-ca-h28qx Created container node-ca Dec 16 16:00:50.633 I ns/openshift-image-registry pod/node-ca-h28qx Started container node-ca Dec 16 16:05:55.777 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (11 times) Dec 16 16:05:55.925 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (14 times) Dec 16 16:05:56.055 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (14 times) Dec 16 16:05:56.187 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (14 times) Dec 16 16:05:57.077 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (11 times) Dec 16 16:05:57.947 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (11 times) Dec 16 16:10:14.051 W ns/openshift-image-registry pod/node-ca-h28qx node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 16:10:14.057 W ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 16:10:14.064 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-h28qx Dec 16 16:10:14.064 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-sz5hb Dec 16 16:10:14.125 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-sz5hb Dec 16 16:10:14.126 I ns/openshift-image-registry pod/node-ca-h28qx Stopping container node-ca Dec 16 16:10:14.126 I ns/openshift-image-registry pod/node-ca-h28qx Marking for deletion Pod openshift-image-registry/node-ca-h28qx Dec 16 16:10:14.126 I ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb Stopping container machine-config-daemon Dec 16 16:10:20.438 W ns/openshift-image-registry pod/node-ca-h28qx node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:10:20.515 W ns/openshift-machine-config-operator pod/machine-config-daemon-sz5hb node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:11:19.316 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j node/ created Dec 16 16:11:19.389 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-vc42j Dec 16 16:11:19.389 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j Successfully assigned openshift-machine-config-operator/machine-config-daemon-vc42j to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:11:19.389 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-s9hls Dec 16 16:11:19.390 I ns/openshift-image-registry pod/node-ca-s9hls node/ created Dec 16 16:11:19.400 I ns/openshift-image-registry pod/node-ca-s9hls Successfully assigned openshift-image-registry/node-ca-s9hls to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:11:19.983 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 16 16:11:20.101 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j Created container machine-config-daemon Dec 16 16:11:20.129 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j Started container machine-config-daemon Dec 16 16:11:27.070 I ns/openshift-image-registry pod/node-ca-s9hls Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 16 16:11:27.189 I ns/openshift-image-registry pod/node-ca-s9hls Created container node-ca Dec 16 16:11:27.212 I ns/openshift-image-registry pod/node-ca-s9hls Started container node-ca Dec 16 16:15:52.229 I ns/kube-system pod/pod0-system-node-critical node/ created Dec 16 16:15:52.238 I ns/kube-system pod/pod0-system-node-critical Successfully assigned kube-system/pod0-system-node-critical to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:15:52.317 I ns/kube-system pod/pod1-system-cluster-critical node/ created Dec 16 16:15:52.324 I ns/kube-system pod/pod1-system-cluster-critical Successfully assigned kube-system/pod1-system-cluster-critical to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:15:52.406 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 0s Dec 16 16:15:52.410 W ns/kube-system pod/pod1-system-cluster-critical node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:15:52.500 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 0s Dec 16 16:15:52.503 W ns/kube-system pod/pod0-system-node-critical node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:15:55.596 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (12 times) Dec 16 16:15:56.418 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (12 times) Dec 16 16:15:57.249 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (12 times) Dec 16 16:15:57.399 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (15 times) Dec 16 16:15:57.531 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (15 times) Dec 16 16:15:57.683 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (15 times) Dec 16 16:16:02.182 W ns/kube-system pod/pod0-system-node-critical Failed create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_pod0-system-node-critical_kube-system_f7ae53a2-3fb9-11eb-8ef4-067d63babeb5_0(5743c7b17a06e4df1af00ebf05c99f1fb06df242a80c8e05c82210ffa0c68931): Multus: Err adding pod to network \"openshift-sdn\": cannot set \"openshift-sdn\" ifname to \"eth0\": no netns: failed to Statfs \"/proc/102608/ns/net\": no such file or directory Dec 16 16:17:55.347 W ns/kube-system pod/pod1-system-cluster-critical Unable to mount volumes for pod \"pod1-system-cluster-critical_kube-system(f7bc2192-3fb9-11eb-8ef4-067d63babeb5)\": timeout expired waiting for volumes to attach or mount for pod \"kube-system\"/\"pod1-system-cluster-critical\". list of unmounted volumes=[default-token-7q7hv]. list of unattached volumes=[default-token-7q7hv] Dec 16 16:19:53.824 W ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 600s Dec 16 16:19:53.824 W ns/openshift-image-registry pod/node-ca-s9hls node/ip-10-0-131-181.us-west-2.compute.internal graceful deletion within 30s Dec 16 16:19:53.834 I ns/openshift-image-registry daemonset/node-ca Deleted pod: node-ca-s9hls Dec 16 16:19:53.835 I ns/openshift-image-registry pod/node-ca-s9hls Marking for deletion Pod openshift-image-registry/node-ca-s9hls Dec 16 16:19:53.837 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j Stopping container machine-config-daemon Dec 16 16:19:53.898 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Deleted pod: machine-config-daemon-vc42j Dec 16 16:19:53.898 I ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j Marking for deletion Pod openshift-machine-config-operator/machine-config-daemon-vc42j Dec 16 16:19:53.898 I ns/openshift-image-registry pod/node-ca-s9hls Stopping container node-ca Dec 16 16:20:00.433 W ns/openshift-machine-config-operator pod/machine-config-daemon-vc42j node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:20:00.516 W ns/openshift-image-registry pod/node-ca-s9hls node/ip-10-0-131-181.us-west-2.compute.internal deleted Dec 16 16:21:10.621 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv7kx node/ created Dec 16 16:21:10.626 I ns/openshift-machine-config-operator daemonset/machine-config-daemon Created pod: machine-config-daemon-pv7kx Dec 16 16:21:10.693 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv7kx Successfully assigned openshift-machine-config-operator/machine-config-daemon-pv7kx to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:21:10.693 I ns/openshift-image-registry daemonset/node-ca Created pod: node-ca-cdgwk Dec 16 16:21:10.693 I ns/openshift-image-registry pod/node-ca-cdgwk Successfully assigned openshift-image-registry/node-ca-cdgwk to ip-10-0-131-181.us-west-2.compute.internal Dec 16 16:21:10.694 I ns/openshift-image-registry pod/node-ca-cdgwk node/ created Dec 16 16:21:11.273 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv7kx Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d5ed57d22e3cbcdc1d7c9f68906b30cc1632aebca05c8e3087d678461a050250\" already present on machine Dec 16 16:21:11.410 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv7kx Created container machine-config-daemon Dec 16 16:21:11.434 I ns/openshift-machine-config-operator pod/machine-config-daemon-pv7kx Started container machine-config-daemon Dec 16 16:21:19.142 I ns/openshift-image-registry pod/node-ca-cdgwk Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 16 16:21:19.258 I ns/openshift-image-registry pod/node-ca-cdgwk Created container node-ca Dec 16 16:21:19.280 I ns/openshift-image-registry pod/node-ca-cdgwk Started container node-ca Dec 16 16:24:30.571 W persistentvolume/pvc-169dd196-3fbb-11eb-a83f-02e495b1b803 Error deleting EBS volume \"vol-0f2193f218508c930\" since volume is currently attached to \"i-0294833eaa2b7d0bc\" Dec 16 16:25:55.628 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (13 times) Dec 16 16:25:56.465 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (13 times) Dec 16 16:25:57.460 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (13 times) Dec 16 16:25:57.661 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (16 times) Dec 16 16:25:57.816 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (16 times) Dec 16 16:25:57.946 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (16 times) Failing tests: [sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201216-162737.xml error: 1 fail, 39 pass, 39 skip (1h22m24s) 2020/12/16 16:27:42 Container test in pod e2e-aws-serial failed, exit code 1, reason Error 2020/12/16 16:35:02 Copied 113.22MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2020/12/16 16:35:03 Releasing leases for \"e2e-aws-serial\" 2020/12/16 16:35:03 Releasing lease us-west-2--06 for \"aws-quota-slice\" 2020/12/16 16:35:03 No custom metadata found and prow metadata already exists. Not updating the metadata. 2020/12/16 16:35:03 Ran for 2h1m19s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-16pnc85m/e2e-aws-serial failed after 1h59m7s (failed containers: test): ContainerFailed one or more containers exited Container test exited with code 1, reason Error --- ine-config-operator pod/machine-config-daemon-pv7kx Started container machine-config-daemon Dec 16 16:21:19.142 I ns/openshift-image-registry pod/node-ca-cdgwk Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3495eb3a228c7a28d644dae16b71ccaa7ea5d3bab00ed16d3e8be82491eee963\" already present on machine Dec 16 16:21:19.258 I ns/openshift-image-registry pod/node-ca-cdgwk Created container node-ca Dec 16 16:21:19.280 I ns/openshift-image-registry pod/node-ca-cdgwk Started container node-ca Dec 16 16:24:30.571 W persistentvolume/pvc-169dd196-3fbb-11eb-a83f-02e495b1b803 Error deleting EBS volume \"vol-0f2193f218508c930\" since volume is currently attached to \"i-0294833eaa2b7d0bc\" Dec 16 16:25:55.628 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-0 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-0 (13 times) Dec 16 16:25:56.465 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-1 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-1 (13 times) Dec 16 16:25:57.460 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-master-2 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-master-2 (13 times) Dec 16 16:25:57.661 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-kl5j7 (16 times) Dec 16 16:25:57.816 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2a-ssm92 (16 times) Dec 16 16:25:57.946 I ns/openshift-machine-api machine/ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l Updated machine ci-op-16pnc85m-7bc5c-t6jp4-worker-us-west-2b-k577l (16 times) Failing tests: [sig-api-machinery] Namespaces [Serial] should always delete fast (ALL of 100 namespaces in 150 seconds) [Feature:ComprehensiveNamespaceDraining] [Suite:openshift/conformance/serial] [Suite:k8s] Writing JUnit report to /tmp/artifacts/junit/junit_e2e_20201216-162737.xml error: 1 fail, 39 pass, 39 skip (1h22m24s) --- '\n",
            "ID=36    : size=1         : b' <!doctype html> \\t<html> \\t<head> \\t <link rel=\"stylesheet\" type=\"text/css\" href=\"/styles/style.css\"> \\t <meta charset=\"utf-8\"> \\t <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> \\t <title>GCS browser: origin-ci-test</title> \\t\\t<style> \\t\\theader { \\t\\t\\tmargin-left: 10px; \\t\\t} \\t\\t.next-button { \\t\\t\\tmargin: 10px 0; \\t\\t} \\t\\t.grid-head { \\t\\t\\tborder-bottom: 1px solid black; \\t\\t} \\t\\t.resource-grid { \\t\\t\\tmargin-right: 20px; \\t\\t} \\t\\tli.grid-row:nth-child(even) { \\t\\t\\tbackground-color: #ddd; \\t\\t} \\t\\tli div { \\t\\t\\tbox-sizing: border-box; \\t\\t\\tborder-left: 1px solid black; \\t\\t\\tpadding-left: 5px; \\t\\t\\toverflow-wrap: break-word; \\t\\t} \\t\\tli div:first-child { \\t\\t\\tborder-left: none; \\t\\t} \\t\\t</style> \\t</head> \\t<body> <header> <h1>origin-ci-test</h1> <h3>/origin-ci-test/logs/release-openshift-ocp-installer-e2e-aws-serial-4.1/1340667363283963904/build-log.txt/</h3> </header> <ul class=\"resource-grid\"> \\t<li class=\"pure-g\"> \\t\\t<div class=\"pure-u-2-5 grid-head\">Name</div> \\t\\t<div class=\"pure-u-1-5 grid-head\">Size</div> \\t\\t<div class=\"pure-u-2-5 grid-head\">Modified</div> \\t</li> <li class=\"pure-g grid-row\"> \\t <div class=\"pure-u-2-5\"><a href=\"/gcs/origin-ci-test/logs/release-openshift-ocp-installer-e2e-aws-serial-4.1/1340667363283963904/\"><img src=\"/icons/back.png\"> ..</a></div> \\t <div class=\"pure-u-1-5\">-</div> \\t <div class=\"pure-u-2-5\">-</div> \\t</li> </ul></body></html>'\n",
            "ID=37    : size=4         : <*> <*> ci-operator version <*> <*> <*> No source defined <*> <*> Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 <*> <*> Using namespace <*> <*> <*> Running [release-inputs], e2e-aws-serial, [images], [release:latest] <*> <*> Creating namespace <*> <*> <*> Setting up pipeline imagestream for the test <*> <*> Created secret e2e-aws-serial-cluster-profile <*> <*> Created secret pull-secret <*> <*> Created PDB for pods with openshift.io/build.name label <*> <*> Created PDB for pods with created-by-ci label <*> <*> Tagged shared images from ocp/4.1:${component}, images will be pullable from <*> <*> <*> Importing release image latest <*> <*> Executing pod \"release-images-latest-cli\" running image \"release:latest\" <*> <*> Executing pod \"release-images-latest\" running image \"stable:cli\" <*> <*> Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest <*> <*> Acquiring leases for \"e2e-aws-serial\" <*> <*> Acquiring 1 lease(s) for \"aws-quota-slice\" <*> <*> Acquired lease(s) <*> <*> <*> <*> <*> Executing template e2e-aws-serial <*> <*> Creating or restarting template instance <*> <*> Template instance e2e-aws-serial already deleted, do not need to wait any longer <*> <*> Waiting for template instance to be ready <*> <*> Running pod e2e-aws-serial <*> <*> Container setup in pod e2e-aws-serial completed successfully <*> <*> Container test in pod e2e-aws-serial completed successfully <*> <*> Container teardown in pod e2e-aws-serial completed successfully <*> <*> Pod e2e-aws-serial succeeded after <*> <*> <*> Copied <*> of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial <*> <*> Releasing leases for \"e2e-aws-serial\" <*> <*> Releasing lease <*> <*> <*> <*> <*> No custom metadata found and prow metadata already exists. Not updating the metadata. <*> <*> Ran for <*> '\n",
            "ID=38    : size=1         : b'2021/01/02 14:47:33 ci-operator version v20201231-4891b38 2021/01/02 14:47:33 No source defined 2021/01/02 14:47:33 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2021/01/02 14:47:33 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-6jxq6mqt 2021/01/02 14:47:33 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2021/01/02 14:47:33 Creating namespace ci-op-6jxq6mqt 2021/01/02 14:47:34 Setting up pipeline imagestream for the test 2021/01/02 14:47:34 Created secret e2e-aws-serial-cluster-profile 2021/01/02 14:47:34 Created secret pull-secret 2021/01/02 14:47:34 Created PDB for pods with openshift.io/build.name label 2021/01/02 14:47:34 Created PDB for pods with created-by-ci label 2021/01/02 14:47:34 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-6jxq6mqt/stable:${component} 2021/01/02 14:48:36 Importing release image latest 2021/01/02 14:48:37 Executing pod \"release-images-latest-cli\" running image \"release:latest\" 2021/01/02 14:48:47 Executing pod \"release-images-latest\" running image \"stable:cli\" 2021/01/02 14:49:35 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2021/01/02 14:49:35 Acquiring leases for \"e2e-aws-serial\" 2021/01/02 14:49:35 Acquiring 1 lease(s) for \"aws-quota-slice\" 2021/01/02 14:49:35 Acquired lease(s) [us-west-2--02] for \"aws-quota-slice\" 2021/01/02 14:49:35 Executing template e2e-aws-serial 2021/01/02 14:49:35 Creating or restarting template instance 2021/01/02 14:49:35 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2021/01/02 14:49:35 Waiting for template instance to be ready 2021/01/02 14:49:37 Running pod e2e-aws-serial Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-1 (zones: us-east-1b us-east-1c) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=warning msg=\"Found override for ReleaseImage. Please be warned, this is not advised\" level=info msg=\"Consuming \\\\\"Common Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Worker Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Openshift Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Master Machines\\\\\" from target directory\" level=info msg=\"Creating infrastructure resources...\" level=error level=error msg=\"Error: Error applying plan:\" level=error level=error msg=\"1 error occurred:\" level=error msg=\"\\\\t* module.dns.aws_route53_record.api_external: 1 error occurred:\" level=error msg=\"\\\\t* aws_route53_record.api_external: [ERR]: Error building changeset: timeout while waiting for state to become \\'accepted\\' (timeout: 5m0s)\" level=error level=error level=error level=error level=error level=error msg=\"Terraform does not automatically rollback in the face of errors.\" level=error msg=\"Instead, your Terraform state file has been partially updated with\" level=error msg=\"any resources that successfully completed. Please address the error\" level=error msg=\"above and apply again to incrementally change your infrastructure.\" level=error level=error level=fatal msg=\"failed to fetch Cluster: failed to generate asset \\\\\"Cluster\\\\\": failed to create cluster: failed to apply using Terraform\" 2021/01/02 15:05:47 Container setup in pod e2e-aws-serial failed, exit code 1, reason Error 2021/01/02 15:08:26 Copied 7.04MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2021/01/02 15:08:26 Releasing leases for \"e2e-aws-serial\" 2021/01/02 15:08:26 Releasing lease us-west-2--02 for \"aws-quota-slice\" 2021/01/02 15:08:26 No custom metadata found and prow metadata already exists. Not updating the metadata. 2021/01/02 15:08:26 Ran for 20m52s error: some steps failed: * could not run steps: step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-6jxq6mqt/e2e-aws-serial failed after 18m49s (failed containers: setup): ContainerFailed one or more containers exited Container setup exited with code 1, reason Error --- Installing from release registry.svc.ci.openshift.org/ocp/release:4.1 AWS region: us-east-1 (zones: us-east-1b us-east-1c) level=info msg=\"Consuming \\\\\"Install Config\\\\\" from target directory\" level=warning msg=\"Found override for ReleaseImage. Please be warned, this is not advised\" level=info msg=\"Consuming \\\\\"Common Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Worker Machines\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Openshift Manifests\\\\\" from target directory\" level=info msg=\"Consuming \\\\\"Master Machines\\\\\" from target directory\" level=info msg=\"Creating infrastructure resources...\" level=error level=error msg=\"Error: Error applying plan:\" level=error level=error msg=\"1 error occurred:\" level=error msg=\"\\\\t* module.dns.aws_route53_record.api_external: 1 error occurred:\" level=error msg=\"\\\\t* aws_route53_record.api_external: [ERR]: Error building changeset: timeout while waiting for state to become \\'accepted\\' (timeout: 5m0s)\" level=error level=error level=error level=error level=error level=error msg=\"Terraform does not automatically rollback in the face of errors.\" level=error msg=\"Instead, your Terraform state file has been partially updated with\" level=error msg=\"any resources that successfully completed. Please address the error\" level=error msg=\"above and apply again to incrementally change your infrastructure.\" level=error level=error level=fatal msg=\"failed to fetch Cluster: failed to generate asset \\\\\"Cluster\\\\\": failed to create cluster: failed to apply using Terraform\" --- '\n",
            "ID=39    : size=1         : b'2021/01/05 14:50:25 ci-operator version v20210105-a274f63 2021/01/05 14:50:25 No source defined 2021/01/05 14:50:25 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1 2021/01/05 14:50:25 Using namespace https://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-n2904x8n 2021/01/05 14:50:25 Running [release-inputs], e2e-aws-serial, [images], [release:latest] 2021/01/05 14:50:25 Creating namespace ci-op-n2904x8n 2021/01/05 14:50:25 Setting up pipeline imagestream for the test 2021/01/05 14:50:25 Created secret e2e-aws-serial-cluster-profile 2021/01/05 14:50:25 Created secret pull-secret 2021/01/05 14:50:25 Created PDB for pods with openshift.io/build.name label 2021/01/05 14:50:25 Created PDB for pods with created-by-ci label 2021/01/05 14:50:25 Tagged shared images from ocp/4.1:${component}, images will be pullable from registry.svc.ci.openshift.org/ci-op-n2904x8n/stable:${component} 2021/01/05 14:51:28 Importing release image latest 2021/01/05 14:51:28 Executing pod \"release-images-latest-cli\" running image \"release:latest\" 2021/01/05 14:52:03 Executing pod \"release-images-latest\" running image \"stable:cli\" 2021/01/05 14:54:17 Imported release 4.1.0-0.nightly-2020-07-29-210856 created at 2020-07-29 21:11:43 +0000 UTC with 84 images to tag release:latest 2021/01/05 14:54:17 Acquiring leases for \"e2e-aws-serial\" 2021/01/05 14:54:17 Acquiring 1 lease(s) for \"aws-quota-slice\" 2021/01/05 14:54:17 Acquired lease(s) for \"aws-quota-slice\": [us-west-1--26] 2021/01/05 14:54:17 Executing template e2e-aws-serial 2021/01/05 14:54:17 Creating or restarting template instance 2021/01/05 14:54:17 Template instance e2e-aws-serial already deleted, do not need to wait any longer 2021/01/05 14:54:17 Waiting for template instance to be ready 2021/01/05 14:54:19 Running pod e2e-aws-serial 2021/01/05 15:20:25 warning: Failed to patch the ci-op-n2904x8n namespace to update the ci.openshift.io/active annotation: namespaces \"ci-op-n2904x8n\" is forbidden: User \"system:serviceaccount:ci:ci-operator\" cannot patch namespaces in the namespace \"ci-op-n2904x8n\": no RBAC policy matched 2021/01/05 15:28:09 Container setup in pod e2e-aws-serial completed successfully 2021/01/05 16:35:49 Container test in pod e2e-aws-serial completed successfully 2021/01/05 16:42:19 Container teardown in pod e2e-aws-serial completed successfully 2021/01/05 16:42:19 Pod e2e-aws-serial succeeded after 1h47m58s 2021/01/05 16:42:27 Copied 106.34MB of artifacts from e2e-aws-serial to /logs/artifacts/e2e-aws-serial 2021/01/05 16:42:27 Releasing leases for \"e2e-aws-serial\" 2021/01/05 16:42:27 Releasing lease for \"aws-quota-slice\": us-west-1--26 2021/01/05 16:42:27 No custom metadata found and prow metadata already exists. Not updating the metadata. 2021/01/05 16:42:27 Ran for 1h52m1s '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RPAVaF3LoRb",
        "outputId": "444d2a18-53d6-4bca-e11a-664b0f8b9af1"
      },
      "source": [
        "!pip3 install drain3\n",
        "!pip3 install kafka-python\n",
        "!pip3 install redis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting drain3\n",
            "  Downloading https://files.pythonhosted.org/packages/03/df/cd2118d85b0401cd4b2cc555b97cd5a918ca551a035c77c853087321328e/drain3-0.9.3.tar.gz\n",
            "Collecting jsonpickle==1.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/77/a7/c2f527ddce3155ae9e008385963c2325cbfd52969f8b38efa2723e2af4af/jsonpickle-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cachetools==4.2.1 in /usr/local/lib/python3.7/dist-packages (from drain3) (4.2.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle==1.5.1->drain3) (3.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle==1.5.1->drain3) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle==1.5.1->drain3) (3.4.1)\n",
            "Building wheels for collected packages: drain3\n",
            "  Building wheel for drain3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for drain3: filename=drain3-0.9.3-cp37-none-any.whl size=16397 sha256=45206756c6e0a71f947270578298db4f7a0c4e53b9ebb00c7455890c77c0b6d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/eb/c8/0c42c729fa7f47040d8b9bc2e8359a96fee8a4b2bf442fd924\n",
            "Successfully built drain3\n",
            "Installing collected packages: jsonpickle, drain3\n",
            "Successfully installed drain3-0.9.3 jsonpickle-1.5.1\n",
            "Collecting kafka-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/68/dcb0db055309f680ab2931a3eeb22d865604b638acf8c914bedf4c1a0c8c/kafka_python-2.0.2-py2.py3-none-any.whl (246kB)\n",
            "\u001b[K     || 256kB 9.2MB/s \n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.0.2\n",
            "Collecting redis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     || 81kB 4.7MB/s \n",
            "\u001b[?25hInstalling collected packages: redis\n",
            "Successfully installed redis-3.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtfC_gaeGv8v"
      },
      "source": [
        "The cell below runs the drain3 program. Process the log info and generate clusters and a prefix tree of the log information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb0-1Ou6EYbx",
        "outputId": "862d6ecb-3a43-48d7-f053-6ae66bb2c3ee"
      },
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(message)s')\n",
        "\n",
        "template_miner = TemplateMiner()\n",
        "\n",
        "line_count = 0\n",
        "start_time = time.time()\n",
        "batch_start_time = start_time\n",
        "batch_size = 10000\n",
        "for i in range(len(array_of_logs2)):\n",
        "  for line in array_of_logs2[i]:\n",
        "      line = line.rstrip()\n",
        "      line = line.partition(\": \")[2]\n",
        "      result = template_miner.add_log_message(line)\n",
        "      line_count += 1\n",
        "      if line_count % batch_size == 0:\n",
        "          time_took = time.time() - batch_start_time\n",
        "          rate = batch_size / time_took\n",
        "          logger.info(f\"Processing line: {line_count}, rate {rate:.1f} lines/sec, \"\n",
        "                      f\"{len(template_miner.drain.clusters)} clusters so far.\")\n",
        "          batch_start_time = time.time()\n",
        "      if result[\"change_type\"] != \"none\":\n",
        "          result_json = json.dumps(result)\n",
        "          logger.info(f\"Input ({line_count}): \" + line)\n",
        "          logger.info(\"Result: \" + result_json)\n",
        "\n",
        "time_took = time.time() - start_time\n",
        "rate = line_count / time_took\n",
        "logger.info(f\"--- Done processing file. Total of {line_count} lines, rate {rate:.1f} lines/sec, \"\n",
        "            f\"{len(template_miner.drain.clusters)} clusters\")\n",
        "sorted_clusters = sorted(template_miner.drain.clusters, key=lambda it: it.size, reverse=True)\n",
        "for cluster in sorted_clusters:\n",
        "    logger.info(cluster)\n",
        "\n",
        "print(\"Prefix Tree:\")\n",
        "template_miner.drain.print_tree()\n",
        "\n",
        "template_miner.profiler.report(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Drain3 template miner\n",
            "Loading configuration from drain3.ini\n",
            "config file not found: drain3.ini\n",
            "Input (1): \n",
            "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 1, \"cluster_size\": 1, \"template_mined\": \"\", \"cluster_count\": 1}\n",
            "Processing line: 10000, rate 118121.1 lines/sec, 1 clusters so far.\n",
            "Processing line: 20000, rate 128148.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 30000, rate 144391.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 40000, rate 138624.0 lines/sec, 1 clusters so far.\n",
            "Processing line: 50000, rate 135807.4 lines/sec, 1 clusters so far.\n",
            "Processing line: 60000, rate 134098.0 lines/sec, 1 clusters so far.\n",
            "Processing line: 70000, rate 135249.5 lines/sec, 1 clusters so far.\n",
            "Processing line: 80000, rate 129770.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 90000, rate 134272.3 lines/sec, 1 clusters so far.\n",
            "Processing line: 100000, rate 140000.5 lines/sec, 1 clusters so far.\n",
            "Processing line: 110000, rate 132182.7 lines/sec, 1 clusters so far.\n",
            "Processing line: 120000, rate 121581.1 lines/sec, 1 clusters so far.\n",
            "Processing line: 130000, rate 135760.4 lines/sec, 1 clusters so far.\n",
            "Processing line: 140000, rate 146449.7 lines/sec, 1 clusters so far.\n",
            "Processing line: 150000, rate 131333.8 lines/sec, 1 clusters so far.\n",
            "Processing line: 160000, rate 136841.1 lines/sec, 1 clusters so far.\n",
            "Processing line: 170000, rate 141831.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 180000, rate 135012.7 lines/sec, 1 clusters so far.\n",
            "Processing line: 190000, rate 134598.9 lines/sec, 1 clusters so far.\n",
            "Processing line: 200000, rate 139246.6 lines/sec, 1 clusters so far.\n",
            "Processing line: 210000, rate 143068.3 lines/sec, 1 clusters so far.\n",
            "Processing line: 220000, rate 136838.9 lines/sec, 1 clusters so far.\n",
            "Processing line: 230000, rate 128096.9 lines/sec, 1 clusters so far.\n",
            "Processing line: 240000, rate 130335.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 250000, rate 136070.5 lines/sec, 1 clusters so far.\n",
            "Processing line: 260000, rate 137761.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 270000, rate 149725.5 lines/sec, 1 clusters so far.\n",
            "Processing line: 280000, rate 135413.3 lines/sec, 1 clusters so far.\n",
            "Processing line: 290000, rate 145574.4 lines/sec, 1 clusters so far.\n",
            "Processing line: 300000, rate 136784.0 lines/sec, 1 clusters so far.\n",
            "Processing line: 310000, rate 135356.5 lines/sec, 1 clusters so far.\n",
            "Processing line: 320000, rate 126879.8 lines/sec, 1 clusters so far.\n",
            "Processing line: 330000, rate 133886.1 lines/sec, 1 clusters so far.\n",
            "Processing line: 340000, rate 135834.7 lines/sec, 1 clusters so far.\n",
            "Processing line: 350000, rate 135627.4 lines/sec, 1 clusters so far.\n",
            "Processing line: 360000, rate 140495.8 lines/sec, 1 clusters so far.\n",
            "Processing line: 370000, rate 141877.3 lines/sec, 1 clusters so far.\n",
            "Processing line: 380000, rate 140146.5 lines/sec, 1 clusters so far.\n",
            "Processing line: 390000, rate 134580.3 lines/sec, 1 clusters so far.\n",
            "Processing line: 400000, rate 137391.1 lines/sec, 1 clusters so far.\n",
            "Processing line: 410000, rate 128744.9 lines/sec, 1 clusters so far.\n",
            "Processing line: 420000, rate 130143.1 lines/sec, 1 clusters so far.\n",
            "Processing line: 430000, rate 141732.0 lines/sec, 1 clusters so far.\n",
            "Processing line: 440000, rate 136977.4 lines/sec, 1 clusters so far.\n",
            "Processing line: 450000, rate 139593.8 lines/sec, 1 clusters so far.\n",
            "Processing line: 460000, rate 144790.4 lines/sec, 1 clusters so far.\n",
            "Processing line: 470000, rate 137773.4 lines/sec, 1 clusters so far.\n",
            "Processing line: 480000, rate 134854.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 490000, rate 131235.6 lines/sec, 1 clusters so far.\n",
            "Processing line: 500000, rate 130277.3 lines/sec, 1 clusters so far.\n",
            "Processing line: 510000, rate 137528.5 lines/sec, 1 clusters so far.\n",
            "Processing line: 520000, rate 147393.0 lines/sec, 1 clusters so far.\n",
            "Processing line: 530000, rate 130455.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 540000, rate 137564.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 550000, rate 145514.8 lines/sec, 1 clusters so far.\n",
            "Processing line: 560000, rate 138612.5 lines/sec, 1 clusters so far.\n",
            "Processing line: 570000, rate 139010.4 lines/sec, 1 clusters so far.\n",
            "Processing line: 580000, rate 136523.6 lines/sec, 1 clusters so far.\n",
            "Processing line: 590000, rate 121904.8 lines/sec, 1 clusters so far.\n",
            "Processing line: 600000, rate 133738.0 lines/sec, 1 clusters so far.\n",
            "Processing line: 610000, rate 137351.1 lines/sec, 1 clusters so far.\n",
            "Processing line: 620000, rate 139653.3 lines/sec, 1 clusters so far.\n",
            "Processing line: 630000, rate 126251.6 lines/sec, 1 clusters so far.\n",
            "Processing line: 640000, rate 143255.9 lines/sec, 1 clusters so far.\n",
            "Processing line: 650000, rate 147238.8 lines/sec, 1 clusters so far.\n",
            "Processing line: 660000, rate 126915.9 lines/sec, 1 clusters so far.\n",
            "Processing line: 670000, rate 135129.7 lines/sec, 1 clusters so far.\n",
            "Processing line: 680000, rate 129566.6 lines/sec, 1 clusters so far.\n",
            "Processing line: 690000, rate 135591.9 lines/sec, 1 clusters so far.\n",
            "Processing line: 700000, rate 147134.0 lines/sec, 1 clusters so far.\n",
            "Processing line: 710000, rate 136911.7 lines/sec, 1 clusters so far.\n",
            "Processing line: 720000, rate 133970.4 lines/sec, 1 clusters so far.\n",
            "Processing line: 730000, rate 135731.4 lines/sec, 1 clusters so far.\n",
            "Processing line: 740000, rate 147852.3 lines/sec, 1 clusters so far.\n",
            "Processing line: 750000, rate 142273.8 lines/sec, 1 clusters so far.\n",
            "Processing line: 760000, rate 138137.3 lines/sec, 1 clusters so far.\n",
            "Processing line: 770000, rate 131218.8 lines/sec, 1 clusters so far.\n",
            "Processing line: 780000, rate 111157.4 lines/sec, 1 clusters so far.\n",
            "Processing line: 790000, rate 150556.3 lines/sec, 1 clusters so far.\n",
            "Processing line: 800000, rate 127328.9 lines/sec, 1 clusters so far.\n",
            "Processing line: 810000, rate 139929.1 lines/sec, 1 clusters so far.\n",
            "Processing line: 820000, rate 137494.7 lines/sec, 1 clusters so far.\n",
            "Processing line: 830000, rate 147884.1 lines/sec, 1 clusters so far.\n",
            "Processing line: 840000, rate 144005.0 lines/sec, 1 clusters so far.\n",
            "Processing line: 850000, rate 128560.3 lines/sec, 1 clusters so far.\n",
            "Processing line: 860000, rate 131156.4 lines/sec, 1 clusters so far.\n",
            "Processing line: 870000, rate 139360.9 lines/sec, 1 clusters so far.\n",
            "Processing line: 880000, rate 138659.7 lines/sec, 1 clusters so far.\n",
            "Processing line: 890000, rate 145846.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 900000, rate 139418.8 lines/sec, 1 clusters so far.\n",
            "Processing line: 910000, rate 138572.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 920000, rate 136414.8 lines/sec, 1 clusters so far.\n",
            "Processing line: 930000, rate 138172.3 lines/sec, 1 clusters so far.\n",
            "Processing line: 940000, rate 117203.5 lines/sec, 1 clusters so far.\n",
            "Processing line: 950000, rate 139526.9 lines/sec, 1 clusters so far.\n",
            "Processing line: 960000, rate 138495.4 lines/sec, 1 clusters so far.\n",
            "Processing line: 970000, rate 140319.5 lines/sec, 1 clusters so far.\n",
            "Processing line: 980000, rate 142715.8 lines/sec, 1 clusters so far.\n",
            "Processing line: 990000, rate 143049.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 1000000, rate 155242.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 1010000, rate 130838.1 lines/sec, 1 clusters so far.\n",
            "Processing line: 1020000, rate 132089.1 lines/sec, 1 clusters so far.\n",
            "Processing line: 1030000, rate 141162.5 lines/sec, 1 clusters so far.\n",
            "Processing line: 1040000, rate 142479.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 1050000, rate 150973.1 lines/sec, 1 clusters so far.\n",
            "Processing line: 1060000, rate 142660.0 lines/sec, 1 clusters so far.\n",
            "Processing line: 1070000, rate 144624.2 lines/sec, 1 clusters so far.\n",
            "Processing line: 1080000, rate 132861.9 lines/sec, 1 clusters so far.\n",
            "Processing line: 1090000, rate 142139.8 lines/sec, 1 clusters so far.\n",
            "Input (1098448): us-east-1 (zones: us-east-1b us-east-1c)\n",
            "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 2, \"cluster_size\": 1, \"template_mined\": \"us-east-1 (zones: us-east-1b us-east-1c)\", \"cluster_count\": 2}\n",
            "Input (1098457): Error applying plan:\"\n",
            "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 3, \"cluster_size\": 1, \"template_mined\": \"Error applying plan:\\\"\", \"cluster_count\": 3}\n",
            "Input (1098460): 1 error occurred:\"\n",
            "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 4, \"cluster_size\": 1, \"template_mined\": \"1 error occurred:\\\"\", \"cluster_count\": 4}\n",
            "Input (1098461): [ERR]: Error building changeset: timeout while waiting for state to become \\'accepted\\' (timeout: 5m0s)\"\n",
            "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 5, \"cluster_size\": 1, \"template_mined\": \"[ERR]: Error building changeset: timeout while waiting for state to become \\\\'accepted\\\\' (timeout: 5m0s)\\\"\", \"cluster_count\": 5}\n",
            "Input (1098473): failed to generate asset \\\\\"Cluster\\\\\": failed to create cluster: failed to apply using Terraform\"\n",
            "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 6, \"cluster_size\": 1, \"template_mined\": \"failed to generate asset \\\\\\\\\\\"Cluster\\\\\\\\\\\": failed to create cluster: failed to apply using Terraform\\\"\", \"cluster_count\": 6}\n",
            "Input (1098480): some steps failed:\n",
            "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 7, \"cluster_size\": 1, \"template_mined\": \"some steps failed:\", \"cluster_count\": 7}\n",
            "Input (1098481): step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-6jxq6mqt/e2e-aws-serial failed after 18m49s (failed containers: setup): ContainerFailed one or more containers exited\n",
            "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 8, \"cluster_size\": 1, \"template_mined\": \"step e2e-aws-serial failed: template pod \\\"e2e-aws-serial\\\" failed: the pod ci-op-6jxq6mqt/e2e-aws-serial failed after 18m49s (failed containers: setup): ContainerFailed one or more containers exited\", \"cluster_count\": 8}\n",
            "Input (1098600): [us-west-1--26]\n",
            "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 9, \"cluster_size\": 1, \"template_mined\": \"[us-west-1--26]\", \"cluster_count\": 9}\n",
            "Input (1098606): Failed to patch the ci-op-n2904x8n namespace to update the ci.openshift.io/active annotation: namespaces \"ci-op-n2904x8n\" is forbidden: User \"system:serviceaccount:ci:ci-operator\" cannot patch namespaces in the namespace \"ci-op-n2904x8n\": no RBAC policy matched\n",
            "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 10, \"cluster_size\": 1, \"template_mined\": \"Failed to patch the ci-op-n2904x8n namespace to update the ci.openshift.io/active annotation: namespaces \\\"ci-op-n2904x8n\\\" is forbidden: User \\\"system:serviceaccount:ci:ci-operator\\\" cannot patch namespaces in the namespace \\\"ci-op-n2904x8n\\\": no RBAC policy matched\", \"cluster_count\": 10}\n",
            "Input (1098613): us-west-1--26\n",
            "Result: {\"change_type\": \"cluster_created\", \"cluster_id\": 11, \"cluster_size\": 1, \"template_mined\": \"us-west-1--26\", \"cluster_count\": 11}\n",
            "--- Done processing file. Total of 1098616 lines, rate 133104.5 lines/sec, 11 clusters\n",
            "ID=1     : size=1098601   : \n",
            "ID=2     : size=2         : us-east-1 (zones: us-east-1b us-east-1c)\n",
            "ID=3     : size=2         : Error applying plan:\"\n",
            "ID=4     : size=2         : 1 error occurred:\"\n",
            "ID=5     : size=2         : [ERR]: Error building changeset: timeout while waiting for state to become \\'accepted\\' (timeout: 5m0s)\"\n",
            "ID=6     : size=2         : failed to generate asset \\\\\"Cluster\\\\\": failed to create cluster: failed to apply using Terraform\"\n",
            "ID=7     : size=1         : some steps failed:\n",
            "ID=8     : size=1         : step e2e-aws-serial failed: template pod \"e2e-aws-serial\" failed: the pod ci-op-6jxq6mqt/e2e-aws-serial failed after 18m49s (failed containers: setup): ContainerFailed one or more containers exited\n",
            "ID=9     : size=1         : [us-west-1--26]\n",
            "ID=10    : size=1         : Failed to patch the ci-op-n2904x8n namespace to update the ci.openshift.io/active annotation: namespaces \"ci-op-n2904x8n\" is forbidden: User \"system:serviceaccount:ci:ci-operator\" cannot patch namespaces in the namespace \"ci-op-n2904x8n\": no RBAC policy matched\n",
            "ID=11    : size=1         : us-west-1--26\n",
            "Prefix Tree:\n",
            "<root>\n",
            "\t<0>\n",
            "\t<4>\n",
            "\t\t<*>\n",
            "\t<3>\n",
            "\t\tError\n",
            "\t\t<*>\n",
            "\t\tsome\n",
            "\t<14>\n",
            "\t\t[ERR]:\n",
            "\t\tfailed\n",
            "\t<22>\n",
            "\t\tstep\n",
            "\t<1>\n",
            "\t<28>\n",
            "\t\tFailed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2QaVqCj4NIF"
      },
      "source": [
        "This cell removes dates and timestamps, whitespace and keeps the stems of words\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGDZGWSf32JQ"
      },
      "source": [
        "# ******* PARKER *******\n",
        "failures = []\n",
        "successes = []\n",
        "# word order for matrix: 0: fail, 1: error, 2: success, 3: run, 4: crashloopbackoff\n",
        "word_matrix = []\n",
        "for log in logs:\n",
        "  tmp = ' '.join(log)\n",
        "  tmp = TextBlob(tmp)\n",
        "  fail = tmp.word_counts['fail']\n",
        "  error = tmp.word_counts['error']\n",
        "  success = tmp.word_counts['success']\n",
        "  run = tmp.word_counts['run']\n",
        "  crash = tmp.word_counts['crashloopbackoff']\n",
        "  word_matrix.append([fail, error, success, run, crash])\n",
        "\n",
        "# remove noise\n",
        "word_matrix = word_matrix[2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHllUg9vwX_H"
      },
      "source": [
        "This cell creates a matrix of keywords. each row is a single log entry and each column is a specific keyword"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xrj2maH9wVJl",
        "outputId": "d2d03a5a-9cc5-44c6-b0f7-ee5bb0dd25e1"
      },
      "source": [
        "# ******* PARKER *******\n",
        "import math\n",
        "# Latent Semantic Analysis approach\n",
        "\n",
        "num_of_docs = [0 for i in range(len(word_matrix[0]))]\n",
        "for i in range(len(word_matrix)):\n",
        "  for j in range(len(word_matrix[i])):\n",
        "    if word_matrix[i][j] != 0:\n",
        "      num_of_docs[j] += 1\n",
        "for i in range(len(word_matrix)):\n",
        "  for j in range(len(word_matrix[i])):\n",
        "    tf = word_matrix[i][j]\n",
        "    idf = math.log(len(word_matrix) / num_of_docs[j])\n",
        "    word_matrix[i][j] = tf * idf\n",
        "\n",
        "# TODO: try to implement anomaly detection with word matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.023530497410194036, 0.0, 0.05848019881595618, 0.0, 0.0], [0.023530497410194036, 0.0, 0.05848019881595618, 0.0, 0.0], [0.023530497410194036, 0.0, 0.05848019881595618, 0.0, 0.0], [0.023530497410194036, 0.0, 0.05848019881595618, 0.0, 0.0], [0.023530497410194036, 0.0, 0.05848019881595618, 0.0, 0.0], [0.21177447669174632, 0.41928699106748313, 0.023392079526382472, 0.0, 0.0], [0.07059149223058211, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.07059149223058211, 0.5390832742296211, 0.2924009940797809, 0.0, 0.0], [0.07059149223058211, 0.4791851326485521, 0.30409703384297215, 0.0, 0.39390428570708846], [0.07059149223058211, 0.4791851326485521, 0.30409703384297215, 0.0, 0.39390428570708846], [0.07059149223058211, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.0, 0.059898141581069014, 0.0, 0.0, 0.0], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.04706099482038807, 0.5390832742296211, 0.3157930736061634, 0.0, 0.0], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.07059149223058211, 0.29949070790534504, 0.1286564373951036, 0.0, 2.363425714242531], [0.04706099482038807, 0.5390832742296211, 0.3157930736061634, 0.0, 0.0], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.04706099482038807, 0.41928699106748313, 0.2924009940797809, 0.0, 0.7878085714141769], [0.04706099482038807, 0.41928699106748313, 0.3157930736061634, 0.0, 0.7878085714141769], [0.04706099482038807, 0.29949070790534504, 0.1286564373951036, 0.0, 2.363425714242531], [0.04706099482038807, 0.4791851326485521, 0.3157930736061634, 0.0, 0.39390428570708846], [0.04706099482038807, 0.4791851326485521, 0.37427327242211955, 0.0, 0.39390428570708846], [0.04706099482038807, 0.6588795573917592, 0.4093613917116933, 0.0, 0.0], [0.04706099482038807, 0.4791851326485521, 0.3157930736061634, 0.0, 0.39390428570708846], [0.04706099482038807, 0.4791851326485521, 0.3157930736061634, 0.0, 0.39390428570708846], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.04706099482038807, 0.41928699106748313, 0.3157930736061634, 0.0, 0.7878085714141769], [0.04706099482038807, 0.5989814158106901, 0.4210574314748845, 0.0, 0.39390428570708846], [0.04706099482038807, 0.29949070790534504, 0.1286564373951036, 0.0, 2.363425714242531], [0.04706099482038807, 0.4791851326485521, 0.3157930736061634, 0.0, 0.39390428570708846], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.04706099482038807, 0.4791851326485521, 0.3157930736061634, 0.0, 0.39390428570708846], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.04706099482038807, 0.5390832742296211, 0.3508811928957371, 0.0, 0.39390428570708846], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.04706099482038807, 0.29949070790534504, 0.1286564373951036, 0.0, 2.363425714242531], [0.0, 0.4791851326485521, 0.4795376302908407, 0.0, 0.0], [0.04706099482038807, 0.5390832742296211, 0.05848019881595618, 0.0, 0.0], [0.8470979067669853, 1.8568423890131394, 0.198832675974251, 0.0, 2.363425714242531], [0.04706099482038807, 0.29949070790534504, 0.3157930736061634, 0.0, 1.5756171428283539], [3.0119036685048366, 6.289304866012246, 2.818745582929088, 0.0, 0.0], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.04706099482038807, 0.41928699106748313, 0.2924009940797809, 0.0, 0.7878085714141769], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [1.270646860150478, 4.552258760161245, 0.36257723265892833, 0.0, 0.39390428570708846], [0.04706099482038807, 0.5989814158106901, 0.37427327242211955, 0.0, 0.0], [2.729537699582508, 2.7553145127291745, 2.6666970660076017, 0.0, 0.39390428570708846], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.04706099482038807, 0.5390832742296211, 0.37427327242211955, 0.0, 0.7878085714141769], [0.04706099482038807, 0.41928699106748313, 0.30409703384297215, 0.0, 0.7878085714141769], [0.04706099482038807, 0.41928699106748313, 0.30409703384297215, 0.0, 0.7878085714141769], [0.04706099482038807, 0.5390832742296211, 0.035088119289573706, 0.0, 0.0], [0.04706099482038807, 0.5989814158106901, 0.36257723265892833, 0.0, 0.0], [0.04706099482038807, 0.41928699106748313, 0.2924009940797809, 0.0, 0.7878085714141769], [0.21177447669174632, 0.41928699106748313, 0.023392079526382472, 0.0, 0.0], [0.04706099482038807, 0.41928699106748313, 0.3157930736061634, 0.0, 0.7878085714141769], [0.04706099482038807, 0.5989814158106901, 0.36257723265892833, 0.0, 0.0], [0.04706099482038807, 0.29949070790534504, 0.1286564373951036, 0.0, 2.363425714242531], [0.04706099482038807, 0.5390832742296211, 0.3157930736061634, 0.0, 0.0], [0.04706099482038807, 0.5390832742296211, 0.035088119289573706, 0.0, 0.0], [0.04706099482038807, 0.41928699106748313, 0.2924009940797809, 0.0, 0.7878085714141769], [0.04706099482038807, 0.4791851326485521, 0.3157930736061634, 0.0, 0.39390428570708846], [0.04706099482038807, 0.5390832742296211, 0.3157930736061634, 0.0, 0.0], [0.04706099482038807, 0.7187776989728282, 0.4093613917116933, 0.0, 0.0], [0.04706099482038807, 0.5390832742296211, 0.3157930736061634, 0.0, 0.7878085714141769], [1.5059518342524183, 6.588795573917592, 0.3274891133693546, 0.0, 0.39390428570708846], [0.14118298446116423, 0.4791851326485521, 0.3157930736061634, 0.0, 0.39390428570708846], [1.5765433264830004, 3.953277344350555, 0.28070495431658965, 0.0, 1.5756171428283539], [0.07059149223058211, 0.4791851326485521, 0.3157930736061634, 0.0, 0.39390428570708846], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.04706099482038807, 0.4791851326485521, 0.3157930736061634, 0.0, 0.39390428570708846], [0.04706099482038807, 0.6588795573917592, 0.035088119289573706, 0.0, 0.0], [0.04706099482038807, 0.4791851326485521, 0.3157930736061634, 0.0, 0.39390428570708846], [0.04706099482038807, 0.41928699106748313, 0.035088119289573706, 0.0, 0.0], [0.04706099482038807, 0.5989814158106901, 0.36257723265892833, 0.0, 0.0], [0.04706099482038807, 0.41928699106748313, 0.2924009940797809, 0.0, 0.7878085714141769], [0.04706099482038807, 0.4791851326485521, 0.3157930736061634, 0.0, 0.39390428570708846], [0.04706099482038807, 0.5989814158106901, 0.36257723265892833, 0.0, 0.0], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.04706099482038807, 0.5390832742296211, 0.2924009940797809, 0.0, 0.0], [0.04706099482038807, 0.4791851326485521, 0.3157930736061634, 0.0, 0.39390428570708846], [0.04706099482038807, 0.29949070790534504, 0.2924009940797809, 0.0, 1.5756171428283539], [0.04706099482038807, 0.29949070790534504, 0.1286564373951036, 0.0, 2.363425714242531], [0.04706099482038807, 0.4791851326485521, 0.3157930736061634, 0.0, 0.39390428570708846]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V_DFewg4cCn"
      },
      "source": [
        "This cell is determining the number of keywords in each log and plotting them.\n",
        "TODO: look at logs and find more keywords, instead of failures and successes make each word an entry in a matrix and use SVD to plot similarities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "Did5do5Ga_TQ",
        "outputId": "3c5cafb4-2d0e-41e7-8157-3d06f19c9cc0"
      },
      "source": [
        "# ******* NINGXIAO *******\n",
        "# find events in log, display frequecy of each event in a bar chart\n",
        "import spacy\n",
        "s = \"\"\n",
        "for log in logs:\n",
        "    temp = \"\"\n",
        "    temp = temp.join(log)\n",
        "    s += temp\n",
        "# Error Message: Text of length 5496377 exceeds maximum of 1000000.\n",
        "s = s[0:100000]\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(s)\n",
        "dictionary = {};\n",
        "for token in doc:\n",
        "    if token.pos_ == 'VERB':\n",
        "        if token.text not in dictionary:\n",
        "            dictionary[token.text] = 1\n",
        "        else:\n",
        "            dictionary[token.text] += 1\n",
        "    #print(token.text, toekn.pos_)\n",
        "\n",
        "#print(dictionary)\n",
        "\n",
        "keys = list(dictionary.keys())\n",
        "values = list(dictionary.values())\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9.2, 7))  \n",
        "ax.barh(keys,values)\n",
        "plt.title('Verbs in logs')\n",
        "plt.ylabel('Verbs')\n",
        "plt.xlabel('Frequency')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 662.4x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1oosA55NoQZ"
      },
      "source": [
        "The image above shows the verbs and their frequency appear in the tokenized log text. It indicates what event and how often it happens in the log."
      ]
    }
  ]
}