{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Codes",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-SchSDwNCgQ"
      },
      "source": [
        "<H1>Red Hat NLP Spark Project</H1>\n",
        "\n",
        "This notebook uses Red Hat's OpenShift data logs and processes them using Drain3. \n",
        "\n",
        "The first thing this notebook does is webscrape the OpenShift logs and installs dependencies. After, the logs are then cleaned by removing leading tags, dates, timestamps and other unique identifiers. This is done manually because drain does not do this consistently for all logs. Finally the parsed logs are then fed into Drain3 and then put into a dictionary sorted by cluster ID.\n",
        "\n",
        "The final step that needs to be done is to sift out the pass and fail logs based on their cluster ID and visualize our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5cWx89XM9t6",
        "outputId": "1da3d5ce-f142-45a5-81d0-795b10385675"
      },
      "source": [
        "!pip3 install drain3\n",
        "!pip3 install kafka-python\n",
        "!pip3 install redis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting drain3\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/1f/09c4ee7d648b66dda7dc8426e9cf9faeee71e544a2e7700d3d959e5cca59/drain3-0.9.5.tar.gz\n",
            "Collecting jsonpickle==1.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/77/a7/c2f527ddce3155ae9e008385963c2325cbfd52969f8b38efa2723e2af4af/jsonpickle-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cachetools==4.2.1 in /usr/local/lib/python3.7/dist-packages (from drain3) (4.2.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle==1.5.1->drain3) (3.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle==1.5.1->drain3) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle==1.5.1->drain3) (3.7.4.3)\n",
            "Building wheels for collected packages: drain3\n",
            "  Building wheel for drain3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for drain3: filename=drain3-0.9.5-cp37-none-any.whl size=18179 sha256=cf176d3168dd7367def207718df165164ac513ab95e148373b6fd8381816fcdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/a1/08/125223534f199f0db7a435b437015f83abba341ef0f4f6c64f\n",
            "Successfully built drain3\n",
            "Installing collected packages: jsonpickle, drain3\n",
            "Successfully installed drain3-0.9.5 jsonpickle-1.5.1\n",
            "Collecting kafka-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/68/dcb0db055309f680ab2931a3eeb22d865604b638acf8c914bedf4c1a0c8c/kafka_python-2.0.2-py2.py3-none-any.whl (246kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 5.4MB/s \n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.0.2\n",
            "Collecting redis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.5MB/s \n",
            "\u001b[?25hInstalling collected packages: redis\n",
            "Successfully installed redis-3.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq5k5nqiC02i"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import dateutil\n",
        "from dateutil import parser\n",
        "import textblob\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import drain3\n",
        "from drain3 import TemplateMiner\n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "from drain3.kafka_persistence import KafkaPersistence\n",
        "import re\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import subprocess\n",
        "import time\n",
        "import spacy\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "import os, os.path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq11lna0RAee"
      },
      "source": [
        "#<H3>Dataset</H3>\n",
        "\n",
        "Gather log files from OpenShift and save them to shared google drive, Red Hat & BU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st2dPqa-UuWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb017d8-1c11-4d75-8042-c7344fac395e"
      },
      "source": [
        "#enable to read files from the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3plLsNUbQdBo"
      },
      "source": [
        "with open ('/content/drive/MyDrive/RedHat_BU/log_file.ob', 'rb') as fp:\n",
        "    logs = pickle.load(fp)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay67SWvwT9bv"
      },
      "source": [
        "with open ('/content/drive/MyDrive/RedHat_BU/label_file.ob', 'rb') as fp:\n",
        "    labels = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUb0Z1vsQo6y",
        "outputId": "4acbfd1b-18c4-478a-84f4-54110d880af7"
      },
      "source": [
        "print(len(logs),len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "416 416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW1GWsgvbOUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ee2c0e-bec2-4a2c-a733-24ab9fbc7b2e"
      },
      "source": [
        "'''\n",
        "folder_path = f'/content/drive/MyDrive/RedHat_BU/log_files_1' #Hong Xin, Tianze Shan\n",
        "dirListing = os.listdir(folder_path)\n",
        "print(\"Number of log files: \", len(dirListing))\n",
        "\n",
        "for val in dirListing:\n",
        "    # Hong's google drive \n",
        "    df = open(f'/content/drive/MyDrive/RedHat_BU/log_files_1/{val}','r'). #Hong Xin, Tianze Shan\n",
        "    lines_list = df.read().split()\n",
        "    lines_array = np.array(lines_list)\n",
        "#print(type(lines_array))\n",
        "\n",
        "count = 0\n",
        "log_line=[]\n",
        "\n",
        "# Strips the newline character\n",
        "for line in lines_array:\n",
        "    count += 1\n",
        "    #print(\"Line{}: {}\".format(count, line.strip()))\n",
        "    log_line.append(line.strip())\n",
        "len(log_line)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3889"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heDQxXYAQzcU"
      },
      "source": [
        "#<H4>Assigning Labels to Logs </H4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S056NuDx71Wt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9c5ebf-503e-4713-ab57-9326345ed783"
      },
      "source": [
        "# 1 := success, 0 := fail\n",
        "y = []\n",
        "count_0 = 0\n",
        "count_1 = 0\n",
        "for label in labels:\n",
        "  if label == 'SUCCESS':\n",
        "    y.append(1)\n",
        "    count_1 = count_1 + 1\n",
        "  else:\n",
        "    y.append(0)\n",
        "    count_0 = count_0 + 1\n",
        "print(\"y = \", y)\n",
        "print(\"number of success = \", count_1)\n",
        "print(\"number of fail = \", count_0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y =  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "number of success =  276\n",
            "number of fail =  140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb5gAOrHQ67U"
      },
      "source": [
        "<H4>Splitting log lines for vectorization</H4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuzkkC0dOamf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd975180-1b03-43d0-a93e-8e9dde2a105b"
      },
      "source": [
        "vocab = {}\n",
        "i = 0\n",
        "for log in logs:\n",
        "  log = str(log).split(\",\")\n",
        "  for line in log:\n",
        "    line = line.replace(\"'\",\"\").replace(\" \",\"\").replace(\"\\\"\",\"\")\n",
        "    if line not in vocab:\n",
        "      vocab[line] = i\n",
        "      i+=1\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d__aAxaiXGmG",
        "outputId": "cc71a0d8-89ba-4a17-a8a8-73d991a8c368"
      },
      "source": [
        "print(list(vocab.items())[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('20/10/2622:30:59ci-operatorversionv20201026-a4c1a36', 0), ('2020/10/2622:30:59Nosourcedefined', 1), ('2020/10/2622:30:59Resolvedreleaselatesttoregistry.svc.ci.openshift.org/ocp/release:4.5-ci', 2), ('2020/10/2622:30:59Usingnamespacehttps://console.svc.ci.openshift.org/k8s/cluster/projects/ci-op-4vpn65p8', 3), ('2020/10/2622:30:59Running[release-inputs]', 4), ('e2e-aws', 5), ('[images]', 6), ('[release:latest]', 7), ('2020/10/2622:31:00Creatingnamespaceci-op-4vpn65p8', 8), ('2020/10/2622:31:00Settinguppipelineimagestreamforthetest', 9), ('2020/10/2622:31:00Createdsecrete2e-aws-cnv-cluster-profile', 10), ('2020/10/2622:31:00Createdsecretpull-secret', 11), ('2020/10/2622:31:00CreatedPDBforpodswithopenshift.io/build.namelabel', 12), ('2020/10/2622:31:00CreatedPDBforpodswithcreated-by-cilabel', 13), ('2020/10/2622:31:00Taggedsharedimagesfromocp/4.5:${component}', 14), ('imageswillbepullablefromregistry.svc.ci.openshift.org/ci-op-4vpn65p8/stable:${component}', 15), ('2020/10/2622:31:04Importingreleaseimagelatest', 16), ('2020/10/2622:31:04Executingpodrelease-images-latest-cli', 17), ('2020/10/2622:31:09Executingpodrelease-images-latest', 18), ('2020/10/2622:32:14Importedrelease4.5.0-0.ci-2020-10-25-032658createdat2020-10-2503:27:31+0000UTCwith110imagestotagrelease:latest', 19), ('2020/10/2622:32:14Acquiringleaseforaws-quota-slice', 20), ('2020/10/2622:32:14Acquiredleasedbccac7a-9b48-4a02-bc5a-771a7ecb5586foraws-quota-slice', 21), ('2020/10/2622:32:14Executingtemplatee2e-aws', 22), ('2020/10/2622:32:14Creatingorrestartingtemplateinstance', 23), ('2020/10/2622:32:14Templateinstancee2e-awsalreadydeleted', 24), ('donotneedtowaitanylonger', 25), ('2020/10/2622:32:14Waitingfortemplateinstancetobeready', 26), ('2020/10/2622:32:16Runningpode2e-aws-cnv', 27), ('2020/10/2622:32:18Containercliinpode2e-aws-cnvcompletedsuccessfully', 28), ('2020/10/2622:32:20Containercli-testsinpode2e-aws-cnvcompletedsuccessfully', 29), ('2020/10/2623:02:26Containersetupinpode2e-aws-cnvcompletedsuccessfully', 30), ('Waitingforsetuptofinish...', 31), ('secret/supportcreated', 32), ('which:nodockerin(/tmp/shared:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin:/go/bin)', 33), ('./hack/deploy.sh', 34), ('+sourcehack/common.sh', 35), ('++set-e', 36), ('++sourcehack/defaults', 37), ('+++++dirname\\\\\\\\hack/defaults[0]\\\\\\\\', 38), ('++++readlink-ehack/../', 39), ('+++PROJECT_ROOT=/go/src/github.com/kubevirt/hyperconverged-cluster-operator', 40), ('+++source/go/src/github.com/kubevirt/hyperconverged-cluster-operator/hack/config', 41), ('++++KUBEVIRT_VERSION=v0.29.2', 42), ('++++CDI_VERSION=v1.18.0', 43), ('++++NETWORK_ADDONS_VERSION=0.38.0', 44), ('++++SSP_VERSION=v1.0.35', 45), ('++++NMO_VERSION=v0.6.0', 46), ('++++HPPO_VERSION=v0.4.3', 47), ('++++HPP_VERSION=v0.4.0', 48), ('++++CONVERSION_CONTAINER_VERSION=v2.0.0', 49), ('++++VMWARE_CONTAINER_VERSION=v2.0.0-3', 50), ('++++VM_IMPORT_VERSION=v0.0.2', 51), ('++++CONTAINER_REGISTRY=quay.io/kubevirt', 52), ('++++mktemp-d', 53), ('+++TEMP_DIR=/tmp/tmp.IIHwaIRhKh', 54), ('+++WAIT_TIMEOUT=450s', 55), ('+++CDI_CONTAINER_REGISTRY=docker.io/kubevirt', 56), ('+++KUBEVIRT_CONTAINER_REGISTRY=docker.io/kubevirt', 57), ('+++NETWORK_ADDONS_CONTAINER_REGISTRY=quay.io/kubevirt', 58), ('+++SSP_CONTAINER_REGISTRY=quay.io/fromani', 59), ('+++CDI_OPERATOR_NAME=cdi-operator', 60), ('++++basenamedocker.io/kubevirt', 61), ('+++CDI_DOCKER_PREFIX=kubevirt', 62), ('+++CONTROLLER_IMAGE=cdi-controller', 63), ('+++IMPORTER_IMAGE=cdi-importer', 64), ('+++CLONER_IMAGE=cdi-cloner', 65), ('+++APISERVER_IMAGE=cdi-apiserver', 66), ('+++UPLOADPROXY_IMAGE=cdi-uploadproxy', 67), ('+++UPLOADSERVER_IMAGE=cdi-uploadserver', 68), ('+++NETWORK_ADDONS_MULTUS_IMAGE=', 69), ('+++NETWORK_ADDONS_LINUX_BRIDGE_CNI_IMAGE=', 70), ('+++NETWORK_ADDONS_LINUX_BRIDGE_MARKER_IMAGE=', 71), ('+++NETWORK_ADDONS_KUBEMACPOOL_IMAGE=', 72), ('+++NETWORK_ADDONS_NMSTATE_HANDLER_IMAGE=', 73), ('+++NETWORK_ADDONS_OVS_CNI_PLUGIN_IMAGE=', 74), ('+++NETWORK_ADDONS_OVS_CNI_MARKER_IMAGE=', 75), ('+++VM_IMPORT_CONTAINER_REGISTRY=quay.io/kubevirt', 76), ('+++VM_IMPORT_IMAGE=vm-import-operator', 77), ('+++echodocker.io/kubevirt', 78), ('+++grepbrew', 79), ('+++OPERATOR_MANIFESTS=\\\\\\\\https://github.com/kubevirt/kubevirt/releases/download/v0.29.2/kubevirt-operator.yaml', 80), ('https://github.com/kubevirt/containerized-data-importer/releases/download/v1.18.0/cdi-operator.yaml.j2', 81), ('https://github.com/kubevirt/cluster-network-addons-operator/releases/download/0.38.0/network-addons-config.crd.yaml', 82), ('https://github.com/kubevirt/cluster-network-addons-operator/releases/download/0.38.0/namespace.yaml', 83), ('https://github.com/kubevirt/cluster-network-addons-operator/releases/download/0.38.0/operator.yaml', 84), ('https://github.com/MarSik/kubevirt-ssp-operator/releases/download/v1.0.35/kubevirt-ssp-operator-crd.yaml', 85), ('https://github.com/MarSik/kubevirt-ssp-operator/releases/download/v1.0.35/kubevirt-ssp-operator.yaml', 86), ('https://github.com/kubevirt/vm-import-operator/releases/download/v0.0.2/operator.yaml\\\\\\\\', 87), ('+++OPERATOR_CRS=\\\\\\\\https://github.com/kubevirt/kubevirt/releases/download/v0.29.2/kubevirt-cr.yaml', 88), ('https://github.com/kubevirt/containerized-data-importer/releases/download/v1.18.0/cdi-operator-cr.yaml', 89), ('https://github.com/kubevirt/cluster-network-addons-operator/releases/download/0.38.0/network-addons-config-example.cr.yaml', 90), ('https://github.com/MarSik/kubevirt-ssp-operator/releases/download/v1.0.35/kubevirt-ssp-operator-cr.yaml', 91), ('https://github.com/kubevirt/vm-import-operator/releases/download/v0.0.2/vmimportconfig_cr.yaml\\\\\\\\', 92), ('++sourcecluster/kubevirtci.sh', 93), ('+++exportKUBEVIRT_PROVIDER=k8s-1.17', 94), ('+++KUBEVIRT_PROVIDER=k8s-1.17', 95), ('+++KUBEVIRTCI_VERSION=9d224d0c22e9ed2ca7588ccf3a258d82e160b195', 96), ('+++KUBEVIRTCI_PATH=/go/src/github.com/kubevirt/hyperconverged-cluster-operator/_kubevirtci', 97), ('++CDI_OPERATOR_URL=https://github.com/kubevirt/containerized-data-importer/releases/download/v1.18.0/cdi-operator.yaml', 98), ('++KUBEVIRT_OPERATOR_URL=https://github.com/kubevirt/kubevirt/releases/download/v0.29.2/kubevirt-operator.yaml', 99)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTYgLuEaRHiX"
      },
      "source": [
        "#<H2>Classifying logs without drain and parsing</H2>\n",
        "\n",
        "Here begins the process of vectorizing each log line and then classifying it as a pass or fail log accordingly. If this cell has an error, run the cell that removes newline characters and the leading b' tag from the logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUhrDxlNO7JS"
      },
      "source": [
        "vectorizer = TfidfVectorizer(vocabulary = vocab)\n",
        "X = vectorizer.fit_transform(logs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukvCx6NDUhU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eeed9c2-7f67-4efc-a912-0922c89efdbc"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(416, 39583)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPerlihUQUkT"
      },
      "source": [
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, train_size= 0.8, random_state=1)\n",
        "sss.get_n_splits(X, y)\n",
        "y = np.array(y)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjhiyTqNr9Gs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8450f4c2-e886-4759-d192-131e0e140030"
      },
      "source": [
        "#tfidf\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(332, 39583)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJcVgQrOVeI-"
      },
      "source": [
        "# Classifying logs without Drain with XGB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMY0yIo1zt4d"
      },
      "source": [
        "#training accuracy calculator\n",
        "def accuracy_cal(prediction, Testy):\n",
        "    count = 0\n",
        "    for i in range(len(prediction)):\n",
        "        if prediction[i] == int(Testy[i]):\n",
        "            count+=1\n",
        "    accuracy = round(count / len(prediction) * 100, 4\n",
        "    )\n",
        "    print(accuracy, \"%\")\n",
        "    return accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAm7oirNUc9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6635cb5-ea26-4ba1-9fc3-50a45f4f5ba3"
      },
      "source": [
        "model_XGB = XGBClassifier(scale_pos_weight=99).fit(X_train, y_train)\n",
        "y_test_predictions = model_XGB.predict(X_test)\n",
        "accuracy = accuracy_cal(y_test_predictions, y_test)\n",
        "\n",
        "\n",
        "print(\"=== Confusion Matrix ===\")\n",
        "print(confusion_matrix(y_test, y_test_predictions))\n",
        "\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_test, y_test_predictions))\n",
        "\n",
        "print(\"=== Training accuracy ===\")\n",
        "print(accuracy)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95.2381 %\n",
            "=== Confusion Matrix ===\n",
            "[[24  4]\n",
            " [ 0 56]]\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.86      0.92        28\n",
            "           1       0.93      1.00      0.97        56\n",
            "\n",
            "    accuracy                           0.95        84\n",
            "   macro avg       0.97      0.93      0.94        84\n",
            "weighted avg       0.96      0.95      0.95        84\n",
            "\n",
            "=== Training accuracy ===\n",
            "95.2381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emguFkVCWH-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a19f1f-f9c7-45c7-e03a-940f5363b65a"
      },
      "source": [
        "print(\"Precision score: {}\".format(precision_score(y_test, y_test_predictions)))\n",
        "print(\"Recall score: {}\".format(recall_score(y_test, y_test_predictions)))\n",
        "print(\"F1 Score: {}\".format(f1_score(y_test, y_test_predictions)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision score: 0.9333333333333333\n",
            "Recall score: 1.0\n",
            "F1 Score: 0.9655172413793104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UBShV3kRskG"
      },
      "source": [
        "#<H3>Log Parsing</H3>\n",
        "\n",
        "Logs need to be parsed before processing into drain. This is necessary because drain does not parse consistently for every log.\n",
        "\n",
        "Things to be parsed:\n",
        "<ul>\n",
        "  <li>Dates</li>\n",
        "  <li>Timestamps</li>\n",
        "  <li>Newline characters</li>\n",
        "  <li>Version numbers</li>\n",
        "  <li>Namespace IDs</li>\n",
        "  <li>URLs</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDM9A5aB2wSl"
      },
      "source": [
        "original_log = logs[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRyol0yU4HP_"
      },
      "source": [
        "#  helper function detecting if a string is a date / timestamp\n",
        "def is_date(str):\n",
        "  try:\n",
        "    dateutil.parser.parse(str)\n",
        "    return True\n",
        "  except:\n",
        "    return False\n",
        "for i in range(len(logs)):\n",
        " # splitting each section as its own index (for parsing)\n",
        "  logs[i] = str(logs[i]).split(' ')\n",
        "  for j in range(len(logs[i])):\n",
        "    if is_date(logs[i][j]) == True:\n",
        "      logs[i][j] = ''\n",
        "  logs[i] = list(filter(lambda x: x != '', logs[i]))\n",
        "  logs[i] = ' '.join(logs[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSINSnQeDqZ9"
      },
      "source": [
        "This code cell removes timestamps and dates in order to mitigate unique identifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8a8Nyc76PQo"
      },
      "source": [
        "for index in range(len(logs)):\n",
        "  # removing version number\n",
        "  if \"version\" in logs[index]:\n",
        "    tmp = logs[index].split(\"version\",1)\n",
        "    tmp_1 = tmp[0]\n",
        "    tmp_2 = tmp[1].split()\n",
        "    tmp_2 = tmp_2[1:]\n",
        "    tmp = ''.join(tmp_1) + ' '.join(tmp_2)\n",
        "    logs[index] = tmp\n",
        "\n",
        "  # removing creating namespace ID\n",
        "  if \"Creating namespace\" in logs[index]:\n",
        "    tmp = logs[index].split(\"Creating namespace\", 1)\n",
        "    tmp_1 = tmp[0]\n",
        "    tmp_2 = tmp[1].split()\n",
        "    tmp_2 = tmp_2[1:]\n",
        "    tmp = ''.join(tmp_1) + ' '.join(tmp_2)\n",
        "    logs[index] = tmp\n",
        "\n",
        "  # removing using namespace ID\n",
        "  if \"Using namespace\" in logs[index]:\n",
        "    tmp = logs[index].split(\"Using namespace\", 1)\n",
        "    tmp_1 = tmp[0]\n",
        "    tmp_2 = tmp[1].split()\n",
        "    tmp_2 = tmp_2[1:]\n",
        "    tmp = ''.join(tmp_1) + ' '.join(tmp_2)\n",
        "    logs[index] = tmp\n",
        "\n",
        "  # removing Imported release stamp\n",
        "  if \"Imported release\" in logs[index]:\n",
        "    tmp = logs[index].split(\"Imported release\", 1)\n",
        "    tmp_1 = tmp[0]\n",
        "    tmp_2 = tmp[1].split()\n",
        "    tmp_2 = tmp_2[1:]\n",
        "    tmp = ''.join(tmp_1) + ' '.join(tmp_2)\n",
        "    logs[index] = tmp\n",
        "\n",
        "  # removing Acquired lease stamp\n",
        "  if \"Acquired lease\" in logs[index]:\n",
        "    tmp = logs[index].split(\"Acquired lease\", 1)\n",
        "    tmp_1 = tmp[0]\n",
        "    tmp_2 = tmp[1].split()\n",
        "    tmp_2 = tmp_2[1:]\n",
        "    tmp = ''.join(tmp_1) + ' '.join(tmp_2)\n",
        "    logs[index] = tmp\n",
        "\n",
        "  # removing \"images will be pullable from\" link\n",
        "  if \"images will be pullable from\" in logs[index]:\n",
        "    tmp = logs[index].split(\"images will be pullable from\", 1)\n",
        "    tmp_1 = tmp[0]\n",
        "    tmp_2 = tmp[1].split()\n",
        "    tmp_2 = tmp_2[1:]\n",
        "    tmp = ''.join(tmp_1) + ' '.join(tmp_2)\n",
        "    logs[index] = tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35RiS8Y2S3Yn"
      },
      "source": [
        "This code cell is removing any unique identifiers such as version numbers, URLs and namespace IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDSHV7fZTOBT"
      },
      "source": [
        "#<H3>Drain3 Processing</H3>\n",
        "\n",
        "The parsed logs are now being processed in Drain. Drain will do additional parsing and also cluster the logs. Drain uses longest common subsequence as their algorithm for clustering, so logs with similar structure will be considered to be in the same clustering. There are two dictionaries that represent the size of each cluster and another that separates the logs by cluster ID. The goal of this is to determine which clusters are pass logs and which clusters are fail logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtFxnlMOmp3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f48147-0577-4fb0-93db-ba4885c3ccef"
      },
      "source": [
        "\n",
        "template_miner = TemplateMiner(None)\n",
        "i = 0\n",
        "ints_from_drain = []\n",
        "while True:\n",
        "    if i >= len(logs):\n",
        "      break\n",
        "    log_line = ' '.join(logs[i])\n",
        "    i += 1\n",
        "    if log_line == 'q':\n",
        "        break\n",
        "    result = template_miner.add_log_message(log_line)\n",
        "    result_json = json.dumps(result)\n",
        "    ints_from_drain.append(re.findall(r'\\d+', result_json))\n",
        "    # print(result_json)\n",
        "\n",
        "ints_from_drain = np.asarray(ints_from_drain)\n",
        "\n",
        "cluster_size = {}\n",
        "cluster_id = {}\n",
        "for cluster in template_miner.drain.clusters:\n",
        "  if cluster.cluster_id not in cluster_size:\n",
        "    cluster_size[cluster.cluster_id] = cluster.size\n",
        "    cluster_id[cluster.cluster_id] = []\n",
        "\n",
        "\n",
        "for i in range(len(ints_from_drain)):\n",
        "  cluster_id[int(ints_from_drain[i][0])].append(logs[i])\n",
        "\n",
        "print(cluster_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config file not found: drain3.ini\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 2, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 1, 36: 1, 37: 1, 38: 1, 39: 1, 40: 1, 41: 1, 42: 1, 43: 1, 44: 1, 45: 1, 46: 1, 47: 1, 48: 1, 49: 1, 50: 1, 51: 1, 52: 1, 53: 1, 54: 1, 55: 1, 56: 1, 57: 1, 58: 1, 59: 1, 60: 1, 61: 1, 62: 1, 63: 1, 64: 1, 65: 1, 66: 1, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 1, 76: 1, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 1, 83: 1, 84: 1, 85: 1, 86: 1, 87: 1, 88: 1, 89: 1, 90: 10, 91: 2, 92: 16, 93: 10, 94: 2, 95: 10, 96: 2, 97: 4, 98: 2, 99: 2, 100: 12, 101: 2, 102: 2, 103: 2, 104: 6, 105: 2, 106: 2, 107: 2, 108: 2, 109: 34, 110: 2, 111: 2, 112: 2, 113: 2, 114: 6, 115: 2, 116: 2, 117: 2, 118: 20, 119: 2, 120: 2, 121: 2, 122: 2, 123: 2, 124: 6, 125: 2, 126: 6, 127: 4, 128: 74, 129: 8, 130: 2, 131: 2, 132: 2, 133: 2, 134: 6, 135: 4, 136: 2, 137: 2, 138: 2, 139: 2, 140: 2, 141: 2, 142: 2, 143: 4, 144: 2, 145: 2, 146: 2, 147: 4, 148: 2, 149: 2}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlznByNoD3w1"
      },
      "source": [
        "#<H3>Parsing Results Print out</H3>\n",
        "\n",
        "Here is a snippet example of what the parsing process does to each log. As you can see the leading byte tag, dates, and unique URLs will be parsed out from both manual parsing and Drain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rqae9VQDaoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2af6815-2c30-4b86-fefc-b29d7c44f9d5"
      },
      "source": [
        "print(original_log.split(',')[:3])\n",
        "print(logs[1].split(',')[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['20/10/28 22:31:45 ci-operator version v20201028-4f6c4ca\"', \" '2020/10/28 22:31:45 No source defined'\", \" '2020/10/28 22:31:45 Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.5-ci'\"]\n",
            "[\"ci-operator No source defined'\", \" Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.5-ci'\", ' Running [release-inputs]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6exsz7srB2en"
      },
      "source": [
        "<H3>Number of unique logs</H3>\n",
        "Passed 416 logs into Drain, it returned 149 unique clusters after that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMuEAdZvVZUX"
      },
      "source": [
        "#<H4>Log cluster example</H4>\n",
        "\n",
        "Here is a printout of logs that fall under a cluster with the ID of 94. This cluster contains the most amount of data logs. Based on the clustering algorithm it is evident that drain uses longest common sequence because all of these logs in this cluster contain the exact same structure with very small differences, especially after being parsed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx-UE8x2VXRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8290ad37-9144-40b2-adf9-d7f1ba9b9bfe"
      },
      "source": [
        "i = 1\n",
        "for log in cluster_id[95]:\n",
        "  print(\"log\", i, \":\",  log[:200], \"...\")\n",
        "  print()\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log 1 : ci-operator No source defined', Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1', warning: overriding parameter \"LEASED_RESOURCE\"', Running [release-inputs], e2e-aws-serial, [ ...\n",
            "\n",
            "log 2 : ci-operator No source defined', Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1', warning: overriding parameter \"LEASED_RESOURCE\"', Running [release-inputs], e2e-aws-serial, [ ...\n",
            "\n",
            "log 3 : ci-operator No source defined', Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1', warning: overriding parameter \"LEASED_RESOURCE\"', Running [release-inputs], e2e-aws-serial, [ ...\n",
            "\n",
            "log 4 : ci-operator No source defined', Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1', warning: overriding parameter \"LEASED_RESOURCE\"', Running [release-inputs], e2e-aws-serial, [ ...\n",
            "\n",
            "log 5 : ci-operator No source defined', Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1', warning: overriding parameter \"LEASED_RESOURCE\"', Running [release-inputs], e2e-aws-serial, [ ...\n",
            "\n",
            "log 6 : ci-operator No source defined', Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1', warning: overriding parameter \"LEASED_RESOURCE\"', Running [release-inputs], e2e-aws-serial, [ ...\n",
            "\n",
            "log 7 : ci-operator No source defined', Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1', warning: overriding parameter \"LEASED_RESOURCE\"', Running [release-inputs], e2e-aws-serial, [ ...\n",
            "\n",
            "log 8 : ci-operator No source defined', Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1', warning: overriding parameter \"LEASED_RESOURCE\"', Running [release-inputs], e2e-aws-serial, [ ...\n",
            "\n",
            "log 9 : ci-operator No source defined', Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1', warning: overriding parameter \"LEASED_RESOURCE\"', Running [release-inputs], e2e-aws-serial, [ ...\n",
            "\n",
            "log 10 : ci-operator No source defined', Resolved release latest to registry.svc.ci.openshift.org/ocp/release:4.1', warning: overriding parameter \"LEASED_RESOURCE\"', Running [release-inputs], e2e-aws-serial, [ ...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMgfiryITDZ5"
      },
      "source": [
        "#<H2>Classifying logs with Drain and parsing</H2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm8de6q1RvAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9431c552-1885-4587-e4a5-c5f41849aa18"
      },
      "source": [
        "vocab = {}\n",
        "i = 0\n",
        "hhh = []\n",
        "for log in logs:\n",
        "  log = str(log).split(\",\")\n",
        "  for line in log:\n",
        "    line = line.replace(\"'\",\"\").replace(\" \",\"\").replace(\"\\\"\",\"\")\n",
        "    if line not in vocab:\n",
        "      vocab[line] = i\n",
        "      i+=1\n",
        "print(len(vocab))\n",
        "# print(hhh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feNguNBnV7c_"
      },
      "source": [
        "<H4>Vectorizing log lines</H4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-7kd8rRR4qL"
      },
      "source": [
        "\n",
        "vectorizer = TfidfVectorizer(vocabulary = vocab)\n",
        "X2 = vectorizer.fit_transform(logs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEYrmPmGWeK3",
        "outputId": "1a3322a8-4714-4fd6-de9f-776a333c7ad0"
      },
      "source": [
        "print(X2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(416, 27665)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfJt0J1bR9ir"
      },
      "source": [
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, train_size= 0.8, random_state=1)\n",
        "sss.get_n_splits(X2, y)\n",
        "y = np.array(y)\n",
        "for train_index, test_index in sss.split(X2, y):\n",
        "  X_train2, X_test2 = X2[train_index], X2[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdOevl5l1Hxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fbd6ae8-e3ea-4ebc-acfb-70a0e8688013"
      },
      "source": [
        "#tfidf\n",
        "X_train2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(332, 27665)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tvepY3kYhXJ"
      },
      "source": [
        "# Classifying logs with Drain with XGB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avuHz_QVYl3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5fbbba-4125-4b10-e6f5-50b2bf08ba3d"
      },
      "source": [
        "model_XGB_D = XGBClassifier(scale_pos_weight=99).fit(X_train2, y_train)\n",
        "y_test_predictions = model_XGB_D.predict(X_test2)\n",
        "accuracy = accuracy_cal(y_test_predictions, y_test)\n",
        "\n",
        "print(\"=== Confusion Matrix ===\")\n",
        "print(confusion_matrix(y_test, y_test_predictions))\n",
        "\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_test, y_test_predictions))\n",
        "\n",
        "print(\"=== Training accuracy ===\")\n",
        "print(accuracy)\n",
        "\n",
        "print(\"Precision score: {}\".format(precision_score(y_test, y_test_predictions)))\n",
        "print(\"Recall score: {}\".format(recall_score(y_test, y_test_predictions)))\n",
        "print(\"F1 Score: {}\".format(f1_score(y_test, y_test_predictions)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "91.6667 %\n",
            "=== Confusion Matrix ===\n",
            "[[21  7]\n",
            " [ 0 56]]\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.75      0.86        28\n",
            "           1       0.89      1.00      0.94        56\n",
            "\n",
            "    accuracy                           0.92        84\n",
            "   macro avg       0.94      0.88      0.90        84\n",
            "weighted avg       0.93      0.92      0.91        84\n",
            "\n",
            "=== Training accuracy ===\n",
            "91.6667\n",
            "Precision score: 0.8888888888888888\n",
            "Recall score: 1.0\n",
            "F1 Score: 0.9411764705882353\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}