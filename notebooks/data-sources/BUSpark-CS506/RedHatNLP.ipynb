{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Due 02/28/2021",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-SchSDwNCgQ"
      },
      "source": [
        "<H1>Red Hat NLP Spark Project</H1>\n",
        "\n",
        "This notebook uses Red Hat's OpenShift data logs and processes them using Drain3. \n",
        "\n",
        "The first thing this notebook does is webscrape the OpenShift logs and installs dependencies. After, the logs are then cleaned by removing leading tags, dates, timestamps and other unique identifiers. This is done manually because drain does not do this consistently for all logs. Finally the parsed logs are then fed into Drain3 and then put into a dictionary sorted by cluster ID.\n",
        "\n",
        "The final step that needs to be done is to sift out the pass and fail logs based on their cluster ID and visualize our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5cWx89XM9t6",
        "outputId": "111f3dc3-e1d8-4288-837a-29fb65ce59ab"
      },
      "source": [
        "!pip3 install drain3\n",
        "!pip3 install kafka-python\n",
        "!pip3 install redis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: drain3 in /usr/local/lib/python3.7/dist-packages (0.9.5)\n",
            "Requirement already satisfied: cachetools==4.2.1 in /usr/local/lib/python3.7/dist-packages (from drain3) (4.2.1)\n",
            "Requirement already satisfied: jsonpickle==1.5.1 in /usr/local/lib/python3.7/dist-packages (from drain3) (1.5.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle==1.5.1->drain3) (3.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle==1.5.1->drain3) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle==1.5.1->drain3) (3.7.4.3)\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: redis in /usr/local/lib/python3.7/dist-packages (3.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq5k5nqiC02i"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import dateutil\n",
        "from dateutil import parser\n",
        "import textblob\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "import drain3\n",
        "from drain3 import TemplateMiner\n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "from drain3.kafka_persistence import KafkaPersistence\n",
        "import re\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import subprocess\n",
        "import time\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq11lna0RAee"
      },
      "source": [
        "#<H3>Web scraping</H3>\n",
        "\n",
        "Gathers OpenShift using BeautifulSoup and then returns an array containing all of the logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT1xSMJ40ewc"
      },
      "source": [
        "# Web scraping \n",
        "# BeautifulSoup for web scraping\n",
        "\n",
        "\n",
        "#url core needed to pull\n",
        "website = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com\"\n",
        "base = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/canary-release-openshift-origin-installer-e2e-aws-4.5-cnv/\"\n",
        "ending = \"build-log.txt\"\n",
        "finished = \"finished.json\"\n",
        "url = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/canary-release-openshift-origin-installer-e2e-aws-4.5-cnv/1300557127638585344/build-log.txt\"\n",
        "page = requests.get(base)   \n",
        "data = page.text\n",
        "soup = BeautifulSoup(data)\n",
        "links = []\n",
        "for link in soup.find_all('a'):\n",
        "    links.append(link.get('href'))\n",
        "links = links[1:-1]\n",
        "\n",
        "final_array = []\n",
        "labels_link = []\n",
        "# create array of urls\n",
        "for x in range(len(links)):\n",
        "  final_array.append(str(website) + str(links[x]) + str(ending))\n",
        "  labels_link.append(str(website) + str(links[x]) + str(finished))\n",
        "\n",
        "# pull all urls logs and store in 2-d array where array_of_logs[x] is a build-log file and \n",
        "# array_of_logs[x][y] is an individual log line split by new line\n",
        "array_of_logs = []\n",
        "for x in range(len(final_array)):\n",
        "  page = urlopen(final_array[x])\n",
        "  html_bytes = page.read()\n",
        "  array_of_logs.append(str(html_bytes).split('\\\\n'))\n",
        "  \n",
        "# first log\n",
        "# print(len(lab))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaYWmzJoL1NL"
      },
      "source": [
        "#url core needed to pull\n",
        "website2 = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com\"\n",
        "base2 = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/release-openshift-ocp-installer-e2e-aws-serial-4.1/\"\n",
        "ending2 = \"build-log.txt\"\n",
        "finished2 = \"finished.json\"\n",
        "page2 = requests.get(base2)    \n",
        "data2 = page2.text\n",
        "soup2 = BeautifulSoup(data2)\n",
        "links2 = []\n",
        "for link2 in soup2.find_all('a'):\n",
        "    links2.append(link2.get('href'))\n",
        "links2 = links2[1:-1]\n",
        "\n",
        "final_array2 = []\n",
        "labels_link2 = []\n",
        "# create array of urls\n",
        "for x in range(len(links2)):\n",
        "  final_array2.append(str(website2) + str(links2[x]) + str(ending2))\n",
        "  labels_link2.append(str(website2) + str(links2[x]) + str(finished2))\n",
        "\n",
        "# pull all urls logs and store in 2-d array where array_of_logs[x] is a build-log file and \n",
        "# array_of_logs[x][y] is an individual log line split by new line\n",
        "array_of_logs2 = []\n",
        "for x in range(len(final_array2)):\n",
        "  page2 = urlopen(final_array2[x])\n",
        "  html_bytes2 = page2.read()\n",
        "  array_of_logs2.append(str(html_bytes2).split('\\\\n'))\n",
        "  \n",
        "# first log\n",
        "# print(array_of_logs2[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGNoRQkn7j3E"
      },
      "source": [
        "# IMPORTANT\n",
        "\n",
        "Run this cell below a few time in a row in order for it to completely remove all the necessary things it is supposed to remove."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsfDoHsxQmqH"
      },
      "source": [
        "for i in range(len(array_of_logs2)):\n",
        "  #  removing newline characters\n",
        "  array_of_logs2[i] = str(array_of_logs2[i]).replace('\\\\n', ' ')\n",
        "  \n",
        " # removes leading 'b from log\n",
        "  array_of_logs2[i] = array_of_logs2[i][2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqeBizbVQn6m"
      },
      "source": [
        "This code cell above removes newline characters and leading byte signature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heDQxXYAQzcU"
      },
      "source": [
        "<H4>Assigning Labels to Logs </H4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX15zcw5v8_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312f448e-bed5-413b-cfe6-c4873561a116"
      },
      "source": [
        "labels= []\n",
        "index = 0\n",
        "to_remove = []\n",
        "\n",
        "for x in range(len(labels_link)):\n",
        "    page = urlopen(labels_link[x])\n",
        "    data = json.load(page) \n",
        "    labels.append(data[\"result\"])\n",
        "    index += 1\n",
        "\n",
        "for x in range(len(labels_link2)):\n",
        "    page = urlopen(labels_link2[x])\n",
        "    try:\n",
        "      data = json.load(page)\n",
        "      labels.append(data[\"result\"])\n",
        "    except:\n",
        "      to_remove.append(index)\n",
        "      continue\n",
        "    index += 1\n",
        "\n",
        "array_of_logs2 = array_of_logs + array_of_logs2\n",
        "\n",
        "for i in range(len(array_of_logs2)):\n",
        "    if i in to_remove:\n",
        "      array_of_logs2[i] = None\n",
        "array_of_logs2 = [log for log in array_of_logs2 if log != None]\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'FAILURE', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'error', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILURE', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb5gAOrHQ67U"
      },
      "source": [
        "<H4>Splitting log lines for vectorization</H4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuzkkC0dOamf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c06b43b0-fc5b-4acf-93df-9959e0f9a40a"
      },
      "source": [
        "vocab = {}\n",
        "i = 0\n",
        "for log in array_of_logs2:\n",
        "  log = str(log).split(\",\")\n",
        "  for line in log:\n",
        "    if line not in vocab:\n",
        "      vocab[line] = i\n",
        "      i+=1\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S056NuDx71Wt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4dc371a-fa11-47cb-b090-67ce29c2293b"
      },
      "source": [
        "# 1 := success, 0 := fail\n",
        "y = []\n",
        "count_0 = 0\n",
        "count_1 = 0\n",
        "for label in labels:\n",
        "  if label == 'SUCCESS':\n",
        "    y.append(1)\n",
        "    count_1 = count_1 + 1\n",
        "  else:\n",
        "    y.append(0)\n",
        "    count_0 = count_0 + 1\n",
        "print(\"y = \", y)\n",
        "print(\"number of success = \", count_1)\n",
        "print(\"number of fail = \", count_0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y =  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "number of success =  140\n",
            "number of fail =  116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTYgLuEaRHiX"
      },
      "source": [
        "#<H2>Classifying logs without drain and parsing</H2>\n",
        "\n",
        "Here begins the process of vectorizing each log line and then classifying it as a pass or fail log accordingly. If this cell has an error, run the cell that removes newline characters and the leading b' tag from the logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ya-78Qo8uQG"
      },
      "source": [
        "total = []\n",
        "for log in array_of_logs2:\n",
        "  s = \", \".join(log)\n",
        "  total.append(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUhrDxlNO7JS"
      },
      "source": [
        "\n",
        "vectorizer = TfidfVectorizer(vocabulary = vocab)\n",
        "# X = vectorizer.fit_transform(total)\n",
        "X = vectorizer.fit_transform(array_of_logs2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn6_PcKe6P6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629e44a7-88a1-4d54-fcf8-936e700c7a94"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<256x40337 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 0 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPerlihUQUkT"
      },
      "source": [
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1)\n",
        "sss.get_n_splits(X, y)\n",
        "y = np.array(y)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTD6hq0FHKsv"
      },
      "source": [
        "Manual splitting to see how classifier is working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHsry0a-6IuU"
      },
      "source": [
        "X_train = X[:100]\n",
        "y_train = y[:100]\n",
        "X_test = X[100:]\n",
        "y_test = y[100:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF3xkjM3_my"
      },
      "source": [
        "#<H4>Classifying logs without Drain with Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y-0e2p9Qa4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7567999-0f6b-4e53-9d75-b6d70abd2813"
      },
      "source": [
        "\n",
        "model = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2,\n",
        "                               min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, \n",
        "                               max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
        "                               bootstrap=True, oob_score=False, n_jobs=None, random_state=1, verbose=0, \n",
        "                               warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None).fit(X_train, y_train)\n",
        "\n",
        "                              \n",
        "\n",
        "y_test_predictions = model.predict(X_test)\n",
        "print(\"=== Confusion Matrix ===\")\n",
        "print(confusion_matrix(y_test, y_test_predictions))\n",
        "\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_test, y_test_predictions))\n",
        "\n",
        "print(y_test_predictions)\n",
        "\n",
        "#print(\"RMSE on testing set = \", mean_squared_error(y_test, y_test_predictions))\n",
        "\n",
        "#print(\"Accuracy on testing set = \",accuracy_score(y_test, y_test_predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Confusion Matrix ===\n",
            "[[ 25   0]\n",
            " [131   0]]\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.16      1.00      0.28        25\n",
            "           1       0.00      0.00      0.00       131\n",
            "\n",
            "    accuracy                           0.16       156\n",
            "   macro avg       0.08      0.50      0.14       156\n",
            "weighted avg       0.03      0.16      0.04       156\n",
            "\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNJ2QKS0CW6r"
      },
      "source": [
        "\n",
        "print(\"Precision score: {}\".format(precision_score(y_test, y_test_predictions)))\n",
        "print(\"Recall score: {}\".format(recall_score(y_test, y_test_predictions)))\n",
        "print(\"F1 Score: {}\".format(f1_score(y_test, y_test_predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UBShV3kRskG"
      },
      "source": [
        "#<H3>Log Parsing</H3>\n",
        "\n",
        "Logs need to be parsed before processing into drain. This is necessary because drain does not parse consistently for every log.\n",
        "\n",
        "Things to be parsed:\n",
        "<ul>\n",
        "  <li>Dates</li>\n",
        "  <li>Timestamps</li>\n",
        "  <li>Newline characters</li>\n",
        "  <li>Version numbers</li>\n",
        "  <li>Namespace IDs</li>\n",
        "  <li>URLs</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDM9A5aB2wSl"
      },
      "source": [
        "original_log = array_of_logs2[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRyol0yU4HP_"
      },
      "source": [
        "#  helper function detecting if a string is a date / timestamp\n",
        "def is_date(str):\n",
        "  try:\n",
        "    dateutil.parser.parse(str)\n",
        "    return True\n",
        "  except:\n",
        "    return False\n",
        "for i in range(len(array_of_logs2)):\n",
        " # splitting each section as its own index (for parsing)\n",
        "  array_of_logs2[i] = str(array_of_logs2[i]).split(' ')\n",
        "  for j in range(len(array_of_logs2[i])):\n",
        "    if is_date(array_of_logs2[i][j]) == True:\n",
        "      array_of_logs2[i][j] = ''\n",
        "  array_of_logs2[i] = list(filter(lambda x: x != '', array_of_logs2[i]))\n",
        "  array_of_logs2[i] = ' '.join(array_of_logs2[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSINSnQeDqZ9"
      },
      "source": [
        "This code cell removes timestamps and dates in order to mitigate unique identifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8a8Nyc76PQo"
      },
      "source": [
        "for index in range(len(array_of_logs2)):\n",
        "  # removing version number\n",
        "  if \"version\" in array_of_logs2[index]:\n",
        "    tmp = array_of_logs2[index].split(\"version\",1)\n",
        "    tmp_1 = tmp[0]\n",
        "    tmp_2 = tmp[1].split()\n",
        "    tmp_2 = tmp_2[1:]\n",
        "    tmp = ''.join(tmp_1) + ' '.join(tmp_2)\n",
        "    array_of_logs2[index] = tmp\n",
        "\n",
        "  # removing creating namespace ID\n",
        "  if \"Creating namespace\" in array_of_logs2[index]:\n",
        "    tmp = array_of_logs2[index].split(\"Creating namespace\", 1)\n",
        "    tmp_1 = tmp[0]\n",
        "    tmp_2 = tmp[1].split()\n",
        "    tmp_2 = tmp_2[1:]\n",
        "    tmp = ''.join(tmp_1) + ' '.join(tmp_2)\n",
        "    array_of_logs2[index] = tmp\n",
        "\n",
        "  # removing using namespace ID\n",
        "  if \"Using namespace\" in array_of_logs2[index]:\n",
        "    tmp = array_of_logs2[index].split(\"Using namespace\", 1)\n",
        "    tmp_1 = tmp[0]\n",
        "    tmp_2 = tmp[1].split()\n",
        "    tmp_2 = tmp_2[1:]\n",
        "    tmp = ''.join(tmp_1) + ' '.join(tmp_2)\n",
        "    array_of_logs2[index] = tmp\n",
        "\n",
        "  # removing Imported release stamp\n",
        "  if \"Imported release\" in array_of_logs2[index]:\n",
        "    tmp = array_of_logs2[index].split(\"Imported release\", 1)\n",
        "    tmp_1 = tmp[0]\n",
        "    tmp_2 = tmp[1].split()\n",
        "    tmp_2 = tmp_2[1:]\n",
        "    tmp = ''.join(tmp_1) + ' '.join(tmp_2)\n",
        "    array_of_logs2[index] = tmp\n",
        "\n",
        "  # removing Acquired lease stamp\n",
        "  if \"Acquired lease\" in array_of_logs2[index]:\n",
        "    tmp = array_of_logs2[index].split(\"Acquired lease\", 1)\n",
        "    tmp_1 = tmp[0]\n",
        "    tmp_2 = tmp[1].split()\n",
        "    tmp_2 = tmp_2[1:]\n",
        "    tmp = ''.join(tmp_1) + ' '.join(tmp_2)\n",
        "    array_of_logs2[index] = tmp\n",
        "\n",
        "  # removing \"images will be pullable from\" link\n",
        "  if \"images will be pullable from\" in array_of_logs2[index]:\n",
        "    tmp = array_of_logs2[index].split(\"images will be pullable from\", 1)\n",
        "    tmp_1 = tmp[0]\n",
        "    tmp_2 = tmp[1].split()\n",
        "    tmp_2 = tmp_2[1:]\n",
        "    tmp = ''.join(tmp_1) + ' '.join(tmp_2)\n",
        "    array_of_logs2[index] = tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35RiS8Y2S3Yn"
      },
      "source": [
        "This code cell is removing any unique identifiers such as version numbers, URLs and namespace IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDSHV7fZTOBT"
      },
      "source": [
        "#<H3>Drain3 Processing</H3>\n",
        "\n",
        "The parsed logs are now being processed in Drain. Drain will do additional parsing and also cluster the logs. Drain uses longest common subsequence as their algorithm for clustering, so logs with similar structure will be considered to be in the same clustering. There are two dictionaries that represent the size of each cluster and another that separates the logs by cluster ID. The goal of this is to determine which clusters are pass logs and which clusters are fail logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtFxnlMOmp3W"
      },
      "source": [
        "\n",
        "template_miner = TemplateMiner(None)\n",
        "i = 0\n",
        "ints_from_drain = []\n",
        "while True:\n",
        "    if i >= len(array_of_logs2):\n",
        "      break\n",
        "    log_line = ' '.join(array_of_logs2[i])\n",
        "    i += 1\n",
        "    if log_line == 'q':\n",
        "        break\n",
        "    result = template_miner.add_log_message(log_line)\n",
        "    result_json = json.dumps(result)\n",
        "    ints_from_drain.append(re.findall(r'\\d+', result_json))\n",
        "    # print(result_json)\n",
        "\n",
        "ints_from_drain = np.asarray(ints_from_drain)\n",
        "\n",
        "cluster_size = {}\n",
        "cluster_id = {}\n",
        "for cluster in template_miner.drain.clusters:\n",
        "  if cluster.cluster_id not in cluster_size:\n",
        "    cluster_size[cluster.cluster_id] = cluster.size\n",
        "    cluster_id[cluster.cluster_id] = []\n",
        "\n",
        "\n",
        "for i in range(len(ints_from_drain)):\n",
        "  cluster_id[int(ints_from_drain[i][0])].append(array_of_logs2[i])\n",
        "\n",
        "print(cluster_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlznByNoD3w1"
      },
      "source": [
        "#<H3>Parsing Results Print out</H3>\n",
        "\n",
        "Here is a snippet example of what the parsing process does to each log. As you can see the leading byte tag, dates, and unique URLs will be parsed out from both manual parsing and Drain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rqae9VQDaoS"
      },
      "source": [
        "# print(original_log.split(',')[:3])\n",
        "print(array_of_logs2[1].split(',')[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6exsz7srB2en"
      },
      "source": [
        "<H3>Number of unique logs</H3>\n",
        "Passed 266 logs into Drain, it returned 155 unique clusters after that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMuEAdZvVZUX"
      },
      "source": [
        "#<H4>Log cluster example</H4>\n",
        "\n",
        "Here is a printout of logs that fall under a cluster with the ID of 94. This cluster contains the most amount of data logs. Based on the clustering algorithm it is evident that drain uses longest common sequence because all of these logs in this cluster contain the exact same structure with very small differences, especially after being parsed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx-UE8x2VXRg"
      },
      "source": [
        "i = 1\n",
        "for log in cluster_id[95]:\n",
        "  print(\"log\", i, \":\",  log[:200], \"...\")\n",
        "  print()\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMgfiryITDZ5"
      },
      "source": [
        "#<H2>Classifying logs with Drain and parsing</H2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm8de6q1RvAx"
      },
      "source": [
        "vocab = {}\n",
        "i = 0\n",
        "hhh = []\n",
        "for log in array_of_logs2:\n",
        "  log = str(log).split(\",\")\n",
        "  for line in log:\n",
        "    if line not in vocab:\n",
        "      vocab[line] = i\n",
        "      i+=1\n",
        "      hhh.append(line)\n",
        "print(len(vocab))\n",
        "# print(hhh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feNguNBnV7c_"
      },
      "source": [
        "<H4>Vectorizing log lines</H4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-7kd8rRR4qL"
      },
      "source": [
        "\n",
        "vectorizer = TfidfVectorizer(vocabulary = vocab)\n",
        "X = vectorizer.fit_transform(hhh)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfJt0J1bR9ir"
      },
      "source": [
        "\n",
        "vectorizer = TfidfVectorizer(vocabulary = vocab)\n",
        "X = vectorizer.fit_transform(total)\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1)\n",
        "sss.get_n_splits(X, y)\n",
        "y = np.array(y)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=1)\n",
        "# sss.get_n_splits(X, y)\n",
        "# y = np.array(y)\n",
        "# for train_index, test_index in sss.split(X, y):\n",
        "#   X_train, X_test = X[train_index], X[test_index]\n",
        "#   y_train, y_test = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4sxG2SiWB0_"
      },
      "source": [
        "#<H4>Classifying logs with Drain with Random Forest Classifier</H4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bpp7dguESDXu"
      },
      "source": [
        "\n",
        "model = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2,\n",
        "                               min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, \n",
        "                               max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
        "                               bootstrap=True, oob_score=False, n_jobs=None, random_state=1, verbose=0, \n",
        "                               warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None).fit(X_train, y_train)\n",
        "                              \n",
        "\n",
        "y_test_predictions = model.predict(X_test)\n",
        "\n",
        "print(\"=== Confusion Matrix ===\")\n",
        "print(confusion_matrix(y_test, y_test_predictions))\n",
        "\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_test, y_test_predictions))\n",
        "\n",
        "#print(\"RMSE on testing set = \", mean_squared_error(y_test, y_test_predictions))\n",
        "\n",
        "\n",
        "#print(\"Accuracy on testing set = \",accuracy_score(y_test, y_test_predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l76BXBD0CnP2"
      },
      "source": [
        "\n",
        "print(\"Precision score: {}\".format(precision_score(y_test, y_test_predictions)))\n",
        "print(\"Recall score: {}\".format(recall_score(y_test, y_test_predictions)))\n",
        "print(\"F1 Score: {}\".format(f1_score(y_test, y_test_predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtfC_gaeGv8v"
      },
      "source": [
        "#Gnerate clusters and a prefix tree\n",
        "The cell below runs the drain3 program. Process the log info and generate clusters and a prefix tree of the log information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb0-1Ou6EYbx"
      },
      "source": [
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(message)s')\n",
        "\n",
        "template_miner = TemplateMiner()\n",
        "\n",
        "line_count = 0\n",
        "start_time = time.time()\n",
        "batch_start_time = start_time\n",
        "batch_size = 10000\n",
        "for i in range(len(array_of_logs2)):\n",
        "  for line in array_of_logs2[i]:\n",
        "      line = line.rstrip()\n",
        "      line = line.partition(\": \")[2]\n",
        "      result = template_miner.add_log_message(line)\n",
        "      line_count += 1\n",
        "      if line_count % batch_size == 0:\n",
        "          time_took = time.time() - batch_start_time\n",
        "          rate = batch_size / time_took\n",
        "          # logger.info(f\"Processing line: {line_count}, rate {rate:.1f} lines/sec, \"\n",
        "          #             f\"{len(template_miner.drain.clusters)} clusters so far.\")\n",
        "          batch_start_time = time.time()\n",
        "      if result[\"change_type\"] != \"none\":\n",
        "          result_json = json.dumps(result)\n",
        "          logger.info(f\"Input ({line_count}): \" + line)\n",
        "          logger.info(\"Result: \" + result_json)\n",
        "\n",
        "time_took = time.time() - start_time\n",
        "rate = line_count / time_took\n",
        "logger.info(f\"--- Done processing file. Total of {line_count} lines, rate {rate:.1f} lines/sec, \"\n",
        "            f\"{len(template_miner.drain.clusters)} clusters\")\n",
        "sorted_clusters = sorted(template_miner.drain.clusters, key=lambda it: it.size, reverse=True)\n",
        "for cluster in sorted_clusters:\n",
        "    logger.info(cluster)\n",
        "\n",
        "print(\"Prefix Tree:\")\n",
        "template_miner.drain.print_tree()\n",
        "\n",
        "template_miner.profiler.report(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Did5do5Ga_TQ"
      },
      "source": [
        "# find events in log, display frequecy of each event in a bar chart\n",
        "\n",
        "s = \"\"\n",
        "for log in array_of_logs2:\n",
        "    temp = \"\"\n",
        "    temp = temp.join(log)\n",
        "    s += temp\n",
        "# Error Message: Text of length 5496377 exceeds maximum of 1000000.\n",
        "s = s[0:100000]\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(s)\n",
        "dictionary = {};\n",
        "for token in doc:\n",
        "    if token.pos_ == 'VERB':\n",
        "        if token.text not in dictionary:\n",
        "            dictionary[token.text] = 1\n",
        "        else:\n",
        "            dictionary[token.text] += 1\n",
        "    #print(token.text, toekn.pos_)\n",
        "\n",
        "#print(dictionary)\n",
        "\n",
        "keys = list(dictionary.keys())\n",
        "values = list(dictionary.values())\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 14))  \n",
        "ax.barh(keys,values)\n",
        "plt.title('Verbs in logs')\n",
        "plt.ylabel('Verbs')\n",
        "plt.xlabel('Frequency')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4HDylvOFVLQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1oosA55NoQZ"
      },
      "source": [
        "The image above shows the verbs and their frequency appear in the tokenized log text. It indicates what event and how often it happens in the log."
      ]
    }
  ]
}