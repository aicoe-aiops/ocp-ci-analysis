{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_WebScraping",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-SchSDwNCgQ"
      },
      "source": [
        "<H1>Web Scraping for NLP Project</H1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq5k5nqiC02i"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import dateutil\n",
        "from dateutil import parser\n",
        "import textblob\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "import re\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import subprocess\n",
        "import time\n",
        "import spacy\n",
        "import pickle\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE517G6j_3Su",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df47290-1cc0-46ef-e7be-71e4b5a9abac"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq11lna0RAee"
      },
      "source": [
        "#<H3>Web scraping</H3>\n",
        "\n",
        "Gathers OpenShift using BeautifulSoup and then returns an array containing all of the logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHDHu0nR9RFe"
      },
      "source": [
        "#log_file_1\n",
        "base1 =\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/canary-release-openshift-origin-installer-e2e-aws-4.5-cnv/\"\n",
        "base2 =\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/release-openshift-ocp-installer-e2e-aws-serial-4.1/\"\n",
        "base3 =\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/release-openshift-ocp-installer-e2e-aws-serial-4.3/\"\n",
        "\n",
        "#log_file_2\n",
        "base4 =\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/branch-ci-open-cluster-management-governance-policy-propagator-main-images/\"\n",
        "base5 =\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/branch-ci-open-cluster-management-console-release-2.3-images/\"\n",
        "base6 =\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/branch-ci-kubevirt-hyperconverged-cluster-operator-release-4.9-images/\"\n",
        "base7=\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/branch-ci-openshift-tektoncd-triggers-release-next-4.5-images/\"\n",
        "base8=\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/branch-ci-openshift-local-storage-operator-master-images/\"\n",
        "base9=\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/periodic-ci-openshift-release-master-ci-4.5-e2e-aws-upgrade-rollback/\"\n",
        "\n",
        "#log_file_3\n",
        "base10 =\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/branch-ci-codeready-toolchain-host-operator-master-test/\"\n",
        "base11 =\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/branch-ci-integr8ly-integreatly-operator-3scale-next-0.7.0-images/\"\n",
        "base12 =\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/branch-ci-open-cluster-management-multicloud-operators-foundation-master-fast-forward/\"\n",
        "\n",
        "#log_file_4\n",
        "base13 =\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/periodic-ci-red-hat-data-services-opendatahub-operator-master-modh-operator-e2e-nightly/\"\n",
        "base14 =\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/release-openshift-origin-installer-e2e-aws-sdn-multitenant-4.6/\"\n",
        "\n",
        "#log_file_5\n",
        "base15 =\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/periodic-ci-openshift-release-master-ocp-4.5-ci-e2e-44-stable-to-45-ci/\"\n",
        "base16 =\"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/periodic-ci-openshift-release-master-ocp-4.7-e2e-metal-assisted-onprem/\"\n",
        "base17 = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com/gcs/origin-ci-test/logs/periodic-ci-operator-framework-operator-lifecycle-managment-rhoperator-metric-e2e-aws-olm-release-4.4-daily/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT1xSMJ40ewc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4cd7981-c5b3-437e-d6fc-ce5d6fbc0ad6"
      },
      "source": [
        "# Web scraping \n",
        "# BeautifulSoup for web scraping\n",
        "#url core needed to pull\n",
        "website = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com\"\n",
        "\n",
        "base = base15\n",
        "ending = \"build-log.txt\"\n",
        "finished = \"finished.json\"\n",
        "page = requests.get(base)   \n",
        "data = page.text\n",
        "soup = BeautifulSoup(data)\n",
        "links = []\n",
        "for link in soup.find_all('a'):\n",
        "    links.append(link.get('href'))\n",
        "links = links[1:-1]\n",
        "\n",
        "final_array = []\n",
        "labels_link = []\n",
        "# create array of urls\n",
        "for x in range(len(links)):\n",
        "  final_array.append(str(website) + str(links[x]) + str(ending))\n",
        "  labels_link.append(str(website) + str(links[x]) + str(finished))\n",
        "\n",
        "# pull all urls logs and store in 2-d array where array_of_logs[x] is a build-log file and \n",
        "# array_of_logs[x][y] is an individual log line split by new line\n",
        "\n",
        "array_of_logs1 = []\n",
        "for x in range(len(final_array)):\n",
        "  page = urlopen(final_array[x])\n",
        "  html_bytes = page.read()\n",
        "  array_of_logs1.append(str(html_bytes).split('\\\\n'))\n",
        "\n",
        "print(len(array_of_logs1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgNDOUFBG4zX",
        "outputId": "d7251cab-a0a4-4d43-c207-2ac948d40b3d"
      },
      "source": [
        "# Web scraping \n",
        "# BeautifulSoup for web scraping\n",
        "#url core needed to pull\n",
        "website = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com\"\n",
        "\n",
        "base = base16\n",
        "ending = \"build-log.txt\"\n",
        "finished = \"finished.json\"\n",
        "page = requests.get(base)   \n",
        "data = page.text\n",
        "soup = BeautifulSoup(data)\n",
        "links = []\n",
        "for link in soup.find_all('a'):\n",
        "    links.append(link.get('href'))\n",
        "links = links[1:-1]\n",
        "\n",
        "final_array = []\n",
        "labels_link2 = []\n",
        "# create array of urls\n",
        "for x in range(len(links)):\n",
        "  final_array.append(str(website) + str(links[x]) + str(ending))\n",
        "  labels_link2.append(str(website) + str(links[x]) + str(finished))\n",
        "\n",
        "# pull all urls logs and store in 2-d array where array_of_logs[x] is a build-log file and \n",
        "# array_of_logs[x][y] is an individual log line split by new line\n",
        "\n",
        "array_of_logs2 = []\n",
        "for x in range(len(final_array)):\n",
        "  page = urlopen(final_array[x])\n",
        "  html_bytes = page.read()\n",
        "  array_of_logs2.append(str(html_bytes).split('\\\\n'))\n",
        "\n",
        "print(len(array_of_logs2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu6M41HEHbHm",
        "outputId": "91c0b7eb-c71c-4675-f3a1-867669ad40c4"
      },
      "source": [
        "# Web scraping \n",
        "# BeautifulSoup for web scraping\n",
        "#url core needed to pull\n",
        "website = \"https://gcsweb-ci.apps.ci.l2s4.p1.openshiftapps.com\"\n",
        "\n",
        "base = base17\n",
        "ending = \"build-log.txt\"\n",
        "finished = \"finished.json\"\n",
        "page = requests.get(base)   \n",
        "data = page.text\n",
        "soup = BeautifulSoup(data)\n",
        "links = []\n",
        "for link in soup.find_all('a'):\n",
        "    links.append(link.get('href'))\n",
        "links = links[1:-1]\n",
        "\n",
        "final_array = []\n",
        "labels_link3 = []\n",
        "# create array of urls\n",
        "for x in range(len(links)):\n",
        "  final_array.append(str(website) + str(links[x]) + str(ending))\n",
        "  labels_link3.append(str(website) + str(links[x]) + str(finished))\n",
        "\n",
        "# pull all urls logs and store in 2-d array where array_of_logs[x] is a build-log file and \n",
        "# array_of_logs[x][y] is an individual log line split by new line\n",
        "\n",
        "array_of_logs3 = []\n",
        "for x in range(len(final_array)):\n",
        "  page = urlopen(final_array[x])\n",
        "  html_bytes = page.read()\n",
        "  array_of_logs3.append(str(html_bytes).split('\\\\n'))\n",
        "\n",
        "print(len(array_of_logs3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEvrmTCnTu-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29bb768b-0866-4f39-84f2-c73361e0bb40"
      },
      "source": [
        "array_of_logs = array_of_logs1 #+ array_of_logs2 #+ array_of_logs3\n",
        "print(len(array_of_logs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGNoRQkn7j3E"
      },
      "source": [
        "# IMPORTANT\n",
        "\n",
        "Run this cell below a few times in a row in order for it to completely remove all the necessary things it is supposed to remove."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsfDoHsxQmqH"
      },
      "source": [
        "\n",
        "for i in range(len(array_of_logs)):\n",
        "  #  removing newline characters\n",
        "  array_of_logs[i] = str(array_of_logs[i]).replace('\\\\n', ' ')\n",
        "  \n",
        " # removes leading 'b from log\n",
        "  array_of_logs[i] = array_of_logs[i][2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqeBizbVQn6m"
      },
      "source": [
        "This code cell above removes newline characters and leading byte signature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heDQxXYAQzcU"
      },
      "source": [
        "<H4>Assigning Labels to Logs </H4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX15zcw5v8_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b072cc2f-d38c-4f5a-dabe-10ac71f41793"
      },
      "source": [
        "labels= []\n",
        "index = 0\n",
        "to_remove = []\n",
        "\n",
        "\n",
        "for x in range(len(labels_link)):\n",
        "    page = urlopen(labels_link[x])\n",
        "    try:\n",
        "      data = json.load(page) \n",
        "    except:\n",
        "      continue\n",
        "    labels.append(data[\"result\"])\n",
        "    index += 1\n",
        "\n",
        "# for x in range(len(labels_link2)):\n",
        "#     page = urlopen(labels_link2[x])\n",
        "#     try:\n",
        "#       data = json.load(page)\n",
        "#       labels.append(data[\"result\"])\n",
        "#     except:\n",
        "#       to_remove.append(index)\n",
        "#       continue\n",
        "#     index += 1\n",
        "\n",
        "# for x in range(len(labels_link3)):\n",
        "#     page = urlopen(labels_link3[x])\n",
        "#     try:\n",
        "#       data = json.load(page)\n",
        "#       labels.append(data[\"result\"])\n",
        "#     except:\n",
        "#       to_remove.append(index)\n",
        "#       continue\n",
        "#     index += 1\n",
        "\n",
        "\n",
        "for i in range(len(array_of_logs)):\n",
        "    if i in to_remove:\n",
        "      array_of_logs[i] = None\n",
        "\n",
        "array_of_logs = [log for log in array_of_logs if log != None]\n",
        "print(len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WflHVjf8LQuc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "63dc2308-b86e-4c23-a59a-d66b6e089d47"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/RedHat_BU/log/log_file_5.ob\", 'wb') as fp:\n",
        "  pickle.dump(array_of_logs,fp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7f26ab9b6050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/RedHat_BU/log/log_file_5.ob\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_of_logs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/RedHat_BU/log/log_file_5.ob'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwk_jjWOR8rs"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/RedHat_BU/label/label_file_5.ob\", 'wb') as fp:\n",
        "  pickle.dump(labels,fp)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}