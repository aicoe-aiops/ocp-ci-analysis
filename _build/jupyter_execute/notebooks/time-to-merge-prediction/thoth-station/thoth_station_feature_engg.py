#!/usr/bin/env python
# coding: utf-8

# # Feature engineering on Thoth Station Github PR Data
# 
# In this notebook, we look at feature importances on PR data extracted from thoth-station organization.

# In[1]:


import os
import ast
from io import StringIO
import sys
import pathlib
from tqdm import tqdm
from functools import partial
from collections import Counter

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import STOPWORDS

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import mutual_info_classif, f_classif

from dotenv import load_dotenv, find_dotenv

metric_template_path = "../../data-sources/TestGrid/metrics"
if metric_template_path not in sys.path:
    sys.path.insert(1, metric_template_path)

from ipynb.fs.defs.metric_template import (  # noqa: E402
    CephCommunication,
    save_to_disk,
)


# In[2]:


sns.set(rc={"figure.figsize": (20, 10)})
load_dotenv(find_dotenv())


# In[3]:


## CEPH Bucket variables
## Create a .env file on your local with the correct configs,
s3_endpoint_url = os.getenv("S3_ENDPOINT")
s3_access_key = os.getenv("S3_ACCESS_KEY")
s3_secret_key = os.getenv("S3_SECRET_KEY")
s3_bucket = os.getenv("S3_BUCKET")
s3_path = os.getenv("S3_PROJECT_KEY", "github")
s3_input_data_path = "raw_data"
REMOTE = os.getenv("REMOTE")


# In[4]:


OUTPUT_DATA_PATH = "output"

if REMOTE:
    print("getting dataset from ceph")
    cc = CephCommunication(s3_endpoint_url, s3_access_key, s3_secret_key, s3_bucket)
    s3_object = cc.s3_resource.Object(s3_bucket, "thoth_PR_data.csv")
    file = s3_object.get()["Body"].read().decode("utf-8")

pr_df = pd.read_csv(StringIO(file))


# In[5]:


pr_df.shape


# In[6]:


pr_df.head(2)


# ## Inspect the dataset
# 
# Let's unroll a single row and see what kind of information we have access to. 

# In[7]:


pr = pr_df.iloc[4]
pr


# ## Remove PRs created by Bots
# 
# As a part of our training dataset, we do not want to include PRs that are created by bots. So lets remove them first.

# In[8]:


set(pr_df["created_by"])


# Let's remove PRs that are created by 'sesheta', 'khebhut[bot]', 'dependabot-preview[bot]', 'dependabot[bot]', 'dependencies[bot]', 'github-actions[bot]', 'codacy-badger'

# In[9]:


bots = [
    "sesheta",
    "khebhut[bot]",
    "dependabot-preview[bot]",
    "dependabot[bot]",
    "dependencies[bot]",
    "github-actions[bot]",
    "codacy-badger",
]


# In[10]:


pr_df = pr_df[~pr_df["created_by"].isin(bots)]


# In[11]:


pr_df.shape


# </br>
# 
# #### **Available Fields**
# 
# * **title**: text providing a short description of the of the PR.
# 
# * **body**: text providing additional details regarding the changes. 
# 
# * **size**: categorical label generated by GitHub summerizing the size of the changes made to the code base. 
# 
# * **created/closed/merged_at**: timestamps for when the PR was opened, merged and finally closed. 
# 
# * **created/closed_by**: usernames for contributors who created the PR and closed it. 
# 
# * **commits_number**: integer indicating the number of commits in the PR.
# 
# * **changed_files_number**: integer indicating the number of files changed.
# 
# The remaining fields appear to be collections of items, let's unroll each one and see what additional information they provide.
# 

# In[12]:


pr["interactions"]


# **interactions**: a dictionary with usernames for keys and the number of words commented on the PR by the user

# In[13]:


pr["reviews"]


# **reviews**: a dictionary of reviews that includes an id number key along with fields for author, word_count, submitted timestamp and state of review.

# In[14]:


pr["labels"]


# **labels**: a list of labels tagged to the PR describing some of its attributes

# In[15]:


pr["commits"]


# **commits**: a list of commit hashes that point to specific changes made to the repo's history

# In[16]:


print(len(pr["changed_files"]))
pr["changed_files"][0:16]


# **changed_files**: a list of the paths and filenames for every file changed by this PR.

# We now know what we have access to in this dataset. It is a collection of numerical, categorical and textual features used to describe a PR. This gives us a lot of potential avenues to explore from an EDA and Data Science perspective. But is also creates an additional challenge insofar as we'll need to do a bit of feature engineering to get this data into format that is ready to be ingested by any ML models. 

# ## Feature engineering
# 
# In the next section we will do some basic feature engineering and correlation analyis on our dataset to see if anything interesting jumps out at us. 

# **Time to Merge**
# 
# We will derive a new feature called `time_to_merge` which will be the difference between the `created_at` and `merged_at` fields. 

# In[17]:


# derive time_to_merege
pr_df["time_to_merge"] = pd.to_datetime(pr_df["merged_at"]) - pd.to_datetime(
    pr_df["created_at"]
)


# In[18]:


pr_df["time_to_merge"] = pr_df["time_to_merge"] / np.timedelta64(1, "s")


# In[19]:


# NOTE: we cant fill NA with 0 because that would imply the PR got merged instantaneously


# Let's look at the distribution of the time to merge column. This can help us determine what the setup for ML problems such as [#236](https://github.com/aicoe-aiops/ocp-ci-analysis/issues/236) could be like. That is, should the time to merge problem be framed as a regression or classification problem.
# 
# First, lets try to determine what the granularity of the output values should be, in case of a regression setup. That is, should the model predict time to merge in number of seconds, number of hours, number of days, or some other chunk of time.

# In[20]:


# what does the distribution of values look like if we choose days as the granularity
ttm_days = np.ceil(pr_df.time_to_merge / (60 * 60 * 24))

# value counts - to how many PRs have value 1, 2, 3, etc. for days to merge
vc = ttm_days.value_counts()

# normalized value counts (i.e. percent of PRs instead of raw number)
nvc = vc / vc.sum()
nvc


# In[21]:


# only plot top 50 values so that graph is legible
plt.rcParams["figure.figsize"] = (10, 6)
nvc.sort_index().iloc[:50].plot(kind="bar")
plt.xlabel("Days")
plt.ylabel("Number of PRs")
plt.title("Number of PRs per Day")
plt.show()


# From the above graph we can see that if we bucket the data into days, then roughly 80% of the data points would have the value "1", 5% would have the value "2" and so on. Therefore we need to select a finer granularity. 

# In[22]:


# what if we chose some intermediate value e.g. 3 hours as the granularity?
ttm_3hours = np.ceil(pr_df.time_to_merge / (60 * 60 * 3))

# value counts - to how many PRs have value 1, 2, 3, etc. for num. 3-hours to merge
vc = ttm_3hours.value_counts()

# normalized value counts (i.e. percent of PRs instead of raw number)
nvc = vc / vc.sum()
nvc


# In[23]:


# only plot top 25 values so that graph is legible
nvc.sort_index().iloc[:25].plot(kind="bar")
plt.xlabel("3 Hour Intervals")
plt.ylabel("Number of PRs")
plt.title("Number of PRs per 3 Hour Interval")
plt.show()


# From the above outputs we can see that if we bucket the data into 3 hours, then roughly 65% of the data points would have the value "1", 5% would have the value "2", 3% would have the value 3, and so on. Lets explore what this would look like with an "hours" granularity.

# In[24]:


# what if we chose hours as the granularity?
ttm_hours = np.ceil(pr_df.time_to_merge / (60 * 60))

# value counts - to how many PRs have value 1, 2, 3, etc. for hours to merge
vc = ttm_hours.value_counts()

# normalized value counts (i.e. percent of PRs instead of raw number)
nvc = vc / vc.sum()
nvc


# In[25]:


# only plot top 25 values so that graph is legible
nvc.sort_index().iloc[:25].plot(kind="bar")
plt.xlabel("Hours")
plt.ylabel("Number of PRs")
plt.title("Number of PRs per Hour")
plt.show()


# From the above outputs we can see that if we bucket the data into hours, then roughly 55% of the data points would have the value "1", 5% would have the value "2", 3% would have the value 3, and so on. 
# 
# As we make it more granular (e.g. minutes), the distribution would get even more uniform. However, a granularity finer than 1 hour might not provide any additional value to the end user. So we will go with granularity of 1 hour for our regression setup.

# Now, lets try to determine, if we were to set this up as a classification problem, what the output classes should be. In the following cell, we'll try to split the time-to-merge values into 10 equally populated buckets.

# In[26]:


pr_df.head(10)


# In[27]:


# lets look at what the 10th, 20th, 30th etc percentiles are, to see what the bin boundaries (in terms of hours)
# would be if we were to split the data in 10 equally populated bins
n_buckets = 10

quantiles = pr_df.time_to_merge.quantile(q=np.arange(0, 1 + 1e-100, 1 / n_buckets))
quantiles / 3600


# In[28]:


# gaps between the buckets (i.e. size of each bucket) in hours
(quantiles - quantiles.shift()) / 3600


# If we want to frame our ML problem as a classification problem, equally sized buckets that can define the classes are class 0 is "merged within 1 minute", class 1 is "merged within 2 mins", class 2 is "merged within 5 minutes", class 3 is "merged within 10 mins", class 4 is "merged within 30 mins", class 5 is "merged within 2 hrs", class 6 is "merged within 8 hrs", class 7 is "merged within 17 hrs", class 8 is "merged within 4 days", class 9 is "merged in greater than 4 days".

# **Body**
# 
# Here, we will derive a feature called `body_size` which will simply be the number of words in the body of the PR. We can use this numerical value to describe the body of the PR.

# In[29]:


# derive body_size
pr_df["body_size"] = pr_df["body"].fillna("").apply(lambda x: len(x.split()))


# In[30]:


pr_df.head(2)


# **Size**
# 
# We can attempt to convert the categorical size label generated by GitHub, which summerizes the size of the changes made to the code base, to a numerical value. 

# In[31]:


convert_size = {"size": {"XS": 0, "S": 1, "M": 2, "L": 3, "XL": 4, "XXL": 5}}
pr_df = pr_df.replace(convert_size)


# **Title**
# 
# Lets try to see if the PR title text can be used to extract any useful information regarding the PR.

# In[32]:


# first lets preprocess the text available in the titles

# convert to lowercase
preproc_titles = pr_df["title"].str.lower()

# remove punctuations and symbols like : ; , # ( ) [ ] etc
preproc_titles = preproc_titles.str.replace(r'[`#-.?!,:;\/()\[\]"\']', " ", regex=True)

# remove hash-like strings i.e. 25+ char long strings containing 0-9 or a-f
preproc_titles = preproc_titles.str.replace(r"[0-9a-f]{25,}", " ", regex=True)

preproc_titles


# In[33]:


# what are the most commonly appearing words in the titles

# combin all titles and split into words
words = preproc_titles.str.cat(sep=" ").split()

# remove stopwords and numbers (e.g. bugzilla ids)
words = [w for w in words if w not in set(STOPWORDS) and not w.isnumeric()]

# word frequencies
unique_words, counts = np.unique(words, return_counts=True)
vc = pd.Series(counts, index=unique_words).sort_values(ascending=False)


# In[34]:


# plot the 20 most common words
sns.barplot(
    x=vc.index[:20],
    y=vc.values[:20],
)
plt.title("Top word frequencies in the PR Titles")
plt.xlabel("words")
plt.xticks(rotation=90)
plt.ylabel("count")
plt.show()


# From the above graph, we can see that titles often contain some keywords that can hint towards the type of changes being made in the PR. We will create a feature which is a vector indicating how many times each word appeared in the title. This way, even if a less prominent word is more correlated with time_to_merge, we will be able to capture that relationship.

# In[35]:


# add word count columns
for word in tqdm(unique_words):
    pr_df[f"title_wordcount_{word}"] = preproc_titles.apply(
        lambda x: x.split().count(word)
    )


# In[36]:


# collapse the high dim vector into one column
wordcount_columns = [c for c in pr_df.columns if "wordcount" in c]
pr_df["title_word_counts_vec"] = pr_df[wordcount_columns].apply(
    lambda x: x.tolist(), axis=1
)

# drop the individual wordcount columns
pr_df = pr_df.drop(columns=wordcount_columns)


# In[37]:


pr_df.head(2)


# **Created By**
# 
# Is the time taken to merge a PR affected by the author of the PR? E.g. if a code reviewer or someone who has already made several contributions to a repo creates a PR, is it likely to get merged relatively faster?

# In[38]:


# Thoth Members list from the repo
thoth_members = [
    "codificat",
    "erikerlandson",
    "fridex",
    "Gkrumbach07",
    "goern",
    "Gregory-Pereira",
    "harshad16",
    "HumairAK",
    "KPostOffice",
    "mayaCostantini",
    "meile18",
    "oindrillac",
    "pacospace",
    "schwesig",
    "sesheta",
    "tumido",
    "xtuchyna",
    "bot",
    "sub-mod",
    "GiorgosKarantonis",
    "CermakM",
    "bjoernh2000",
    "srushtikotak",
    "4n4nd",
    "EldritchJS",
    "sesheta-srcops",
    "Shreyanand",
    "bissenbay",
    "saisankargochhayat",
    "pacospace",
]


# In[39]:


# is the pr creator on the approvers or reviewers list
pr_df["is_member"] = pr_df["created_by"].isin(thoth_members)


# In[40]:


pr_df.head(2)


# **Created At**
# 
# This column can give us information on when the PR was created. Using the `created_at` field, we can derive other features like `time_of_day`, `day_of_week`, `day_of_month`, `month_in_year` each of which can be categorical features that can be useful to our analysis.

# In[41]:


# get day as categorical variable
pr_df["created_at_day"] = pd.to_datetime(pr_df["created_at"]).apply(lambda x: x.day)

# get month as categorical variable
pr_df["created_at_month"] = pd.to_datetime(pr_df["created_at"]).apply(lambda x: x.month)

# get weekday as categorical variable
pr_df["created_at_weekday"] = pd.to_datetime(pr_df["created_at"]).apply(
    lambda x: x.weekday()
)

# get hour of day as categorical variable
pr_df["created_at_hour"] = pd.to_datetime(pr_df["created_at"]).apply(lambda x: x.hour)


# In[42]:


pr_df.head(2)


# **Commits Number**  
# Commits Number gives the number of commits created within the PR. This could be a numerical feature for our model.

# In[43]:


pr_df.commits_number.value_counts()


# In[44]:


pr_df.commits_number.value_counts().sort_index(ascending=True).plot(kind="bar")
plt.title("Commit frequency across PRs")
plt.xlabel("No. of commits")
plt.ylabel("No. of PRs")
plt.show()


# As we see above, in this dataset, there is not much variability in the commits number. Majority of the PRs (77%) have 1 commit, 11% have 2 commits and rest(12%) have more than 2 commits. We can break commits number down into 3 categorical variables, `contains_1_commit`, `contains_2_commits`, `contains_3_or_more_commits`. However, there might be some value in knowing whether a PR had 5 commits vs. 10 commits. So we will treat it as a numerical variable to begin with.

# In[45]:


pr_df.head(2)


# **Changed Files Number**  
# Changed files is a numerical variable which captures the number of files that were modified in the PR.

# In[46]:


# convert changed_files_number to int
pr_df["changed_files_number"] = pr_df["changed_files_number"].astype("int")

pr_df.changed_files_number.value_counts()


# In[47]:


pr_df.changed_files_number.value_counts().iloc[:50].sort_index(ascending=True).plot(
    kind="bar"
)
plt.title("Frequency of changed files across PRs")
plt.xlabel("No. of changed files")
plt.ylabel("No. of PRs")
plt.show()


# `changed_files_number` has more variability than the `commits_number`. We can incorporate this as a numerical feature.

# **Changed_files**
# 
# `changed_files` is a list of the paths and filenames for every file changed by this PR. From `changed_files`, we know, for each PR, which file types were changed which can be encoded as a frequency vector. Also we know which directory the change is made in, so that can be encoded as a categorical variable.

# In[48]:


pr_df.changed_files


# In[49]:


def filetype(filepath):

    # if standard file extension, return file extension
    if pathlib.Path(filepath).suffix:
        return pathlib.Path(filepath).suffix

    else:
        # if single file at the root of repo
        if "/" not in filepath:
            # if a file like README/Dockerfile etc
            if "." not in filepath:
                return filepath
        # if file not at root
        else:
            # get file name
            file = filepath.rsplit("/", 1)[1]
            if "." not in file:
                return file
            elif "." in file:
                return "." + file.rsplit(".", 1)[1]


# In[50]:


# get list of all file types that were modified across all thoth station PRs
changed_files = list(pr_df["changed_files"])


# In[51]:


changed_files[0]


# In[52]:


all_changed_files = [
    filetype(filepath) for pr in changed_files for filepath in ast.literal_eval(pr)
]


# In[53]:


count = Counter(all_changed_files)


# In[54]:


# save 100 most commonly occuring file extensions across PRs
top_fileextensions_freq = count.most_common(100)


# In[55]:


top_fileextensions_freq


# In[56]:


top_fileextensions = [ext[0] for ext in top_fileextensions_freq]


# In[57]:


top_fileextensions


# In[58]:


def file_type_freq(list_of_filepaths):
    """
    For a given PR's list of changed files,
    return a changed files frequency vector
    ( a vector which holds the frequency of
    changed files of the type  for every
    file type in top_fileextensions )
    """

    file_extensions = [filetype(f) for f in list_of_filepaths]
    ext_dict = {key: 0 for key in top_fileextensions}
    for f in file_extensions:
        if f in ext_dict:
            ext_dict[f] += 1

    return list(ext_dict.values())


# In[59]:


pr_df["changed_file_type_vec"] = pr_df["changed_files"].apply(
    lambda x: file_type_freq(x)
)


# In[60]:


pr_df.head(2)


# ## Infer dataframe columns as Features

# In[61]:


# subset DataFrame to use as features
feature_df = pr_df[
    [
        "time_to_merge",
        "changed_files_number",
        "body_size",
        "size",
        "title_word_counts_vec",
        "is_member",
        "created_at_day",
        "created_at_month",
        "created_at_weekday",
        "created_at_hour",
        "commits_number",
        "changed_file_type_vec",
    ]
]


# ### Add PR TTM Class Column 
# 
# Lets split up the time to merge values into 10 discrete bins (as discussed above) to use as our prediction classes.

# In[62]:


def get_ttm_class(ttm):
    """
    Assign a ttm "class" / "category" / "bin" to the input numerical ttm value
    E.g. if the time to merge was 1 hours, this function will return
    class "5" which represents "merged in 20 mins to 1 hour"
    """
    if ttm < 0.00166:
        return 0
    elif ttm < 0.03333:
        return 1
    elif ttm < 0.13333:
        return 2
    elif ttm < 0.33333:
        return 3
    elif ttm < 1:
        return 4
    elif ttm < 4:
        return 5
    elif ttm < 18:
        return 6
    elif ttm < 72:
        return 7
    elif ttm < 504:
        return 8
    else:
        return 9


feature_df["ttm_class"] = (feature_df["time_to_merge"] / 3600).apply(get_ttm_class)
feature_df.head()


# In[63]:


# visualize data points distribution across classes
feature_df["ttm_class"].hist()
plt.title("Distribution of PRs across TTM Labels")
plt.xlabel("Time to Merge Class")
plt.ylabel("Number of PRs")
plt.show()


# From the above graph, we can see that data is fairly uniformly distributed across all classes. At the same time, the class definitions are also easily interpretable by human experts. So this seems like a good setup for our classification task.

# ## Evaluating Feature Importances
# 
# ### Categorical Features
# 
# First, lets try to determine what categorical featuers are powerful influencers for time to merge of a PR

# In[64]:


# feature selection helper function
def select_features(x, y, func):
    fs = SelectKBest(score_func=func, k="all")
    fs.fit(x, y)
    x_fs = fs.transform(x)
    return x_fs, fs


selected_features = []


# In[65]:


cat_df = feature_df[
    [
        "size",
        "is_member",
        "created_at_day",
        "created_at_month",
        "created_at_weekday",
        "created_at_hour",
        "ttm_class",
    ]
]

cat_df.head()


# In[66]:


# split into input (x) and output (y) variables
X_inp = cat_df.drop(columns=["ttm_class"])
y = cat_df["ttm_class"]


# In[67]:


# rank using mutual_info_classif
X_fs, fs = select_features(X_inp, y, mutual_info_classif)


# In[68]:


for i in range(len(fs.scores_)):
    print("Feature %s: %f" % (X_inp.columns[i], fs.scores_[i]))


# In[69]:


# visualize scores of all features
sns.barplot(x=X_inp.columns, y=fs.scores_)
plt.title("Feature importance scores for Mutual Information Statistic")
plt.xticks(rotation=60)
plt.xlabel("Features")
plt.ylabel("Scores")
plt.show()


# In[70]:


# Lets get a list of the top 5 feature indices based on Mutual Info Stat feature scores
top_mis = fs.scores_.argsort()[-5:][::-1]


# In[71]:


# rank using chi_sq
X_fs, fs = select_features(X_inp, y, chi2)


# In[72]:


for i in range(len(fs.scores_)):
    print("Feature %s: %f" % (X_inp.columns[i], fs.scores_[i]))


# In[73]:


# visualize scores of all features
sns.barplot(x=X_inp.columns, y=fs.scores_)
plt.title("Feature importance scores for Chi Squared")
plt.xticks(rotation=60)
plt.xlabel("Features")
plt.ylabel("Scores")
plt.show()


# In[74]:


# Lets get a list of the top 5 feature indices based on Chi Sq feature scores
top_chi2 = fs.scores_.argsort()[-5:][::-1]


# Let's get a Union of the top 5 features derived from features based on both scores of both Mutual Info Statistic and Chi Squared

# In[75]:


union = set(top_mis).union(set(top_chi2))
print(len(union))
union


# We now have the top 12 features based on their feature importance scores. We can eliminate the remaining 7 features from the categorical features which have low feature importance scores for both  Mutual Info Statistic and Chi Squared 

# In[76]:


cat_df.head()


# In[77]:


# update list of features to keep for training
selected_features += [
    "size",
    "created_at_day",
    "created_at_month",
    "created_at_weekday",
    "created_at_hour",
]


# ### Numerical Features
# 
# Now, lets try to determine what numerical featuers are powerful influencers for time to merge of a PR

# In[78]:


numerical_df = feature_df[
    [
        "changed_files_number",
        "body_size",
        "commits_number",
        "ttm_class",
    ]
]

numerical_df.head()


# In[79]:


# split into input features and target
X_inp = numerical_df.drop(columns=["ttm_class"])
y = numerical_df["ttm_class"]


# In[80]:


# create k best feature selector object based on mutual information
numerical_feats_mi, k_best_selector_mi = select_features(
    X_inp,
    y,
    partial(
        mutual_info_classif,
        discrete_features=False,
        random_state=42,
        n_neighbors=3,
    ),
)

# visualize the MI scores of all features
sns.barplot(x=X_inp.columns, y=k_best_selector_mi.scores_)
plt.title("Feature importance based on Mutual Information")
plt.xticks(rotation=60)
plt.xlabel("Features")
plt.ylabel("Scores")
plt.show()


# In[81]:


# create k best feature selector object based on anova f statistic
numerical_feats_f, k_best_selector_f = select_features(
    X_inp,
    y,
    f_classif,
)

# visualize the f stat scores and pvalues of all features
fig, axes = plt.subplots(2, 1, sharex=True)
fig.suptitle("Feature importance based on ANOVA F stat")
sns.barplot(x=X_inp.columns, y=k_best_selector_f.scores_, ax=axes[0])
sns.barplot(x=X_inp.columns, y=k_best_selector_f.pvalues_, ax=axes[1])
plt.xticks(rotation=60)
plt.xlabel("Features")
axes[0].set_ylabel("Scores")
axes[1].set_ylabel("p-values")
plt.show()


# From the above cells, we can see that different features are scored differently based on mutual information criterion and ANOVA F statistic. Since there are only 4 of these features, we will keep all of them in our features dataframe.

# In[82]:


# update list of features to keep for training
selected_features += [
    "changed_files_number",
    "body_size",
    "commits_number",
]


# ### Count Vector Type Features
# 
# Finally, lets determine the most important features from the count vector type columns.

# In[83]:


# explode the list column so that there is 1 column representing frequency of each file type
filetype_df = pd.DataFrame(feature_df.changed_file_type_vec.to_list())
filetype_df.index = feature_df.index
filetype_df.columns = [f"filetype_{f}" for f in top_fileextensions]


# In[84]:


filetype_df


# In[85]:


# compute correlation with ttm
corr = filetype_df.corrwith(feature_df["ttm_class"])
corr.sort_values(ascending=False)


# In[86]:


corr[corr.abs() > 0.01]


# We see above that there is no correlation between ttm_class and filetype

# In[87]:


# explode the list column so that there is 1 column representing frequency of each word
title_wordcount_df = pd.DataFrame(feature_df.title_word_counts_vec.to_list())

title_wordcount_df.index = feature_df.index
title_wordcount_df.columns = [f"title_wordcount_{w}" for w in unique_words]


# In[88]:


# compute correlation with ttm
corr = title_wordcount_df.corrwith(feature_df["ttm_class"])
corr = corr.dropna()
corr.sort_values(ascending=False)


# In[89]:


corr[corr.abs() > 0.045]


# In[90]:


# update list of features to keep for training
# take everything that has more than 0.045 correlation magnitude
keep_cols = corr[corr.abs() > 0.045].index.tolist()
selected_features += keep_cols

title_wordcount_df = title_wordcount_df[keep_cols]


# Using various feature importance scores such as chi-squared, mutual information statistic, correlation values, we have selected the most relevant features amongst all of our features. 

# In[91]:


selected_features


# ## Save Feature Engineered Dataset

# In[92]:


# join all types of features
ttm_dataset = pd.concat(
    [feature_df, filetype_df, title_wordcount_df], axis=1, ignore_index=False
)

# keep only the selected features and the prediction label
ttm_dataset = ttm_dataset[selected_features + ["ttm_class", "time_to_merge"]]

ttm_dataset.head()


# In[93]:


ttm_dataset.info()


# In[94]:


ttm_dataset


# ## Save results to Ceph or locally
# 
# Using the helper function to save the data frame in a parquet format on the Ceph bucket if we are running in automation, and locally if not.

# In[95]:


## Sanity check to see if the dataset is the same

if REMOTE:
    cc = CephCommunication(s3_endpoint_url, s3_access_key, s3_secret_key, s3_bucket)
    try:
        cc.upload_to_ceph(
            ttm_dataset,
            s3_path,
            "thoth-station/ttm_train_test_dataset.parquet",
        )
    except ValueError as ve:
        print(ve)
        print("Files already uploaded to S3")

else:
    save_to_disk(
        ttm_dataset,
        OUTPUT_DATA_PATH,
        "ttm_dataset.parquet",
    )


# # Conclusion
# 
# This notebook walked through how to access and engineer features from the the relevant PR data for repos on the Thoth Station Org. We also outlined what is contained in each record and provided a quick demonstration of how to convert the mixed data types into numerical values for statistical analysis and eventually machine learning. 
# 
# We have also engineered some possible features which can be used to predict time to merge of a PR. As next steps, we will train a classifier to predict time to merge of a PR using the features.
