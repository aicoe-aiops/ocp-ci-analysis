#!/usr/bin/env python
# coding: utf-8

# ## Create Tables for Metric Data on Trino 
# 
# In this notebook, we programatically create sql tables on Trino from metric parquet files generated by analyzing Testgrid and stored on S3. These metric files correspond to various [key performance indicator metrics](metrics/README.md) that are relevant to various personas (developer, manager, etc.) involved in the CI process.

# In[1]:


import os
from dotenv import load_dotenv, find_dotenv
import trino


# In[2]:


load_dotenv(find_dotenv())


# In[3]:


## CEPH Bucket variables
## Create a .env file on your local with the correct configs,
s3_endpoint_url = os.getenv("S3_ENDPOINT")
s3_access_key = os.getenv("S3_ACCESS_KEY")
s3_secret_key = os.getenv("S3_SECRET_KEY")
s3_bucket = os.getenv("S3_BUCKET")
AUTOMATION = os.getenv("IN_AUTOMATION")


# In[4]:


# Create an Trino client
conn = trino.dbapi.connect(
    auth=trino.auth.BasicAuthentication(
        os.environ["TRINO_USER"], os.environ["TRINO_PASSWD"]
    ),
    host=os.environ["TRINO_HOST"],
    port=int(os.environ["TRINO_PORT"]),
    http_scheme="https",
    verify=True,
)
cur = conn.cursor()


# In[5]:


test_pass_failures = """CREATE TABLE IF NOT EXISTS hive.default.test_pass_failures (
          timestamp timestamp,
          tab varchar,
          grid varchar,
          test varchar,
          failure boolean,
          passing boolean
)
WITH (
   external_location = 's3a://opf-datacatalog-zero-backup/ai4ci/testgrid/metrics/test_pass_failures',
   format = 'PARQUET'
)"""


# In[6]:


cur.execute(test_pass_failures)
cur.fetchall()


# In[7]:


## Check if the table is there
cur.execute("select * from hive.default.test_pass_failures LIMIT 2")
cur.fetchall()


# In[8]:


blocked_timed_out = """CREATE TABLE IF NOT EXISTS hive.default.blocked_timed_out (
        timestamp timestamp,
        tab varchar,
        grid varchar,
        test_blocked boolean,
        test_timed_out boolean
)
WITH (
   external_location = 's3a://opf-datacatalog-zero-backup/ai4ci/testgrid/metrics/blocked_timed_out',
   format = 'PARQUET'
)"""


# In[9]:


cur.execute(blocked_timed_out)
cur.fetchall()


# In[10]:


## Check if the table is there
cur.execute("select * from hive.default.blocked_timed_out LIMIT 2")
cur.fetchall()


# In[11]:


number_of_flakes = """CREATE TABLE IF NOT EXISTS hive.default.number_of_flakes (
        timestamp timestamp,
        tab varchar,
        grid varchar,
        test varchar,
        flake boolean
)
WITH (
   external_location = 's3a://opf-datacatalog-zero-backup/ai4ci/testgrid/metrics/number_of_flakes',
   format = 'PARQUET'
)
"""


# In[12]:


cur.execute(number_of_flakes)
cur.fetchall()


# In[13]:


## Check if the table is there
cur.execute("select * from hive.default.number_of_flakes LIMIT 2")
cur.fetchall()


# In[14]:


avg_correlation = """CREATE TABLE IF NOT EXISTS hive.default.avg_correlation (
        timestamp timestamp,
        average_number_of_correlated_failures double
)
WITH (
   external_location = 's3a://opf-datacatalog-zero-backup/ai4ci/testgrid/metrics/avg_correlation',
   format = 'PARQUET'
)"""


# In[15]:


cur.execute(avg_correlation)
cur.fetchall()


# In[16]:


## Check if the table is there
cur.execute("select * from hive.default.avg_correlation LIMIT 2")
cur.fetchall()


# In[17]:


correlation = """CREATE TABLE IF NOT EXISTS hive.default.correlation (
        test_name varchar,
        correlated_tests varchar
)
WITH (
   external_location = 's3a://opf-datacatalog-zero-backup/ai4ci/testgrid/metrics/correlation',
   format = 'PARQUET'
)"""


# In[18]:


cur.execute(correlation)
cur.fetchall()


# In[19]:


## Check if the table is there
cur.execute("select * from hive.default.correlation LIMIT 1")
cur.fetchall()


# In[20]:


pct_fixed_each_ts = """CREATE TABLE IF NOT EXISTS hive.default.pct_fixed_each_ts (
        timestamp timestamp,
        tab varchar,
        grid varchar,
        pct_fixed double
)
WITH (
   external_location = 's3a://opf-datacatalog-zero-backup/ai4ci/testgrid/metrics/pct_fixed_each_ts',
   format = 'PARQUET'
)"""


# In[21]:


cur.execute(pct_fixed_each_ts)
cur.fetchall()


# In[22]:


## Check if the table is there
cur.execute("select * from hive.default.pct_fixed_each_ts LIMIT 2")
cur.fetchall()


# In[23]:


persistent_failures = """CREATE TABLE IF NOT EXISTS hive.default.persistent_failures (
        tab varchar,
        grid varchar,
        test varchar,
        consec_fail_rate double,
        mean_fail_len double,
        mean_time_to_fix double,
        pass_to_fail_rate double,
        fail_to_pass_rate double
)
WITH (
   external_location = 's3a://opf-datacatalog-zero-backup/ai4ci/testgrid/metrics/persistent_failures',
   format = 'PARQUET'
)"""


# In[24]:


cur.execute(persistent_failures)
cur.fetchall()


# In[25]:


## Check if the table is there
cur.execute("select * from hive.default.persistent_failures LIMIT 2")
cur.fetchall()


# In[26]:


build_pass_failure = """CREATE TABLE IF NOT EXISTS hive.default.build_pass_failure (
        timestamp timestamp,
        tab varchar,
        grid varchar,
        test varchar,
        build_failure boolean,
        build_passing boolean,
        build_status varchar
)
WITH (
   external_location = 's3a://opf-datacatalog-zero-backup/ai4ci/testgrid/metrics/build_pass_failure',
   format = 'PARQUET'
)"""


# In[27]:


cur.execute(build_pass_failure)
cur.fetchall()


# In[28]:


## Check if the table is there
cur.execute("select * from hive.default.build_pass_failure LIMIT 1")
cur.fetchall()


# In[29]:


time_to_test = """CREATE TABLE IF NOT EXISTS hive.default.time_to_test (
        timestamp timestamp,
        tab varchar,
        grid varchar,
        test varchar,
        test_duration double
)
WITH (
   external_location = 's3a://opf-datacatalog-zero-backup/ai4ci/testgrid/metrics/time_to_test',
   format = 'PARQUET'
)"""


# In[30]:


cur.execute(time_to_test)
cur.fetchall()


# In[31]:


## Check if the table is there
cur.execute("select * from hive.default.time_to_test LIMIT 2")
cur.fetchall()


# In[32]:


# TODO: location of metric file needs to be changed once smaug cluster is available
# probability_to_fail is ebing excluded from OSS demo due to resource limitations
probability_to_fail = """CREATE TABLE IF NOT EXISTS hive.default.probability_to_fail (
        timestamp timestamp,
        tab varchar,
        grid varchar,
        test varchar,
        prob double
)
WITH (
   external_location = 's3a://opf-datacatalog-zero-backup/s3_path/probability_to_fail',
   format = 'PARQUET'
)
"""


# In[33]:


cur.execute(probability_to_fail)
cur.fetchall()


# In[34]:


## Check if the table is there
cur.execute("select * from hive.default.probability_to_fail LIMIT 2")
cur.fetchall()


# ## Conclusion
# 
# In this notebook, we succesfully created sql tables from metric parquet files which is a result of metric calculation on testgrid data
